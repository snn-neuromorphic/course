@article{Hodgkin.1948, 
author = {Hodgkin, A. L.}, 
title = {The local electric changes associated with repetitive action in a non‐medullated axon}, 
issn = {1469-7793}, 
doi = {10.1113/jphysiol.1948.sp004260}, 
pmid = {16991796}, 
pmcid = {PMC1392160}, 
pages = {165--181}, 
number = {2}, 
volume = {107}, 
journal = {The Journal of Physiology}, 
year = {1948}
}

@article{Izhikevich.2003,
author = {Izhikevich, Eugene M.}, 
title = {Simple Model of Spiking Neurons}, 
issn = {1045-9227}, 
doi = {10.1109/tnn.2003.820440}, 
pmid = {18244602}, 
abstract = {{A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin–Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.}}, 
pages = {1569}, 
number = {6}, 
volume = {14}, 
journal = {IEEE Transactions on Neural Networks}, 
year = {2003}
}

@article{rodrigez2004,
author = {Toledo-Rodriguez, Maria and Blumenfeld, Barak and Wu, Caizhi and Luo, Junyi and Attali, Bernard and Goodman, Philip and Markram, Henry}, 
title = {Correlation Maps Allow Neuronal Electrical Properties to be Predicted from Single-cell Gene Expression Profiles in Rat Neocortex}, 
issn = {1047-3211}, 
doi = {10.1093/cercor/bhh092}, 
pmid = {15192011}, 
abstract = {{The computational power of the neocortex arises from interactions of multiple neurons, which display a wide range of electrical properties. The gene expression profiles underlying this phenotypic diversity are unknown. To explore this relationship, we combined whole-cell electrical recordings with single-cell multiplex RT-PCR of rat (p13–16) neocortical neurons to obtain cDNA libraries of 26 ion channels (including voltage activated potassium channels, Kv1.1/2/4/6, Kvβ1/2, Kv2.1/2, Kv3.1/2/3/4, Kv4.2/3; sodium/potassium permeable hyperpolarization activated channels, HCN1/2/3/4; the calcium activated potassium channel, SK2; voltage activated calcium channels, Caα1A/B/G/I, Caβ1/3/4), three calcium binding proteins (calbindin, parvalbumin and calretinin) and GAPDH. We found a previously unreported clustering of ion channel genes around the three calcium-binding proteins. We further determined that cells similar in their expression patterns were also similar in their electrical properties. Subsequent regression modeling with statistical resampling yielded a set of coefficients that reliably predicted electrical properties from the expression profile of individual neurons. This is the first report of a consistent relationship between the co-expression of a large profile of ion channel and calcium binding protein genes and the electrical phenotype of individual neocortical neurons.}}, 
pages = {1310--1327}, 
number = {12}, 
volume = {14}, 
journal = {Cerebral Cortex}, 
year = {2004}
}

@article{cajal1909,
author = {Cajal, Santiago Ramón}, 
title = {Histologie du système nerveux de l'homme & des vertébrés.}, 
doi = {10.5962/bhl.title.48637}, 
year = {1909}
}

@article{2013,
  title = {Whatever next? {{Predictive}} Brains, Situated Agents, and the Future of Cognitive Science},
  shorttitle = {Whatever Next?},
  year = {2013},
  month = jun,
  volume = {36},
  pages = {181--204},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X12000477},
  abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this ``hierarchical prediction machine'' approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
  file = {/Users/x0r/Zotero/storage/XKACTJ6L/2013_Whatever next.pdf},
  journal = {Behavioral and Brain Sciences},
  keywords = {AGI},
  language = {en},
  number = {03}
}

@misc{2018,
  title = {{{AI}} and {{Compute}}},
  year = {2018},
  month = may,
  abstract = {Since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.5 month doubling time (by comparison, Moore's Law had an 18 month doubling period).},
  file = {/Users/x0r/Zotero/storage/B4DHAM4F/2018_AI and Compute.pdf;/Users/x0r/Zotero/storage/NL59YH5Z/ai-and-compute.html},
  howpublished = {https://blog.openai.com/ai-and-compute/},
  journal = {OpenAI Blog},
  keywords = {neuromorphic}
}

@article{abdollahramezani2020,
  title = {Tunable Nanophotonics Enabled by Chalcogenide Phase-Change Materials},
  author = {Abdollahramezani, Sajjad and Hemmatyar, Omid and Taghinejad, Hossein and Krasnok, Alex and Kiarashinejad, Yashar and Zandehshahvar, Mohammadreza and Al{\`u}, Andrea and Adibi, Ali},
  year = {2020},
  month = jun,
  volume = {-1},
  publisher = {{De Gruyter}},
  issn = {2192-8614, 2192-8606},
  doi = {10.1515/nanoph-2020-0039},
  abstract = {{$<$}section class="abstract"{$><$}h2 class="abstractTitle text-title my-1" id="d80e2"{$>$}Abstract{$<$}/h2{$><$}p{$>$}Nanophotonics has garnered intensive attention due to its unique capabilities in molding the flow of light in the subwavelength regime. Metasurfaces (MSs) and photonic integrated circuits (PICs) enable the realization of mass-producible, cost-effective, and efficient flat optical components for imaging, sensing, and communications. In order to enable nanophotonics with multipurpose functionalities, chalcogenide phase-change materials (PCMs) have been introduced as a promising platform for tunable and reconfigurable nanophotonic frameworks. Integration of non-volatile chalcogenide PCMs with unique properties such as drastic optical contrasts, fast switching speeds, and long-term stability grants substantial reconfiguration to the more conventional static nanophotonic platforms. In this review, we discuss state-of-the-art developments as well as emerging trends in tunable MSs and PICs using chalcogenide PCMs. We outline the unique material properties, structural transformation, and thermo-optic effects of well-established classes of chalcogenide PCMs. The emerging deep learning-based approaches for the optimization of reconfigurable MSs and the analysis of light-matter interactions are also discussed. The review is concluded by discussing existing challenges in the realization of adjustable nanophotonics and a perspective on the possible developments in this promising area.{$<$}/p{$><$}/section{$>$}},
  chapter = {Nanophotonics},
  file = {/Users/x0r/Zotero/storage/Q2FJX3GM/Abdollahramezani et al. - 2020 - Tunable nanophotonics enabled by chalcogenide phas.pdf;/Users/x0r/Zotero/storage/56R6SCLQ/article-10.1515-nanoph-2020-0039.html},
  journal = {Nanophotonics},
  language = {en},
  number = {ahead-of-print}
}

@article{abraham2018,
  title = {Memristor - {{The}} Fictional Circuit Element},
  author = {Abraham, Isaac},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-29394-7},
  abstract = {The memory resistor abbreviated memristor was a harmless postulate in 1971. In the decade since 2008, a device claiming to be the missing memristor is on the prowl, seeking recognition as a fundamental circuit element, sometimes wanting electronics textbooks to be rewritten, always promising remarkable digital, analog and neuromorphic computing possibilities. A systematic discussion about the fundamental nature of the device is almost universally absent. This report investigates the assertion that the memristor is a fundamental passive circuit element, from the perspective that electrical engineering is the science of charge management. With a periodic table of fundamental elements, we demonstrate that there can only be three fundamental passive circuit elements. The ideal memristor is shown to be an unphysical active device. A vacancy transport model further reveals that a physically realizable memristor is a nonlinear composition of two resistors with active hysteresis.},
  archivePrefix = {arXiv},
  eprint = {1808.05982},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GSZKJST3/Abraham_2018_Memristor - The fictional circuit element.pdf},
  journal = {Scientific Reports},
  number = {1}
}

@article{achiam2019,
  title = {Towards {{Characterizing Divergence}} in {{Deep Q}}-{{Learning}}},
  author = {Achiam, Joshua and Knight, Ethan and Abbeel, Pieter},
  year = {2019},
  month = mar,
  abstract = {Deep Q-Learning (DQL), a family of temporal difference algorithms for control, employs three techniques collectively known as the `deadly triad' in reinforcement learning: bootstrapping, off-policy learning, and function approximation. Prior work has demonstrated that together these can lead to divergence in Q-learning algorithms, but the conditions under which divergence occurs are not well-understood. In this note, we give a simple analysis based on a linear approximation to the Q-value updates, which we believe provides insight into divergence under the deadly triad. The central point in our analysis is to consider when the leading order approximation to the deep-Q update is or is not a contraction in the sup norm. Based on this analysis, we develop an algorithm which permits stable deep Q-learning for continuous control without any of the tricks conventionally used (such as target networks, adaptive gradient optimizers, or using multiple Q functions). We demonstrate that our algorithm performs above or near state-of-the-art on standard MuJoCo benchmarks from the OpenAI Gym.},
  archivePrefix = {arXiv},
  eprint = {1903.08894},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NZUBW7FH/Achiam et al_2019_Towards Characterizing Divergence in Deep Q-Learning.pdf},
  journal = {arXiv:1903.08894 [cs]},
  keywords = {rl},
  primaryClass = {cs}
}

@article{adam2018,
  title = {Challenges Hindering Memristive Neuromorphic Hardware from Going Mainstream},
  author = {Adam, Gina C. and Khiat, Ali and Prodromakis, Themis},
  year = {2018},
  month = dec,
  volume = {9},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07565-4},
  file = {/Users/x0r/Zotero/storage/4G7R9YF4/Adam et al_2018_Challenges hindering memristive neuromorphic hardware from going mainstream.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {1}
}

@article{agrawal2015,
  title = {Learning to {{See}} by {{Moving}}},
  author = {Agrawal, Pulkit and Carreira, Joao and Malik, Jitendra},
  year = {2015},
  month = may,
  abstract = {The current dominant paradigm for feature learning in computer vision relies on training neural networks for the task of object recognition using millions of hand labelled images. Is it also possible to learn useful features for a diverse set of visual tasks using any other form of supervision ? In biology, living organisms developed the ability of visual perception for the purpose of moving and acting in the world. Drawing inspiration from this observation, in this work we investigate if the awareness of egomotion can be used as a supervisory signal for feature learning. As opposed to the knowledge of class labels, information about egomotion is freely available to mobile agents. We show that using the same number of training images, features learnt using egomotion as supervision compare favourably to features learnt using class-label as supervision on the tasks of scene recognition, object recognition, visual odometry and keypoint matching.},
  archivePrefix = {arXiv},
  eprint = {1505.01596},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/FHJVEHPJ/Agrawal et al_2015_Learning to See by Moving.pdf},
  journal = {arXiv:1505.01596 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{ahn2015,
  title = {Energy-{{Efficient Phase}}-{{Change Memory}} with {{Graphene}} as a {{Thermal Barrier}}},
  author = {Ahn, Chiyui and Fong, Scott W. and Kim, Yongsung and Lee, Seunghyun and Sood, Aditya and Neumann, Christopher M. and Asheghi, Mehdi and Goodson, Kenneth E. and Pop, Eric and Wong, H.-S. Philip},
  year = {2015},
  month = oct,
  volume = {15},
  pages = {6809--6814},
  issn = {1530-6984, 1530-6992},
  doi = {10.1021/acs.nanolett.5b02661},
  abstract = {Phase-change memory (PCM) is an important class of data storage, yet lowering the programming current of individual devices is known to be a significant challenge. Here we improve the energy-efficiency of PCM by placing a graphene layer at the interface between the phase-change material, Ge2Sb2Te5 (GST), and the bottom electrode (W) heater. Graphene-PCM (G-PCM) devices have {$\sim$}40\% lower RESET current compared to control devices without the graphene. This is attributed to the graphene as an added interfacial thermal resistance which helps confine the generated heat inside the active PCM volume. The G-PCM achieves programming up to 105 cycles, and the graphene could further enhance the PCM endurance by limiting atomic migration or material segregation at the bottom electrode interface.},
  file = {/Users/x0r/Zotero/storage/ITW57Y7Z/Ahn et al_2015_Energy-Efficient Phase-Change Memory with Graphene as a Thermal Barrier.pdf},
  journal = {Nano Letters},
  keywords = {modeling,PCM},
  language = {en},
  number = {10}
}

@article{akrout2019,
  title = {Deep {{Learning}} without {{Weight Transport}}},
  author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter C. and Lillicrap, Timothy and Tweed, Douglas},
  year = {2019},
  month = apr,
  abstract = {Current algorithms for deep learning probably cannot run in the brain because they rely on weight transport, where forward-path neurons transmit their synaptic weights to a feedback path, in a way that is likely impossible biologically. An algorithm called feedback alignment achieves deep learning without weight transport by using random feedback weights, but it performs poorly on hard visual-recognition tasks. Here we describe two mechanisms - a neural circuit called a weight mirror and a modification of an algorithm proposed by Kolen and Pollack in 1994 - both of which let the feedback path learn appropriate synaptic weights quickly and accurately even in large networks, without weight transport or complex wiring.Tested on the ImageNet visual-recognition task, these mechanisms outperform both feedback alignment and the newer sign-symmetry method, and nearly match backprop, the standard algorithm of deep learning, which uses weight transport.},
  archivePrefix = {arXiv},
  eprint = {1904.05391},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NFBJL2FT/Akrout et al_2019_Deep Learning without Weight Transport.pdf},
  journal = {arXiv:1904.05391 [cs, stat]},
  keywords = {neuroscience,To read},
  primaryClass = {cs, stat}
}

@article{alber2018,
  title = {Backprop {{Evolution}}},
  author = {Alber, Maximilian and Bello, Irwan and Zoph, Barret and Kindermans, Pieter-Jan and Ramachandran, Prajit and Le, Quoc},
  year = {2018},
  month = aug,
  abstract = {The back-propagation algorithm is the cornerstone of deep learning. Despite its importance, few variations of the algorithm have been attempted. This work presents an approach to discover new variations of the back-propagation equation. We use a domain specific language to describe update equations as a list of primitive functions. An evolution-based method is used to discover new propagation rules that maximize the generalization performance after a few epochs of training. We find several update equations that can train faster with short training times than standard back-propagation, and perform similar as standard back-propagation at convergence.},
  archivePrefix = {arXiv},
  eprint = {1808.02822},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/MAUPU4PJ/Alber et al_2018_Backprop Evolution.pdf},
  journal = {arXiv:1808.02822 [cs, stat]},
  keywords = {backprop},
  language = {en},
  primaryClass = {cs, stat}
}

@article{alemi2017a,
  title = {Learning Arbitrary Dynamics in Efficient, Balanced Spiking Networks Using Local Plasticity Rules},
  author = {Alemi, Alireza and Machens, Christian and Den{\`e}ve, Sophie and Slotine, Jean-Jacques},
  year = {2017},
  month = aug,
  abstract = {Understanding how recurrent neural circuits can learn to implement dynamical systems is a fundamental challenge in neuroscience. The credit assignment problem, i.e. determining the local contribution of each synapse to the network's global output error, is a major obstacle in deriving biologically plausible local learning rules. Moreover, spiking recurrent networks implementing such tasks should not be hugely costly in terms of number of neurons and spikes, as they often are when adapted from rate models. Finally, these networks should be robust to noise and neural deaths in order to sustain these representations in the face of such naturally occurring perturbation. We approach this problem by fusing the theory of efficient, balanced spiking networks (EBN) with nonlinear adaptive control theory. Local learning rules are ensured by feeding back into the network its own error, resulting in a synaptic plasticity rule depending solely on presynaptic inputs and post-synaptic feedback. The spiking efficiency and robustness of the network are guaranteed by maintaining a tight excitatory/inhibitory balance, ensuring that each spike represents a local projection of the global output error and minimizes a loss function. The resulting networks can learn to implement complex dynamics with very small numbers of neurons and spikes, exhibit the same spike train variability as observed experimentally, and are extremely robust to noise and neuronal loss.},
  archivePrefix = {arXiv},
  eprint = {1705.08026},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NCVQYGD5/Alemi et al. - 2017 - Learning arbitrary dynamics in efficient, balanced.pdf},
  journal = {arXiv:1705.08026 [q-bio]},
  language = {en},
  primaryClass = {q-bio}
}

@article{alexander2016,
  title = {Strategic {{Attentive Writer}} for {{Learning Macro}}-{{Actions}}},
  author = {Alexander and Vezhnevets and Mnih, Volodymyr and Agapiou, John and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  abstract = {We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner by purely interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub- sequences by learning for how long the plan can be committed to \textendash{} i.e. followed without re-planing. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro- actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macroactions), demonstrating the generality of the approach.},
  archivePrefix = {arXiv},
  eprint = {1606.04695},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/CZ7UKBYC/Alexander et al_2016_Strategic Attentive Writer for Learning Macro-Actions.pdf},
  journal = {arXiv:1606.04695 [cs]},
  keywords = {attention,rl},
  language = {en},
  primaryClass = {cs}
}

@book{alexashenko,
  title = {Cortical {{Circuits}}},
  author = {Alexashenko, Sergey},
  file = {/Users/x0r/Zotero/storage/7SM7UJP4/Alexashenko - Figures are by the author, unless otherwise attrib.pdf},
  language = {en}
}

@article{almasi2016,
  title = {Review of Advances in Neural Networks: {{Neural}} Design Technology Stack},
  shorttitle = {Review of Advances in Neural Networks},
  author = {Alm{\'a}si, Adela-Diana and Wo{\'z}niak, Stanis{\l}aw and Cristea, Valentin and Leblebici, Yusuf and Engbersen, Ton},
  year = {2016},
  month = jan,
  volume = {174},
  pages = {31--41},
  issn = {09252312},
  doi = {10.1016/j.neucom.2015.02.092},
  abstract = {This review provides a high-level synthesis of significant recent advances in artificial neural network research, as well as multi-disciplinary concepts connected to the far-reaching goal of obtaining intelligent systems. We assume that a global outlook of these interconnected fields can benefit researchers by providing alternative viewpoints. Therefore, we present different network and neuron models, we discuss model parameters and the means to obtain them, and we draw a quick outline of information encoding, before proceeding to an overview of the relevant learning mechanisms, ranging from established approaches to novel ideas. We specifically focus on comparing the classical artificial model with the biologically-feasible spiking neuron, and we take this comparison further into a discussion on the biological plausibility of various learning approaches.},
  file = {/Users/x0r/Zotero/storage/IN9ZV43S/Almási et al_2016_Review of advances in neural networks.pdf},
  journal = {Neurocomputing},
  keywords = {dl},
  language = {en}
}

@article{aluguri2016,
  title = {Overview of {{Selector Devices}} for 3-{{D Stackable Cross Point RRAM Arrays}}},
  author = {Aluguri, Rakesh and Tseng, Tseung-Yuen},
  year = {2016},
  month = sep,
  volume = {4},
  pages = {294--306},
  issn = {2168-6734},
  doi = {10.1109/JEDS.2016.2594190},
  abstract = {Cross point RRAM arrays is the emerging area for future memory devices due to their high density, excellent scalability. Sneak path problem is the main disadvantage of cross point structures which needed to be overcome to produce real devices. Various self-rectifying cells like complementary resistive cell, hybrid RRAM cell, valence modulated conductive oxide RRAM and non-linear resistive memory with tunneling barrier, etc., are proposed to overcome the sneak path problem and to achieve the high density with good on/off ratio. However, it is challenging to fabricate the self-rectifying cells operating at low program/erase voltages with high non-linearity for both read and write operations and exhibiting good retention and endurance characteristics at the same time for a single device. 1S1R devices are more attractive than SRC due to large optimization possibilities to obtain better device performance as they have separate selector cell and memory cell which decouples the control parameters. Various kinds of selector devices like Si-based selector, metal-oxide based selector, threshold switch selector, mixed ionic-electronic conduction selector etc., are under intense research to obtain the best performance cross point memory devices. In this paper, we have briefly discussed about recent progress on various self-rectifying cells and selector devices for obtaining 3-D stackable cross point memory arrays.},
  file = {/Users/x0r/Zotero/storage/4CXD8IW5/Aluguri_Tseng_2016_Overview of Selector Devices for 3-D Stackable Cross Point RRAM Arrays.pdf},
  journal = {IEEE Journal of the Electron Devices Society},
  keywords = {neuromorphic,OTS},
  language = {en},
  number = {5}
}

@article{ambrogio2017,
  title = {Modeling Resistive Switching Materials and Devices across Scales},
  author = {Ambrogio, Stefano and {Magyari-K{\"o}pe}, Blanka and Onofrio, Nicolas and Mahbubul Islam, Md and Duncan, Dan and Nishi, Yoshio and Strachan, Alejandro},
  year = {2017},
  month = dec,
  volume = {39},
  pages = {39--60},
  issn = {1385-3449, 1573-8663},
  doi = {10.1007/s10832-017-0093-y},
  file = {/Users/x0r/Zotero/storage/EQ3PKKW4/Ambrogio et al_2017_Modeling resistive switching materials and devices across scales.pdf},
  journal = {Journal of Electroceramics},
  keywords = {ReRAM,Sungjung},
  language = {en},
  number = {1-4}
}

@article{ambrogio2018,
  ids = {Ambrogio\_etal18},
  title = {Equivalent-Accuracy Accelerated Neural-Network Training Using Analogue Memory},
  author = {Ambrogio, Stefano and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert M. and Boybat, Irem and {di Nolfo}, Carmelo and Sidler, Severin and Giordano, Massimo and Bodini, Martina and Farinha, Nathan C. P. and Killeen, Benjamin and Cheng, Christina and Jaoudi, Yassine and Burr, Geoffrey W.},
  year = {2018},
  month = jun,
  volume = {558},
  pages = {60--67},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0180-5},
  file = {/Users/x0r/Zotero/storage/R27LS3ZX/Ambrogio et al_2018_Equivalent-accuracy accelerated neural-network training using analogue memory.pdf},
  journal = {Nature},
  keywords = {backprop,neuromorphic,PCM},
  language = {en},
  number = {7708}
}

@article{amodei2016,
  title = {Concrete {{Problems}} in {{AI Safety}}},
  author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Man{\'e}, Dan},
  year = {2016},
  month = jun,
  abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
  archivePrefix = {arXiv},
  eprint = {1606.06565},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/U76D33AF/Amodei et al_2016_Concrete Problems in AI Safety.pdf;/Users/x0r/Zotero/storage/T8Z3DW3Z/1606.html},
  journal = {arXiv:1606.06565 [cs]},
  keywords = {safety},
  primaryClass = {cs}
}

@article{andoni2014,
  title = {Learning {{Polynomials}} with {{Neural Networks}}},
  author = {Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},
  year = {2014},
  pages = {9},
  abstract = {We study the effectiveness of learning low degree polynomials using neural networks by the gradient descent method. While neural networks have been shown to have great expressive power, and gradient descent has been widely used in practice for learning neural networks, few theoretical guarantees are known for such methods. In particular, it is well known that gradient descent can get stuck at local minima, even for simple classes of target functions. In this paper, we present several positive theoretical results to support the effectiveness of neural networks. We focus on twolayer neural networks where the bottom layer is a set of non-linear hidden nodes, and the top layer node is a linear function, similar to Barron (1993). First we show that for a randomly initialized neural network with sufficiently many hidden units, the generic gradient descent algorithm learns any low degree polynomial, assuming we initialize the weights randomly. Secondly, we show that if we use complex-valued weights (the target function can still be real), then under suitable conditions, there are no ``robust local minima'': the neural network can always escape a local minimum by performing a random perturbation. This property does not hold for real-valued weights. Thirdly, we discuss whether sparse polynomials can be learned with small neural networks, with the size dependent on the sparsity of the target function.},
  file = {/Users/x0r/Zotero/storage/CPYKGXZD/Andoni et al_2014_Learning Polynomials with Neural Networks.pdf},
  keywords = {dl},
  language = {en}
}

@article{andrychowicz2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = jun,
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archivePrefix = {arXiv},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/TBWJB32X/Andrychowicz et al_2016_Learning to learn by gradient descent by gradient descent.pdf},
  journal = {arXiv:1606.04474 [cs]},
  keywords = {backprop,meta-learning},
  language = {en},
  primaryClass = {cs}
}

@article{andrychowicz2017,
  title = {Hindsight {{Experience Replay}}},
  author = {Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  year = {2017},
  month = jul,
  file = {/Users/x0r/Zotero/storage/M5WHPPXV/Andrychowicz et al_2017_Hindsight Experience Replay.pdf},
  keywords = {rl,spinning-up},
  language = {en}
}

@article{ankit2019,
  title = {{{PUMA}}: {{A Programmable Ultra}}-Efficient {{Memristor}}-Based {{Accelerator}} for {{Machine Learning Inference}}},
  shorttitle = {{{PUMA}}},
  author = {Ankit, Aayush and Hajj, Izzat El and Chalamalasetti, Sai Rahul and Ndu, Geoffrey and Foltin, Martin and Williams, R. Stanley and Faraboschi, Paolo and Hwu, Wen-mei and Strachan, John Paul and Roy, Kaushik and Milojicic, Dejan S.},
  year = {2019},
  month = jan,
  abstract = {Memristor crossbars are circuits capable of performing analog matrix-vector multiplications, overcoming the fundamental energy efficiency limitations of digital logic. They have been shown to be effective in special-purpose accelerators for a limited set of neural network applications.},
  archivePrefix = {arXiv},
  eprint = {1901.10351},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NP75MBSR/Ankit et al. - 2019 - PUMA A Programmable Ultra-efficient Memristor-bas.pdf},
  journal = {arXiv:1901.10351 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{anumanchipalli2019,
  title = {Speech Synthesis from Neural Decoding of Spoken Sentences},
  author = {Anumanchipalli, Gopala K. and Chartier, Josh and Chang, Edward F.},
  year = {2019},
  month = apr,
  volume = {568},
  pages = {493},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1119-1},
  abstract = {A neural decoder uses kinematic and sound representations encoded in human cortical activity to synthesize audible sentences, which are readily identified and transcribed by listeners.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {/Users/x0r/Zotero/storage/4S4SIUED/Anumanchipalli et al_2019_Speech synthesis from neural decoding of spoken sentences.pdf},
  journal = {Nature},
  keywords = {DNN,EEG},
  language = {En},
  number = {7753}
}

@article{ardila2019,
  title = {End-to-End Lung Cancer Screening with Three-Dimensional Deep Learning on Low-Dose Chest Computed Tomography},
  author = {Ardila, Diego and Kiraly, Atilla P. and Bharadwaj, Sujeeth and Choi, Bokyung and Reicher, Joshua J. and Peng, Lily and Tse, Daniel and Etemadi, Mozziyar and Ye, Wenxing and Corrado, Greg and Naidich, David P. and Shetty, Shravya},
  year = {2019},
  month = may,
  pages = {1},
  issn = {1546-170X},
  doi = {10.1038/s41591-019-0447-x},
  abstract = {A convolutional neural network performs automated prediction of malignancy risk of pulmonary nodules in chest CT scan volumes and improves accuracy of lung cancer screening.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  journal = {Nature Medicine},
  keywords = {dl,medical},
  language = {En}
}

@article{arjovsky2015,
  title = {Unitary {{Evolution Recurrent Neural Networks}}},
  author = {Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  year = {2015},
  month = nov,
  abstract = {Recurrent neural networks (RNNs) are notoriously difficult to train. When the eigenvalues of the hidden to hidden weight matrix deviate from absolute value 1, optimization becomes difficult due to the well studied issue of vanishing and exploding gradients, especially when trying to learn long-term dependencies. To circumvent this problem, we propose a new architecture that learns a unitary weight matrix, with eigenvalues of absolute value exactly 1. The challenge we address is that of parametrizing unitary matrices in a way that does not require expensive computations (such as eigendecomposition) after each weight update. We construct an expressive unitary weight matrix by composing several structured matrices that act as building blocks with parameters to be learned. Optimization with this parameterization becomes feasible only when considering hidden states in the complex domain. We demonstrate the potential of this architecture by achieving state of the art results in several hard tasks involving very long-term dependencies.},
  archivePrefix = {arXiv},
  eprint = {1511.06464},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VGG4KU3K/Arjovsky et al_2015_Unitary Evolution Recurrent Neural Networks.pdf},
  journal = {arXiv:1511.06464 [cs, stat]},
  keywords = {RNN},
  language = {en},
  primaryClass = {cs, stat}
}

@article{arulkumaran2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  volume = {34},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/S7LTSA3G/Arulkumaran et al_2017_A Brief Survey of Deep Reinforcement Learning.pdf},
  journal = {IEEE Signal Processing Magazine},
  keywords = {rl,spinning-up,survey},
  language = {en},
  number = {6}
}

@article{arumugam2019,
  title = {Deep {{Reinforcement Learning}} from {{Policy}}-{{Dependent Human Feedback}}},
  author = {Arumugam, Dilip and Lee, Jun Ki and Saskin, Sophie and Littman, Michael L.},
  year = {2019},
  month = feb,
  abstract = {To widen their accessibility and increase their utility, intelligent agents must be able to learn complex behaviors as specified by (non-expert) human users. Moreover, they will need to learn these behaviors within a reasonable amount of time while efficiently leveraging the sparse feedback a human trainer is capable of providing. Recent work has shown that human feedback can be characterized as a critique of an agent's current behavior rather than as an alternative reward signal to be maximized, culminating in the COnvergent Actor-Critic by Humans (COACH) algorithm for making direct policy updates based on human feedback. Our work builds on COACH, moving to a setting where the agent's policy is represented by a deep neural network. We employ a series of modifications on top of the original COACH algorithm that are critical for successfully learning behaviors from high-dimensional observations, while also satisfying the constraint of obtaining reduced sample complexity. We demonstrate the effectiveness of our Deep COACH algorithm in the rich 3D world of Minecraft with an agent that learns to complete tasks by mapping from raw pixels to actions using only real-time human feedback in 10\textendash 15 minutes of interaction.},
  archivePrefix = {arXiv},
  eprint = {1902.04257},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/6VKZY3U2/Arumugam et al_2019_Deep Reinforcement Learning from Policy-Dependent Human Feedback.pdf},
  journal = {arXiv:1902.04257 [cs, stat]},
  keywords = {AGI,rl},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{athmanathan2015,
  title = {A Finite-Element Thermoelectric Model for Phase-Change Memory Devices},
  author = {Athmanathan, Aravinthan and Krebs, Daniel and Sebastian, Abu and Le Gallo, Manuel and Pozidis, Haralampos and Eleftheriou, Evangelos},
  year = {2015},
  month = sep,
  pages = {289--292},
  publisher = {{IEEE}},
  doi = {10.1109/SISPAD.2015.7292316},
  abstract = {In this article, we present a finite-element method (FEM)-based thermo-electric model to accurately capture the characteristics of phase-change memory devices. The individual thermoelectric heating components are separated to obtain a detailed understanding of thermal transport in the device. This work is different from other exciting modeling work on thermoelectrics in that, for the first time, it compares the modeling results with experimental measurements obtained over a range of ambient temperatures, thereby validating the accuracy of the proposed model.},
  file = {/Users/x0r/Zotero/storage/9YJTSZTI/Athmanathan et al_2015_A finite-element thermoelectric model for phase-change memory devices.pdf},
  isbn = {978-1-4673-7858-1 978-1-4673-7860-4},
  keywords = {modeling,PCM},
  language = {en}
}

@article{azar2019,
  title = {World {{Discovery Models}}},
  author = {Azar, Mohammad Gheshlaghi and Piot, Bilal and Pires, Bernardo Avila and Grill, Jean-Bastian and Altch{\'e}, Florent and Munos, R{\'e}mi},
  year = {2019},
  month = feb,
  abstract = {As humans we are driven by a strong desire for seeking novelty in our world. Also upon observing a novel pattern we are capable of refining our understanding of the world based on the new information---humans can discover their world. The outstanding ability of the human mind for discovery has led to many breakthroughs in science, art and technology. Here we investigate the possibility of building an agent capable of discovering its world using the modern AI technology. In particular we introduce NDIGO, Neural Differential Information Gain Optimisation, a self-supervised discovery model that aims at seeking new information to construct a global view of its world from partial and noisy observations. Our experiments on some controlled 2-D navigation tasks show that NDIGO outperforms state-of-the-art information-seeking methods in terms of the quality of the learned representation. The improvement in performance is particularly significant in the presence of white or structured noise where other information-seeking methods follow the noise instead of discovering their world.},
  archivePrefix = {arXiv},
  eprint = {1902.07685},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/XALLABVA/Azar et al_2019_World Discovery Models.pdf},
  journal = {arXiv:1902.07685 [cs, stat]},
  keywords = {AGI},
  primaryClass = {cs, stat}
}

@article{ba2016,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
  year = {2016},
  month = dec,
  abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These ``fast weights'' can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
  archivePrefix = {arXiv},
  eprint = {1610.06258},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/SXIKN2SH/Ba et al. - 2016 - Using Fast Weights to Attend to the Recent Past.pdf},
  journal = {arXiv:1610.06258 [cs, stat]},
  keywords = {backprop,dl,To read},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bader2005,
  title = {Dimensions of {{Neural}}-Symbolic {{Integration}} - {{A Structured Survey}}},
  author = {Bader, Sebastian and Hitzler, Pascal},
  year = {2005},
  month = nov,
  abstract = {Research on integrated neural-symbolic systems has made significant progress in the recent past. In particular the understanding of ways to deal with symbolic knowledge within connectionist systems (also called artificial neural networks) has reached a critical mass which enables the community to strive for applicable implementations and use cases. Recent work has covered a great variety of logics used in artificial intelligence and provides a multitude of techniques for dealing with them within the context of artificial neural networks. We present a comprehensive survey of the field of neural-symbolic integration, including a new classification of system according to their architectures and abilities.},
  archivePrefix = {arXiv},
  eprint = {cs/0511042},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ADDJIKLM/Bader_Hitzler_2005_Dimensions of Neural-symbolic Integration - A Structured Survey.pdf},
  journal = {arXiv:cs/0511042},
  keywords = {AGI},
  language = {en}
}

@inproceedings{baek2020,
  title = {A {{Multi}}-{{Neural Network Acceleration Architecture}}},
  booktitle = {2020 {{ACM}}/{{IEEE}} 47th {{Annual International Symposium}} on {{Computer Architecture}} ({{ISCA}})},
  author = {Baek, Eunjin and Kwon, Dongup and Kim, Jangwoo},
  year = {2020},
  publisher = {{IEEE}},
  address = {{Los Angeles, CA}},
  doi = {10.1109/ISCA45697.2020.00081},
  abstract = {A cost-effective multi-tenant neural network execution is becoming one of the most important design goals for modern neural network accelerators. For example, as emerging AI services consist of many heterogeneous neural network executions, a cloud provider wants to serve a large number of clients using a single AI accelerator for improving its cost effectiveness. Therefore, an ideal next-generation neural network accelerator should support a simultaneous multi-neural network execution, while fully utilizing its hardware resources. However, existing accelerators which are optimized for a single neural network execution can suffer from severe resource underutilization when running multiple neural networks, mainly due to the load imbalance between computation and memory-access tasks from different neural networks.},
  file = {/Users/x0r/Zotero/storage/7ZZ56LVB/2018 - [No title found].pdf},
  isbn = {978-1-5386-5984-7},
  language = {en}
}

@incollection{baez2010,
  title = {Physics, {{Topology}}, {{Logic}} and {{Computation}}: {{A Rosetta Stone}}},
  shorttitle = {Physics, {{Topology}}, {{Logic}} and {{Computation}}},
  booktitle = {New {{Structures}} for {{Physics}}},
  author = {Baez, J. and Stay, M.},
  editor = {Coecke, Bob},
  year = {2010},
  volume = {813},
  pages = {95--172},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-12821-9_2},
  abstract = {In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology. Namely, a linear operator behaves very much like a `cobordism': a manifold representing spacetime, going between two manifolds representing space. This led to a burst of work on topological quantum field theory and `quantum topology'. But this was just the beginning: similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of `closed symmetric monoidal category'. We assume no prior knowledge of category theory, proof theory or computer science.},
  file = {/Users/x0r/Zotero/storage/TLGTV2YK/Baez_Stay_2010_Physics, Topology, Logic and Computation.pdf},
  isbn = {978-3-642-12820-2 978-3-642-12821-9},
  language = {en}
}

@misc{baez2018,
  title = {Struggles with the {{Continuum}}},
  author = {Baez, J.},
  year = {2018},
  file = {/Users/x0r/Zotero/storage/VWUIA2DL/Baez_2018_Struggles with the Continuum.pdf}
}

@article{Bahdanau_etal14,
  ids = {bahdanau2014},
  title = {Neural Machine Translation by Jointly Learning to Align and Translate},
  author = {Bahdanau, D. and Cho, K. and Bengio, Y.},
  year = {2014},
  month = sep,
  arxiv = {[object Object]},
  file = {/Users/x0r/Zotero/storage/UFS7HAHA/Bahdanau et al_2014_Neural Machine Translation by Jointly Learning to Align and Translate.pdf},
  journal = {ArXiv e-prints},
  keywords = {Computer Science - Computation and Language,Computer Science - Learning,Computer Science - Neural and Evolutionary Computing,NMT,Statistics - Machine Learning}
}

@article{bahdanau2018,
  title = {Learning to {{Understand Goal Specifications}} by {{Modelling Reward}}},
  author = {Bahdanau, Dzmitry and Hill, Felix and Leike, Jan and Hughes, Edward and Kohli, Pushmeet and Grefenstette, Edward},
  year = {2018},
  month = sep,
  abstract = {Recent work has shown that deep reinforcement-learning agents can learn to follow language-like instructions from infrequent environment rewards. However, this places on environment designers the...},
  file = {/Users/x0r/Zotero/storage/G3UBLG8N/Bahdanau et al_2018_Learning to Understand Goal Specifications by Modelling Reward.pdf},
  keywords = {AGI,rl}
}

@article{baker,
  title = {Emergent {{Tool Use From Multi}}-{{Agent Autocurrila}}},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi},
  pages = {28},
  abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a selfsupervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  file = {/Users/x0r/Zotero/storage/Y3A5KF5X/Baker et al. - EMERGENT TOOL USE FROM MULTI-AGENT AUTOCURRICULA.pdf},
  keywords = {AGI,rl},
  language = {en}
}

@article{baker2019,
  title = {Emergent {{Tool Use From Multi}}-{{Agent Autocurricula}}},
  author = {Baker, Bowen and Kanitscheider, Ingmar and Markov, Todor and Wu, Yi and Powell, Glenn and McGrew, Bob and Mordatch, Igor},
  year = {2019},
  month = sep,
  abstract = {Through multi-agent competition, the simple objective of hide-and-seek, and standard reinforcement learning algorithms at scale, we find that agents create a self-supervised autocurriculum inducing multiple distinct rounds of emergent strategy, many of which require sophisticated tool use and coordination. We find clear evidence of six emergent phases in agent strategy in our environment, each of which creates a new pressure for the opposing team to adapt; for instance, agents learn to build multi-object shelters using moveable boxes which in turn leads to agents discovering that they can overcome obstacles using ramps. We further provide evidence that multi-agent competition may scale better with increasing environment complexity and leads to behavior that centers around far more human-relevant skills than other self-supervised reinforcement learning methods such as intrinsic motivation. Finally, we propose transfer and fine-tuning as a way to quantitatively evaluate targeted capabilities, and we compare hide-and-seek agents to both intrinsic motivation and random initialization baselines in a suite of domain-specific intelligence tests.},
  archivePrefix = {arXiv},
  eprint = {1909.07528},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9NLXZKAE/Baker et al_2019_Emergent Tool Use From Multi-Agent Autocurricula.pdf;/Users/x0r/Zotero/storage/CMUXGXN6/1909.html},
  journal = {arXiv:1909.07528 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ballinger2018,
  title = {{{DeepHeart}}: {{Semi}}-{{Supervised Sequence Learning}} for {{Cardiovascular Risk Prediction}}},
  author = {Ballinger, Bradon and Hsieh, Johnson and Singh, Avesh and Sohoni, Nimit and Wang, Jack and Maguire, Carol and Marcus, Gregory M and Olgin, Jeffrey E and Sanchez, Jose M and Tison, Geoffrey H and Pletcher, Mark J},
  year = {2018},
  pages = {8},
  abstract = {We train and validate a semi-supervised, multi-task LSTM on 57,675 person-weeks of data from off-the-shelf wearable heart rate sensors, showing high accuracy at detecting multiple medical conditions, including diabetes (0.8451), high cholesterol (0.7441), high blood pressure (0.8086), and sleep apnea (0.8298). We compare two semi-supervised training methods, semi-supervised sequence learning and heuristic pretraining, and show they outperform hand-engineered biomarkers from the medical literature. We believe our work suggests a new approach to patient risk stratification based on cardiovascular risk scores derived from popular wearables such as Fitbit, Apple Watch, or Android Wear.},
  file = {/Users/x0r/Zotero/storage/LG4IR7C5/Ballinger et al_2018_DeepHeart.pdf},
  keywords = {application,dl,health},
  language = {en}
}

@article{banino2018,
  title = {Vector-Based Navigation Using Grid-like Representations in Artificial Agents},
  author = {Banino, Andrea and Barry, Caswell and Uria, Benigno and Blundell, Charles and Lillicrap, Timothy and Mirowski, Piotr and Pritzel, Alexander and Chadwick, Martin J. and Degris, Thomas and Modayil, Joseph and Wayne, Greg and Soyer, Hubert and Viola, Fabio and Zhang, Brian and Goroshin, Ross and Rabinowitz, Neil and Pascanu, Razvan and Beattie, Charlie and Petersen, Stig and Sadik, Amir and Gaffney, Stephen and King, Helen and Kavukcuoglu, Koray and Hassabis, Demis and Hadsell, Raia and Kumaran, Dharshan},
  year = {2018},
  month = may,
  volume = {557},
  pages = {429--433},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0102-6},
  file = {/Users/x0r/Zotero/storage/49RX2A3B/Banino et al. - 2018 - Vector-based navigation using grid-like representa.pdf},
  journal = {Nature},
  keywords = {hippocampus,neuroscience},
  language = {en},
  number = {7705}
}

@article{barmak2010,
  title = {A {{Commentary}} on: ``{{Reaction Kinetics}} in {{Processes}} of {{Nucleation}} and {{Growth}}''*},
  shorttitle = {A {{Commentary}} On},
  author = {Barmak, Katayun},
  year = {2010},
  month = nov,
  volume = {41},
  pages = {2711--2775},
  issn = {1073-5623, 1543-1940},
  doi = {10.1007/s11661-010-0421-1},
  file = {/Users/x0r/Zotero/storage/2HAAD96P/Barmak_2010_A Commentary on.pdf;/Users/x0r/Zotero/storage/CZ4HESN3/Barmak_2010_A Commentary on.pdf},
  journal = {Metallurgical and Materials Transactions A},
  keywords = {GST},
  language = {en},
  number = {11}
}

@article{barrett2018,
  title = {Measuring Abstract Reasoning in Neural Networks},
  author = {Barrett, David G T and Hill, Felix and Santoro, Adam and Morcos, Ari S and Lillicrap, Timothy},
  year = {2018},
  pages = {10},
  abstract = {Whether neural networks can learn abstract reasoning or whether they merely rely on superficial statistics is a topic of recent debate. Here, we propose a dataset and challenge designed to probe abstract reasoning, inspired by a well-known human IQ test. To succeed at this challenge, models must cope with various generalisation `regimes' in which the training and test data differ in clearlydefined ways. We show that popular models such as ResNets perform poorly, even when the training and test sets differ only minimally, and we present a novel architecture, with a structure designed to encourage reasoning, that does significantly better. When we vary the way in which the test questions and training data differ, we find that our model is notably proficient at certain forms of generalisation, but notably weak at others. We further show that the model's ability to generalise improves markedly if it is trained to predict symbolic explanations for its answers. Altogether, we introduce and explore ways to both measure and induce stronger abstract reasoning in neural networks. Our freely-available dataset should motivate further progress in this direction.},
  file = {/Users/x0r/Zotero/storage/2MQUT5LP/Barrett et al_2018_Measuring abstract reasoning in neural networks.pdf},
  keywords = {rl},
  language = {en}
}

@article{barrett2018a,
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  shorttitle = {Analyzing Biological and Artificial Neural Networks},
  author = {Barrett, David G. T. and Morcos, Ari S. and Macke, Jakob H.},
  year = {2018},
  month = oct,
  abstract = {Deep neural networks (DNNs) transform stimuli across multiple processing stages to produce representations that can be used to solve complex tasks, such as object recognition in images. However, a full understanding of how they achieve this remains elusive. The complexity of biological neural networks substantially exceeds the complexity of DNNs, making it even more challenging to understand the representations that they learn. Thus, both machine learning and computational neuroscience are faced with a shared challenge: how can we analyze their representations in order to understand how they solve complex tasks? We review how data-analysis concepts and techniques developed by computational neuroscientists can be useful for analyzing representations in DNNs, and in turn, how recently developed techniques for analysis of DNNs can be useful for understanding representations in biological neural networks. We explore opportunities for synergy between the two fields, such as the use of DNNs as in-silico model systems for neuroscience, and how this synergy can lead to new hypotheses about the operating principles of biological neural networks.},
  archivePrefix = {arXiv},
  eprint = {1810.13373},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/U4KNIFKS/Barrett et al_2018_Analyzing biological and artificial neural networks.pdf},
  journal = {arXiv:1810.13373 [cs, q-bio, stat]},
  keywords = {neuromorphic,neuroscience},
  primaryClass = {cs, q-bio, stat}
}

@article{barrett2019,
  title = {Analyzing Biological and Artificial Neural Networks: Challenges with Opportunities for Synergy?},
  shorttitle = {Analyzing Biological and Artificial Neural Networks},
  author = {Barrett, David GT and Morcos, Ari S and Macke, Jakob H},
  year = {2019},
  month = apr,
  volume = {55},
  pages = {55--64},
  issn = {09594388},
  doi = {10.1016/j.conb.2019.01.007},
  file = {/Users/x0r/Zotero/storage/Z32RB7NS/Barrett et al_2019_Analyzing biological and artificial neural networks.pdf},
  journal = {Current Opinion in Neurobiology},
  keywords = {SNN},
  language = {en}
}

@article{barron2016,
  title = {What Insects Can Tell Us about the Origins of Consciousness},
  author = {Barron, Andrew B. and Klein, Colin},
  year = {2016},
  month = may,
  volume = {113},
  pages = {4900--4908},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1520084113},
  file = {/Users/x0r/Zotero/storage/FC6EQGVK/Barron_Klein_2016_What insects can tell us about the origins of consciousness.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {AGI},
  language = {en},
  number = {18}
}

@article{bartunov2018,
  title = {Assessing the {{Scalability}} of {{Biologically}}-{{Motivated Deep Learning Algorithms}} and {{Architectures}}},
  author = {Bartunov, Sergey and Santoro, Adam and Richards, Blake A. and Marris, Luke and Hinton, Geoffrey E. and Lillicrap, Timothy},
  year = {2018},
  month = jul,
  abstract = {The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets and explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and explore performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.},
  archivePrefix = {arXiv},
  eprint = {1807.04587},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/K5ES92LN/Bartunov et al_2018_Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms.pdf},
  journal = {arXiv:1807.04587 [cs, stat]},
  keywords = {dl,neuromorphic,SNN,To read},
  primaryClass = {cs, stat}
}

@article{bashivan2019,
  title = {Neural Population Control via Deep Image Synthesis},
  author = {Bashivan, Pouya and Kar, Kohitij and DiCarlo, James J.},
  year = {2019},
  month = may,
  volume = {364},
  pages = {eaav9436},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aav9436},
  abstract = {Predicting behavior of visual neurons
To what extent are predictive deep learning models of neural responses useful for generating experimental hypotheses? Bashivan et al. took an artificial neural network built to model the behavior of the target visual system and used it to construct images predicted to either broadly activate large populations of neurons or selectively activate one population while keeping the others unchanged. They then analyzed the effectiveness of these images in producing the desired effects in the macaque visual cortex. The manipulations showed very strong effects and achieved considerable and highly selective influence over the neuronal populations. Using novel and non-naturalistic images, the neural network was shown to reproduce the overall behavior of the animals' neural responses.
Science, this issue p. eaav9436
Structured Abstract
INTRODUCTIONThe pattern of light that strikes the eyes is processed and re-represented via patterns of neural activity in a ``deep'' series of six interconnected cortical brain areas called the ventral visual stream. Visual neuroscience research has revealed that these patterns of neural activity underlie our ability to recognize objects and their relationships in the world. Recent advances have enabled neuroscientists to build ever more precise models of this complex visual processing. Currently, the best such models are particular deep artificial neural network (ANN) models in which each brain area has a corresponding model layer and each brain neuron has a corresponding model neuron. Such models are quite good at predicting the responses of brain neurons, but their contribution to an understanding of primate visual processing remains controversial.
RATIONALEThese ANN models have at least two potential limitations. First, because they aim to be high-fidelity computerized copies of the brain, the total set of computations performed by these models is difficult for humans to comprehend in detail. In that sense, each model seems like a ``black box,'' and it is unclear what form of understanding has been achieved. Second, the generalization ability of these models has been questioned because they have only been tested on visual stimuli that are similar to those used to ``teach'' the models. Our goal was to assess both of these potential limitations through nonhuman primate neurophysiology experiments in a mid-level visual brain area. We sought to answer two questions: (i) Despite these ANN models' opacity to simple ``understanding,'' is the knowledge embedded in them already useful for a potential application (i.e., neural activity control)? (ii) Do these models accurately predict brain responses to novel images?
RESULTSWe conducted several closed-loop neurophysiology experiments: After matching model neurons to each of the recorded brain neural sites, we used the model to synthesize entirely novel ``controller'' images based on the model's implicit knowledge of how the ventral visual stream works. We then presented those images to each subject to test the model's ability to control the subject's neurons. In one test, we asked the model to try to control each brain neuron so strongly as to activate it beyond its typically observed maximal activation level. We found that the model-generated synthetic stimuli successfully drove 68\% of neural sites beyond their naturally observed activation levels (chance level is 1\%). In an even more stringent test, the model revealed that it is capable of selectively controlling an entire neural subpopulation, activating a particular neuron while simultaneously inactivating the other recorded neurons (76\% success rate; chance is 1\%).Next, we used these non-natural synthetic controller images to ask whether the model's ability to predict the brain responses would hold up for these highly novel images. We found that the model was indeed quite accurate, predicting 54\% of the image-evoked patterns of brain response (chance level is 0\%), but it is clearly not yet perfect.
CONCLUSIONEven though the nonlinear computations of deep ANN models of visual processing are difficult to accurately summarize in a few words, they nonetheless provide a shareable way to embed collective knowledge of visual processing, and they can be refined by new knowledge. Our results demonstrate that the currently embedded knowledge already has potential application value (neural control) and that these models can partially generalize outside the world in which they ``grew up.'' Our results also show that these models are not yet perfect and that more accurate ANN models would produce even more precise neural control. Such noninvasive neural control is not only a potentially powerful tool in the hands of neuroscientists but also could lead to a new class of therapeutic applications. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/364/6439/eaav9436/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Collection of images synthesized by a deep neural network model to control the activity of neural populations in primate cortical area V4.We used a deep artificial neural network to control the activity pattern of a population of neurons in cortical area V4 of macaque monkeys by synthesizing visual stimuli that, when applied to the subject's retinae, successfully induced the experimenter-desired neural response patterns.
Particular deep artificial neural networks (ANNs) are today's most accurate models of the primate brain's ventral visual stream. Using an ANN-driven image synthesis method, we found that luminous power patterns (i.e., images) can be applied to primate retinae to predictably push the spiking activity of targeted V4 neural sites beyond naturally occurring levels. This method, although not yet perfect, achieves unprecedented independent control of the activity state of entire populations of V4 neural sites, even those with overlapping receptive fields. These results show how the knowledge embedded in today's ANN models might be used to noninvasively set desired internal brain states at neuron-level resolution, and suggest that more accurate ANN models would produce even more accurate control.
A deep artificial neural network can model primate vision.
A deep artificial neural network can model primate vision.},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/x0r/Zotero/storage/PXMPMUQH/Bashivan et al_2019_Neural population control via deep image synthesis.pdf},
  journal = {Science},
  keywords = {dl,neuroscience,vision},
  language = {en},
  number = {6439},
  pmid = {31048462}
}

@article{bassett2017,
  title = {A {{Network Neuroscience}} of {{Human Learning}}: {{Potential}} to {{Inform Quantitative Theories}} of {{Brain}} and {{Behavior}}},
  shorttitle = {A {{Network Neuroscience}} of {{Human Learning}}},
  author = {Bassett, Danielle S. and Mattar, Marcelo G.},
  year = {2017},
  month = apr,
  volume = {21},
  pages = {250--264},
  issn = {13646613},
  doi = {10.1016/j.tics.2017.01.010},
  file = {/Users/x0r/Zotero/storage/2YKJRVTN/Bassett_Mattar_2017_A Network Neuroscience of Human Learning.pdf;/Users/x0r/Zotero/storage/APSVLACQ/Bassett_Mattar_2017_A Network Neuroscience of Human Learning.pdf},
  journal = {Trends in Cognitive Sciences},
  keywords = {neuroscience},
  language = {en},
  number = {4}
}

@article{bastos2012,
  title = {Canonical Microcircuits for Predictive Coding},
  author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
  year = {2012},
  month = nov,
  volume = {76},
  pages = {695--711},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2012.10.038},
  abstract = {This review considers the influential notion of a canonical (cortical) microcircuit in light of recent theories about neuronal processing. Specifically, we conciliate quantitative studies of microcircuitry and the functional logic of neuronal computations. We revisit the established idea that message passing among hierarchical cortical areas implements a form of Bayesian inference \textendash{} paying careful attention to the implications for intrinsic connections among neuronal populations. By deriving canonical forms for these computations, one can associate specific neuronal populations with specific computational roles. This analysis discloses a remarkable correspondence between the microcircuitry of the cortical column and the connectivity implied by predictive coding. Furthermore, it provides some intuitive insights into the functional asymmetries between feedforward and feedback connections and the characteristic frequencies over which they operate.},
  file = {/Users/x0r/Zotero/storage/LKBXTXZL/Bastos et al. - 2012 - Canonical microcircuits for predictive coding.pdf},
  journal = {Neuron},
  number = {4},
  pmcid = {PMC3777738},
  pmid = {23177956}
}

@article{battaglia2010,
  title = {Thermal Characterization of the {{SiO2}}-{{Ge2Sb2Te5}} Interface from Room Temperature up to 400\textdegree{{C}}},
  author = {Battaglia, J.-L. and Kusiak, A. and Schick, V. and Cappella, A. and Wiemer, C. and Longo, M. and Varesi, E.},
  year = {2010},
  month = feb,
  volume = {107},
  pages = {044314},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.3284084},
  file = {/Users/x0r/Zotero/storage/BGNBNNIU/Battaglia et al_2010_Thermal characterization of the SiO2-Ge2Sb2Te5 interface from room temperature.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling},
  language = {en},
  number = {4}
}

@article{battaglia2016,
  title = {Interaction {{Networks}} for {{Learning}} about {{Objects}}, {{Relations}} and {{Physics}}},
  author = {Battaglia, Peter W. and Pascanu, Razvan and Lai, Matthew and Rezende, Danilo and Kavukcuoglu, Koray},
  year = {2016},
  month = dec,
  abstract = {Reasoning about objects, relations, and physics is central to human intelligence, and a key goal of artificial intelligence. Here we introduce the interaction network, a model which can reason about how objects in complex systems interact, supporting dynamical predictions, as well as inferences about the abstract properties of the system. Our model takes graphs as input, performs object- and relation-centric reasoning in a way that is analogous to a simulation, and is implemented using deep neural networks. We evaluate its ability to reason about several challenging physical domains: n-body problems, rigid-body collision, and non-rigid dynamics. Our results show it can be trained to accurately simulate the physical trajectories of dozens of objects over thousands of time steps, estimate abstract quantities such as energy, and generalize automatically to systems with different numbers and configurations of objects and relations. Our interaction network implementation is the first general-purpose, learnable physics engine, and a powerful general framework for reasoning about object and relations in a wide variety of complex real-world domains.},
  archivePrefix = {arXiv},
  eprint = {1612.00222},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/QMTPNPJQ/Battaglia et al_2016_Interaction Networks for Learning about Objects, Relations and Physics.pdf},
  journal = {arXiv:1612.00222 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{bauer2019,
  title = {Real-{{Time Ultra}}-{{Low Power ECG Anomaly Detection Using}} an {{Event}}-{{Driven Neuromorphic Processor}}},
  author = {Bauer, Felix Christian and Muir, Dylan Richard and Indiveri, Giacomo},
  year = {2019},
  month = dec,
  volume = {13},
  pages = {1575--1582},
  issn = {1932-4545, 1940-9990},
  doi = {10.1109/TBCAS.2019.2953001},
  abstract = {Accurate detection of pathological conditions in human subjects can be achieved through off-line analysis of recorded biological signals such as electrocardiograms (ECGs). However, human diagnosis is time-consuming and expensive, as it requires the time of medical professionals. This is especially inefficient when indicative patterns in the biological signals are infrequent. Moreover, patients with suspected pathologies are often monitored for extended periods, requiring the storage and examination of large amounts of non-pathological data, and entailing a difficult visual search task for diagnosing professionals. In this work we propose a compact and sub-mW low power neural processing system that can be used to perform on-line and real-time preliminary diagnosis of pathological conditions, to raise warnings for the existence of possible pathological conditions, or to trigger an off-line data recording system for further analysis by a medical professional. We apply the system to real-time classification of ECG data for distinguishing between healthy heartbeats and pathological rhythms. Multi-channel analog ECG traces are encoded as asynchronous streams of binary events and processed using a spiking recurrent neural network operated in a reservoir computing paradigm. An event-driven neuron output layer is then trained to recognize one of several pathologies. Finally, the filtered activity of this output layer is used to generate a binary trigger signal indicating the presence or absence of a pathological pattern. We validate the approach proposed using a Dynamic Neuromorphic Asynchronous Processor (DYNAP) chip, implemented using a standard 180 nm CMOS VLSI process, and present experimental results measured from the chip.},
  file = {/Users/x0r/Zotero/storage/TJFAJXBD/Bauer et al. - 2019 - Real-Time Ultra-Low Power ECG Anomaly Detection Us.pdf},
  journal = {IEEE Transactions on Biomedical Circuits and Systems},
  language = {en},
  number = {6}
}

@article{behrens2018,
  title = {What {{Is}} a {{Cognitive Map}}? {{Organizing Knowledge}} for {{Flexible Behavior}}},
  shorttitle = {What {{Is}} a {{Cognitive Map}}?},
  author = {Behrens, Timothy E. J. and Muller, Timothy H. and Whittington, James C. R. and Mark, Shirley and Baram, Alon B. and Stachenfeld, Kimberly L. and {Kurth-Nelson}, Zeb},
  year = {2018},
  month = oct,
  volume = {100},
  pages = {490--509},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2018.10.002},
  abstract = {It is proposed that a cognitive map encoding the relationships between entities in the world supports flexible behavior, but the majority of the neural evidence for such a system comes from studies of spatial navigation. Recent work describing neuronal parallels between spatial and non-spatial behaviors has rekindled the notion of a systematic organization of knowledge across multiple domains. We review experimental evidence and theoretical frameworks that point to principles unifying these apparently disparate functions. These principles describe how to learn and use abstract, generalizable knowledge and suggest that map-like representations observed in a spatial context may be an instance of general coding mechanisms capable of organizing knowledge of all kinds. We highlight how artificial agents endowed with such principles exhibit flexible behavior and learn map-like representations observed in the brain. Finally, we speculate on how these principles may offer insight into the extreme generalizations, abstractions, and inferences that characterize human cognition.},
  file = {/Users/x0r/Zotero/storage/A3RYXCAX/Behrens et al_2018_What Is a Cognitive Map.pdf},
  journal = {Neuron},
  number = {2}
}

@article{bellec2018,
  title = {Long Short-Term Memory and Learning-to-Learn in Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  year = {2018},
  month = mar,
  abstract = {The brain carries out demanding computations and learning processes with recurrent networks of spiking neurons (RSNNs). But computing and learning capabilities of currently available RSNN models have remained poor, especially in comparison with the performance of recurrent networks of artificial neurons, such as Long Short-Term Memory (LSTM) networks. In this article, we investigate whether deep learning can improve RSNN performance. We applied backpropagation through time (BPTT), augmented by biologically inspired heuristics for synaptic rewiring, to RSNNs whose inherent time constants were enriched through simple models for adapting spiking neurons. We found that the resulting RSNNs approximate, for the first time, the computational power of LSTM networks on two common benchmark tasks. Furthermore, our results show that recent successes with applications of Learning-to-Learn (L2L) to LSTM networks can be ported to RSNNs. This opens the door to the investigation of L2L in data-based models for neural networks of the brain, whose activity can -- unlike that of LSTM networks -- be compared directly with recordings from neurons in the brain. In particular, L2L shows that RSNNs can learn large families of non-linear transformations from very few examples, using previously unknown network learning mechanisms. Furthermore, meta-reinforcement learning (meta-RL) shows that LSNNs can learn and execute complex exploration and exploitation strategies.},
  archivePrefix = {arXiv},
  eprint = {1803.09574},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/SX9BRWVR/Bellec et al_2018_Long short-term memory and learning-to-learn in networks of spiking neurons.pdf},
  journal = {arXiv:1803.09574 [cs, q-bio]},
  keywords = {neuromorphic,SNN},
  primaryClass = {cs, q-bio}
}

@techreport{bellec2019,
  title = {A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/738385},
  abstract = {Abstract
          
            Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. But in spite of extensive research, it has remained open how they can learn through synaptic plasticity to carry out complex network computations. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A new mathematical insight tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This new learning method \textendash{} called
            e-prop
            \textendash{} approaches the performance of
            BPTT
            (backpropagation through time), the best known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in novel energy-efficient spike-based hardware for AI.},
  file = {/Users/x0r/Zotero/storage/4HC764EA/Screenshot 2020-07-09 at 16.41.19.png;/Users/x0r/Zotero/storage/9BQSBP7T/media-2.mp4;/Users/x0r/Zotero/storage/CPDFVRZI/media-5.mp4;/Users/x0r/Zotero/storage/DR4Q29A4/media-6.pdf;/Users/x0r/Zotero/storage/F82N5YJK/media-1.mp4;/Users/x0r/Zotero/storage/GXJSHB9R/Screenshot 2020-07-09 at 02.12.01.png;/Users/x0r/Zotero/storage/KN53KYA8/Bellec et al. - 2019 - A solution to the learning dilemma for recurrent n.pdf;/Users/x0r/Zotero/storage/KV3F4XEL/media-3.mp4},
  keywords = {neuromorphic,reservoir-computing,SNN},
  language = {en},
  type = {Preprint}
}

@article{bellec2019b,
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = jan,
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  archivePrefix = {arXiv},
  eprint = {1901.09049},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/W32KEDTG/Bellec et al. - 2019 - Biologically inspired alternatives to backpropagat.pdf},
  journal = {arXiv:1901.09049 [cs]},
  keywords = {neuroscience,SNN,To read},
  language = {en},
  primaryClass = {cs}
}

@article{bellec2020,
  title = {A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons},
  author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {3625},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17236-y},
  file = {/Users/x0r/Zotero/storage/KZDPUALE/Bellec et al. - 2020 - A solution to the learning dilemma for recurrent n.pdf;/Users/x0r/Zotero/storage/Z96B63ZQ/41467_2020_17236_MOESM1_ESM.pdf},
  journal = {Nature Communications},
  keywords = {To read},
  language = {en},
  number = {1}
}

@article{bellemare2015,
  title = {Increasing the {{Action Gap}}: {{New Operators}} for {{Reinforcement Learning}}},
  shorttitle = {Increasing the {{Action Gap}}},
  author = {Bellemare, Marc G. and Ostrovski, Georg and Guez, Arthur and Thomas, Philip S. and Munos, R{\'e}mi},
  year = {2015},
  month = dec,
  abstract = {This paper introduces new optimality-preserving operators on Q-functions. We first describe an operator for tabular representations, the consistent Bellman operator, which incorporates a notion of local policy consistency. We show that this local consistency leads to an increase in the action gap at each state; increasing this gap, we argue, mitigates the undesirable effects of approximation and estimation errors on the induced greedy policies. This operator can also be applied to discretized continuous space and time problems, and we provide empirical results evidencing superior performance in this context. Extending the idea of a locally consistent operator, we then derive sufficient conditions for an operator to preserve optimality, leading to a family of operators which includes our consistent Bellman operator. As corollaries we provide a proof of optimality for Baird's advantage learning algorithm and derive other gap-increasing operators with interesting properties. We conclude with an empirical study on 60 Atari 2600 games illustrating the strong potential of these new operators.},
  archivePrefix = {arXiv},
  eprint = {1512.04860},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/MLFNHY5G/Bellemare et al_2015_Increasing the Action Gap.pdf},
  journal = {arXiv:1512.04860 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{bellemare2016,
  title = {Unifying {{Count}}-{{Based Exploration}} and {{Intrinsic Motivation}}},
  author = {Bellemare, Marc G. and Srinivasan, Sriram and Ostrovski, Georg and Schaul, Tom and Saxton, David and Munos, Remi},
  year = {2016},
  month = jun,
  abstract = {We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.},
  archivePrefix = {arXiv},
  eprint = {1606.01868},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/FXN8THWA/Bellemare et al_2016_Unifying Count-Based Exploration and Intrinsic Motivation.pdf},
  journal = {arXiv:1606.01868 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bellemare2017,
  title = {A {{Distributional Perspective}} on {{Reinforcement Learning}}},
  author = {Bellemare, Marc G. and Dabney, Will and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  abstract = {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman's equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.},
  archivePrefix = {arXiv},
  eprint = {1707.06887},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/UTZZR285/Bellemare et al_2017_A Distributional Perspective on Reinforcement Learning.pdf},
  journal = {arXiv:1707.06887 [cs, stat]},
  keywords = {rl,spinning-up},
  primaryClass = {cs, stat}
}

@article{beltagy2019,
  title = {{{SciBERT}}: {{Pretrained Contextualized Embeddings}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Cohan, Arman and Lo, Kyle},
  year = {2019},
  month = mar,
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained contextualized embedding model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks.},
  archivePrefix = {arXiv},
  eprint = {1903.10676},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/HX9MQ5PS/Beltagy et al_2019_SciBERT.pdf},
  journal = {arXiv:1903.10676 [cs]},
  keywords = {dl},
  primaryClass = {cs}
}

@article{bengio,
  title = {Experiments on the {{Consciousness Prior}}},
  author = {Bengio, Yoshua and Fedus, William},
  pages = {5},
  abstract = {Experiments are proposed to explore a novel prior for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. Experiments on a synthetic dataset are proposed to validate some of the mechanisms proposed to implement the consciousness prior, in the simplest scenario where the consciousness mechanism is only used to make a prediction.},
  file = {/Users/x0r/Zotero/storage/C9WC7RHU/Bengio_Fedus_Experiments on the Consciousness Prior.pdf},
  keywords = {AGI},
  language = {en}
}

@article{berner,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  pages = {66},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  file = {/Users/x0r/Zotero/storage/JW2C92HU/Berner et al. - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf},
  keywords = {To read},
  language = {en}
}

@article{berner2019,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  year = {2019},
  pages = {66},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  file = {/Users/x0r/Zotero/storage/ZIFYY8IR/Berner et al. - Dota 2 with Large Scale Deep Reinforcement Learnin.pdf},
  keywords = {rl},
  language = {en}
}

@inproceedings{bichler2011,
  title = {Unsupervised Features Extraction from Asynchronous Silicon Retina through {{Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  author = {Bichler, Olivier and Querlioz, Damien and Thorpe, Simon J. and Bourgoin, Jean-Philippe and Gamrat, Christian},
  year = {2011},
  month = jul,
  pages = {859--866},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2011.6033311},
  abstract = {In this paper, we present a novel approach to extract complex and overlapping temporally correlated features directly from spike-based dynamic vision sensors. A spiking neural network capable of performing multilayer unsupervised learning through Spike-Timing-Dependent Plasticity is introduced. It shows exceptional performances at detecting cars passing on a freeway recorded with a dynamic vision sensor, after only 10 minutes of fully unsupervised learning. Our methodology is thoroughly explained and first applied to a simpler example of ball trajectory learning. Two unsupervised learning strategies are investigated for advanced features learning. Robustness of our network to synaptic and neuron variability is assessed and virtual immunity to noise and jitter is demonstrated.},
  file = {/Users/x0r/Zotero/storage/I782MDZK/Bichler et al_2011_Unsupervised features extraction from asynchronous silicon retina through.pdf},
  isbn = {978-1-4244-9635-8},
  keywords = {STDP},
  language = {en}
}

@article{bien1988,
  title = {The {{Promise}} of {{Neural Networks}}},
  author = {Bien, Bill},
  year = {1988},
  volume = {76},
  pages = {561--564},
  file = {/Users/x0r/Zotero/storage/7KJHEHK6/The Promise of Neural Networks.pdf},
  journal = {American Scientist},
  language = {en},
  number = {6}
}

@article{bird2017,
  title = {The Role of the Hippocampus in Recognition Memory},
  author = {Bird, Chris M.},
  year = {2017},
  month = aug,
  volume = {93},
  pages = {155--165},
  issn = {00109452},
  doi = {10.1016/j.cortex.2017.05.016},
  abstract = {Many theories of declarative memory propose that it is supported by partially separable processes underpinned by different brain structures. The hippocampus plays a critical role in binding together item and contextual information together and processing the relationships between individual items. By contrast, the processing of individual items and their later recognition can be supported by extrahippocampal regions of the medial temporal lobes (MTL), particularly when recognition is based on feelings of familiarity without the retrieval of any associated information. These theories are domain-general in that ``items'' might be words, faces, objects, scenes, etc. However, there is mixed evidence that item recognition does not require the hippocampus, or that familiarity-based recognition can be supported by extrahippocampal regions. By contrast, there is compelling evidence that in humans, hippocampal damage does not affect recognition memory for unfamiliar faces, whilst recognition memory for several other stimulus classes is impaired. I propose that regions outside of the hippocampus can support recognition of unfamiliar faces because they are perceived as discrete items and have no prior conceptual associations. Conversely, extrahippocampal processes are inadequate for recognition of items which (a) have been previously experienced, (b) are conceptually meaningful, or (c) are perceived as being comprised of individual elements. This account reconciles findings from primate and human studies of recognition memory. Furthermore, it suggests that while the hippocampus is critical for binding and relational processing, these processes are required for item recognition memory in most situations.},
  file = {/Users/x0r/Zotero/storage/Q3K7DCT3/Bird_2017_The role of the hippocampus in recognition memory.pdf},
  journal = {Cortex},
  keywords = {neuroscience},
  language = {en}
}

@book{Bishop06,
  ids = {bishop2006},
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, C.M.},
  year = {2006},
  publisher = {{Springer New York}},
  file = {/Users/x0r/Zotero/storage/ARN4QE9F/Bishop_2006_Pattern recognition and machine learning.pdf},
  keywords = {ml}
}

@article{blackmore2010,
  title = {Minimum-{{Landing}}-{{Error Powered}}-{{Descent Guidance}} for {{Mars Landing Using Convex Optimization}}},
  author = {Blackmore, Lars and Acikmese, Behcet and Scharf, Daniel P.},
  year = {2010},
  month = jul,
  volume = {33},
  pages = {1161--1171},
  issn = {0731-5090, 1533-3884},
  doi = {10.2514/1.47202},
  file = {/Users/x0r/Zotero/storage/6XYUSEGV/Blackmore et al_2010_Minimum-Landing-Error Powered-Descent Guidance for Mars Landing Using Convex.pdf},
  journal = {Journal of Guidance, Control, and Dynamics},
  keywords = {rocket},
  language = {en},
  number = {4}
}

@article{blier2018,
  title = {The {{Description Length}} of {{Deep Learning Models}}},
  author = {Blier, L{\'e}onard and Ollivier, Yann},
  year = {2018},
  month = feb,
  abstract = {Solomonoff's general theory of inference and the Minimum Description Length principle formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks. Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.},
  archivePrefix = {arXiv},
  eprint = {1802.07044},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/R8YRGUXK/Blier_Ollivier_2018_The Description Length of Deep Learning Models.pdf},
  journal = {arXiv:1802.07044 [cs]},
  primaryClass = {cs}
}

@book{blum,
  title = {Foundations of {{Data Science}}},
  author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
  file = {/Users/x0r/Zotero/storage/5AKA4IXU/Blum et al_Foundations of Data Science.pdf}
}

@article{bohnstingl2019,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  author = {Bohnstingl, Thomas and Scherr, Franz and Pehle, Christian and Meier, Karlheinz and Maass, Wolfgang},
  year = {2019},
  volume = {13},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00483},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  file = {/Users/x0r/Zotero/storage/5PX8KW56/Bohnstingl et al_2019_Neuromorphic Hardware Learns to Learn.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {meta-learning,neuromorphic},
  language = {English}
}

@article{bohnstingl2019a,
  title = {Neuromorphic {{Hardware Learns}} to {{Learn}}},
  author = {Bohnstingl, Thomas and Scherr, Franz and Pehle, Christian and Meier, Karlheinz and Maass, Wolfgang},
  year = {2019},
  month = may,
  volume = {13},
  pages = {483},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00483},
  abstract = {Hyperparameters and learning algorithms for neuromorphic hardware are usually chosen by hand to suit a particular task. In contrast, networks of neurons in the brain were optimized through extensive evolutionary and developmental processes to work well on a range of computing and learning tasks. Occasionally this process has been emulated through genetic algorithms, but these require themselves hand-design of their details and tend to provide a limited range of improvements. We employ instead other powerful gradient-free optimization tools, such as cross-entropy methods and evolutionary strategies, in order to port the function of biological optimization processes to neuromorphic hardware. As an example, we show these optimization algorithms enable neuromorphic agents to learn very efficiently from rewards. In particular, meta-plasticity, i.e., the optimization of the learning rule which they use, substantially enhances reward-based learning capability of the hardware. In addition, we demonstrate for the first time Learning-to-Learn benefits from such hardware, in particular, the capability to extract abstract knowledge from prior learning experiences that speeds up the learning of new but related tasks. Learning-to-Learn is especially suited for accelerated neuromorphic hardware, since it makes it feasible to carry out the required very large number of network computations.},
  file = {/Users/x0r/Zotero/storage/6HZ5Q66D/Bohnstingl et al. - 2019 - Neuromorphic Hardware Learns to Learn.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {To read},
  language = {en}
}

@article{bohte,
  title = {{{SpikeProp}}: {{Backpropagation}} for {{Networks}} of {{Spiking Neurons}}},
  author = {Bohte, Sander M and Kok, Joost N},
  pages = {7},
  abstract = {For a network of spiking neurons with reasonable postsynaptic potentials, we derive a supervised learning rule akin to traditional error-back-propagation, SpikeProp and show how to overcome the discontinuities introduced by thresholding. Using this learning algorithm, we demonstrate how networks of spiking neurons with biologically plausible time-constants can perform complex non-linear classification in fast temporal coding just as well as rate-coded networks. When comparing the (implicit) number of neurons required for the respective encodings, it is empirically demonstrated that temporal coding potentially requires significantly less neurons.},
  file = {/Users/x0r/Zotero/storage/2AY4MB5Y/Bohte_Kok_SpikeProp.pdf},
  keywords = {neuromorphic,STDP},
  language = {en}
}

@article{bohte2002,
  title = {Error-Backpropagation in Temporally Encoded Networks of Spiking Neurons},
  author = {Bohte, Sander M. and Kok, Joost N. and La Poutr{\'e}, Han},
  year = {2002},
  month = oct,
  volume = {48},
  pages = {17--37},
  issn = {09252312},
  doi = {10.1016/S0925-2312(01)00658-0},
  abstract = {For a network of spiking neurons that encodes information in the timing of individual spike times, we derive a supervised learning rule, SpikeProp, akin to traditional errorbackpropagation. With this algorithm, we demonstrate how networks of spiking neurons with biologically reasonable action potentials can perform complex non-linear classi\"ycation in fast temporal coding just as well as rate-coded networks. We perform experiments for the classical XOR problem, when posed in a temporal setting, as well as for a number of other benchmark datasets. Comparing the (implicit) number of spiking neurons required for the encoding of the interpolated XOR problem, the trained networks demonstrate that temporal coding is a viable code for fast neural information processing, and as such requires less neurons than instantaneous rate-coding. Furthermore, we \"ynd that reliable temporal computation in the spiking networks was only accomplished when using spike response functions with a time constant longer than the coding interval, as has been predicted by theoretical considerations. c 2002 Elsevier Science B.V. All rights reserved.},
  file = {/Users/x0r/Zotero/storage/Z3RBZWPS/Bohte et al_2002_Error-backpropagation in temporally encoded networks of spiking neurons.pdf},
  journal = {Neurocomputing},
  keywords = {backprop,SNN},
  language = {en},
  number = {1-4}
}

@article{bohtea,
  title = {Efficient {{Spike}}-{{Coding}} with {{Multiplicative Adaptation}} in a {{Spike Response Model}}},
  author = {Bohte, Sander M},
  pages = {9},
  abstract = {Neural adaptation underlies the ability of neurons to maximize encoded information over a wide dynamic range of input stimuli. Recent spiking neuron models like the adaptive Spike Response Model implement adaptation as additive fixed-size fast spike-triggered threshold dynamics and slow spike-triggered currents. Such adaptation accurately models neural spiking behavior over a limited dynamic input range. To extend efficient coding over large changes in dynamic input range, we propose a multiplicative adaptive Spike Response Model where the spike-triggered adaptation dynamics are scaled multiplicatively by the adaptation state at the time of spiking. We show that, unlike the additive adaptation model, the firing rate in our multiplicative adaptation model saturates to a realistic maximum spike-rate regardless of input magnitude. Additionally, when simulating variance switching experiments, the model quantitatively fits experimental data over a wide dynamic range. Dynamic threshold models of adaptation furthermore suggest a straightforward interpretation of neural activity in terms of dynamic differential signal encoding with shifted and weighted exponential kernels. We show that when thus encoding rectified filtered stimulus signals, the multiplicative adaptive Spike Response Model achieves a high coding efficiency and maintains this efficiency over changes in the dynamic signal range of several orders of magnitude, without changing model parameters.},
  file = {/Users/x0r/Zotero/storage/5DJNDKGU/Bohte_Efficient Spike-Coding with Multiplicative Adaptation in a Spike Response Model.pdf},
  keywords = {neuromorphic,SNN},
  language = {en}
}

@article{boniardi2011,
  title = {Physical Origin of the Resistance Drift Exponent in Amorphous Phase Change Materials},
  author = {Boniardi, Mattia and Ielmini, Daniele},
  year = {2011},
  month = jun,
  volume = {98},
  pages = {243506},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.3599559},
  file = {/Users/x0r/Zotero/storage/W2WTHIGE/Boniardi_Ielmini_2011_Physical origin of the resistance drift exponent in amorphous phase change.pdf},
  journal = {Applied Physics Letters},
  keywords = {drift,modeling,PCM},
  language = {en},
  number = {24}
}

@article{borst1999,
  title = {Information Theory and Neural Coding},
  author = {Borst, Alexander and Theunissen, Fr{\'e}d{\'e}ric E.},
  year = {1999},
  month = nov,
  volume = {2},
  pages = {947--957},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/14731},
  file = {/Users/x0r/Zotero/storage/UZGE7CYN/Borst_Theunissen_1999_Information theory and neural coding.pdf},
  journal = {Nature Neuroscience},
  keywords = {neuroscience},
  language = {en},
  number = {11}
}

@inproceedings{boybat2017,
  title = {Stochastic Weight Updates in Phase-Change Memory-Based Synapses and Their Influence on Artificial Neural Networks},
  author = {Boybat, Irem and Le Gallo, Manuel and Moraitis, Timoleon and Leblebici, Yusuf and Sebastian, Abu and Eleftheriou, Evangelos},
  year = {2017},
  month = jun,
  pages = {13--16},
  publisher = {{IEEE}},
  doi = {10.1109/PRIME.2017.7974095},
  abstract = {Artificial neural networks (ANN) have become a powerful tool for machine learning. Resistive memory devices can be used for the realization of a non-von Neumann computational platform for ANN training in an area-efficient way. For instance, the conductance values of phase-change memory (PCM) devices can be used to represent synaptic weights and can be updated in-situ according to learning rules. However, non-ideal device characteristics pose challenges to reach competitive classification accuracies. In this paper, we investigate the impact of granularity and stochasticity associated with the conductance changes on ANN performance. Using a PCM prototype chip fabricated in the 90 nm technology node, we present a detailed experimental characterization of the conductance changes. Simulations are done in order to quantify the effect of the experimentally observed conductance change granularity and stochasticity on classification accuracies in a fully connected ANN trained with backpropagation.},
  file = {/Users/x0r/Zotero/storage/PJV4QEQB/Boybat et al_2017_Stochastic weight updates in phase-change memory-based synapses and their.pdf},
  isbn = {978-1-5090-6508-0},
  keywords = {backprop,GST,neuromorphic},
  language = {en}
}

@article{boybat2018,
  title = {Neuromorphic Computing with Multi-Memristive Synapses},
  author = {Boybat, Irem and Le Gallo, Manuel and Nandakumar, S. R. and Moraitis, Timoleon and Parnell, Thomas and Tuma, Tomas and Rajendran, Bipin and Leblebici, Yusuf and Sebastian, Abu and Eleftheriou, Evangelos},
  year = {2018},
  month = dec,
  volume = {9},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04933-y},
  file = {/Users/x0r/Zotero/storage/4H7DIF47/Boybat et al_2018_Neuromorphic computing with multi-memristive synapses.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {1}
}

@article{Brette_Gerstner05,
  ids = {brette2005},
  title = {Adaptive Exponential Integrate-and-Fire Model as an Effective Description of Neuronal Activity},
  author = {Brette, Romain and Gerstner, Wulfram},
  year = {2005},
  volume = {94},
  pages = {3637--3642},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/K57BM2BT/Brette_Gerstner_2005_Adaptive Exponential Integrate-and-Fire Model as an Effective Description of.pdf},
  journal = {Journal of neurophysiology},
  keywords = {neuroscience,SNN},
  number = {5}
}

@article{brette2015,
  title = {Philosophy of the {{Spike}}: {{Rate}}-{{Based}} vs. {{Spike}}-{{Based Theories}} of the {{Brain}}},
  shorttitle = {Philosophy of the {{Spike}}},
  author = {Brette, Romain},
  year = {2015},
  month = nov,
  volume = {9},
  issn = {1662-5137},
  doi = {10.3389/fnsys.2015.00151},
  abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
  file = {/Users/x0r/Zotero/storage/GETAXP8V/Brette_2015_Philosophy of the Spike.pdf},
  journal = {Frontiers in Systems Neuroscience},
  keywords = {neuroscience,SNN},
  language = {en}
}

@article{brinker2019,
  title = {Deep Learning Outperformed 136 of 157 Dermatologists in a Head-to-Head Dermoscopic Melanoma Image Classification Task},
  author = {Brinker, Titus J. and Hekler, Achim and Enk, Alexander H. and Klode, Joachim and Hauschild, Axel and Berking, Carola and Schilling, Bastian and Haferkamp, Sebastian and Schadendorf, Dirk and {Holland-Letz}, Tim and Utikal, Jochen S. and von Kalle, Christof and {Ludwig-Peitsch}, Wiebke and Sirokay, Judith and Heinzerling, Lucie and Albrecht, Magarete and Baratella, Katharina and Bischof, Lena and Chorti, Eleftheria and Dith, Anna and Drusio, Christina and Giese, Nina and Gratsias, Emmanouil and Griewank, Klaus and Hallasch, Sandra and Hanhart, Zdenka and Herz, Saskia and Hohaus, Katja and Jansen, Philipp and Jockenh{\"o}fer, Finja and Kanaki, Theodora and Knispel, Sarah and Leonhard, Katja and Martaki, Anna and Matei, Liliana and Matull, Johanna and Olischewski, Alexandra and Petri, Maximilian and Placke, Jan-Malte and Raub, Simon and Salva, Katrin and Schlott, Swantje and Sody, Elsa and Steingrube, Nadine and Stoffels, Ingo and Ugurel, Selma and Zaremba, Anne and Gebhardt, Christoffer and Booken, Nina and Christolouka, Maria and {Buder-Bakhaya}, Kristina and {Bokor-Billmann}, Therezia and Enk, Alexander and Gholam, Patrick and H{\"a}n{\ss}le, Holger and Salzmann, Martin and Sch{\"a}fer, Sarah and Sch{\"a}kel, Knut and Schank, Timo and Bohne, Ann-Sophie and Deffaa, Sophia and Drerup, Katharina and Egberts, Friederike and Erkens, Anna-Sophie and Ewald, Benjamin and Falkvoll, Sandra and Gerdes, Sascha and Harde, Viola and Hauschild, Axel and Jost, Marion and Kosova, Katja and Messinger, Laetitia and Metzner, Malte and Morrison, Kirsten and Motamedi, Rogina and Pinczker, Anja and Rosenthal, Anne and Scheller, Natalie and Schwarz, Thomas and St{\"o}lzl, Dora and Thielking, Federieke and Tomaschewski, Elena and Wehkamp, Ulrike and Weichenthal, Michael and Wiedow, Oliver and B{\"a}r, Claudia Maria and {Bender-S{\"a}belkampf}, Sophia and Horbr{\"u}gger, Marc and Karoglan, Ante and Kraas, Luise and Faulhaber, J{\"o}rg and Geraud, Cyrill and Guo, Ze and Koch, Philipp and Linke, Miriam and Maurier, Nolwenn and M{\"u}ller, Verena and Thomas, Benjamin and Utikal, Jochen Sven and Alamri, Ali Saeed M. and Baczako, Andrea and Berking, Carola and Betke, Matthias and Haas, Carolin and Hartmann, Daniela and Heppt, Markus V. and Kilian, Katharina and Krammer, Sebastian and Lapczynski, Natalie Lidia and Mastnik, Sebastian and Nasifoglu, Suzan and Ruini, Cristel and Sattler, Elke and Schlaak, Max and Wolff, Hans and Achatz, Birgit and Bergbreiter, Astrid and Drexler, Konstantin and Ettinger, Monika and Haferkamp, Sebastian and Halupczok, Anna and Hegemann, Marie and Dinauer, Verena and Maagk, Maria and Mickler, Marion and Philipp, Biance and Wilm, Anna and Wittmann, Constanze and Gesierich, Anja and Glutsch, Valerie and Kahlert, Katrin and Kerstan, Andreas and Schilling, Bastian and Schr{\"u}fer, Philipp},
  year = {2019},
  month = may,
  volume = {113},
  pages = {47--54},
  issn = {0959-8049, 1879-0852},
  doi = {10.1016/j.ejca.2019.04.001},
  abstract = {{$<$}h2{$>$}Abstract{$<$}/h2{$><$}h3{$>$}Background{$<$}/h3{$><$}p{$>$}Recent studies have successfully demonstrated the use of deep-learning algorithms for dermatologist-level classification of suspicious lesions by the use of excessive proprietary image databases and limited numbers of dermatologists. For the first time, the performance of a deep-learning algorithm trained by open-source images exclusively is compared to a large number of dermatologists covering all levels within the clinical hierarchy.{$<$}/p{$><$}h3{$>$}Methods{$<$}/h3{$><$}p{$>$}We used methods from enhanced deep learning to train a convolutional neural network (CNN) with 12,378 open-source dermoscopic images. We used 100 images to compare the performance of the CNN to that of the 157 dermatologists from 12 university hospitals in Germany. Outperformance of dermatologists by the deep neural network was measured in terms of sensitivity, specificity and receiver operating characteristics.{$<$}/p{$><$}h3{$>$}Findings{$<$}/h3{$><$}p{$>$}The mean sensitivity and specificity achieved by the dermatologists with dermoscopic images was 74.1\% (range 40.0\%\textendash 100\%) and 60\% (range 21.3\%\textendash 91.3\%), respectively. At a mean sensitivity of 74.1\%, the CNN exhibited a mean specificity of 86.5\% (range 70.8\%\textendash 91.3\%). At a mean specificity of 60\%, a mean sensitivity of 87.5\% (range 80\%\textendash 95\%) was achieved by our algorithm. Among the dermatologists, the chief physicians showed the highest mean specificity of 69.2\% at a mean sensitivity of 73.3\%. With the same high specificity of 69.2\%, the CNN had a mean sensitivity of 84.5\%.{$<$}/p{$><$}h3{$>$}Interpretation{$<$}/h3{$><$}p{$>$}A CNN trained by open-source images exclusively outperformed 136 of the 157 dermatologists and all the different levels of experience (from junior to chief physicians) in terms of average specificity and sensitivity.{$<$}/p{$>$}},
  file = {/Users/x0r/Zotero/storage/35RYP4WS/Brinker et al_2019_Deep learning outperformed 136 of 157 dermatologists in a head-to-head.pdf},
  journal = {European Journal of Cancer},
  keywords = {dl,health},
  language = {English},
  pmid = {30981091}
}

@article{Brivio_etal19,
  ids = {brivio2019},
  title = {Extended Memory Lifetime in Spiking Neural Networks Employing Memristive Synapses with Nonlinear Conductance Dynamics},
  author = {Brivio, Stefano and Conti, Daniele and Nair, Manu V and Frascaroli, Jacopo and Covi, Erika and Ricciardi, Carlo and Indiveri, Giacomo and Spiga, Sabina},
  year = {2019},
  volume = {30},
  pages = {015102},
  abstract = {Spiking neural networks (SNNs) employing memristive synapses are capable of life-long online learning. Because of their ability to process and classify large amounts of data in real-time using compact and low-power electronic systems, they promise a substantial technology breakthrough. However, the critical issue that memristor-based SNNs have to face is the fundamental limitation in their memory capacity due to finite resolution of the synaptic elements, which leads to the replacement of old memories with new ones and to a finite memory lifetime. In this study we demonstrate that the nonlinear conductance dynamics of memristive devices can be exploited to improve the memory lifetime of a network. The network is simulated on the basis of a spiking neuron model of mixed-signal digital-analogue sub-threshold neuromorphic CMOS circuits, and on memristive synapse models derived from the experimental nonlinear conductance dynamics of resistive memory devices when stimulated by trains of identical pulses. The network learning circuits implement a spike-based plasticity rule compatible with both spike-timing and rate-based learning rules. In order to get an insight on the memory lifetime of the network, we analyse the learning dynamics in the context of a classical benchmark of neural network learning, that is hand-written digit classification. In the proposed architecture, the memory lifetime and the performance of the network are improved for memristive synapses with nonlinear dynamics with respect to linear synapses with similar resolution. These results demonstrate the importance of following holistic approaches that combine the study of theoretical learning models with the development of neuromorphic CMOS SNNs with memristive devices used to implement life-long on-chip learning.},
  file = {/Users/x0r/Zotero/storage/VZXYGPI4/Brivio et al_2019_Extended memory lifetime in spiking neural networks employing memristive.pdf},
  journal = {Nanotechnology},
  keywords = {neuromorphic},
  number = {1}
}

@article{brockman2016,
  title = {{{OpenAI Gym}}},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  year = {2016},
  month = jun,
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a growing collection of benchmark problems that expose a common interface, and a website where people can share their results and compare the performance of algorithms. This whitepaper discusses the components of OpenAI Gym and the design decisions that went into the software.},
  archivePrefix = {arXiv},
  eprint = {1606.01540},
  eprinttype = {arxiv},
  file = {/Users/x0r/switchdrive/zotero/Brockman et al_2016_OpenAI Gym.pdf;/Users/x0r/Zotero/storage/VYCM2ZYT/1606.html},
  journal = {arXiv:1606.01540 [cs]},
  primaryClass = {cs}
}

@article{brooks1991,
  title = {Intelligence without Representation},
  author = {Brooks, Rodney A.},
  year = {1991},
  month = jan,
  volume = {47},
  pages = {139--159},
  issn = {00043702},
  doi = {10.1016/0004-3702(91)90053-M},
  abstract = {Brooks, R.A., Intelligence without representation, Artificial Intelligence 47 (1991) 139159. Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporate-everything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environments.},
  file = {/Users/x0r/Zotero/storage/2ITSQBX3/Brooks_1991_Intelligence without representation.pdf},
  journal = {Artificial Intelligence},
  keywords = {AGI,To read},
  language = {en},
  number = {1-3}
}

@article{brunton2013,
  title = {Rats and {{Humans Can Optimally Accumulate Evidence}} for {{Decision}}-{{Making}}},
  author = {Brunton, Bingni W. and Botvinick, Matthew M. and Brody, Carlos D.},
  year = {2013},
  month = apr,
  volume = {340},
  pages = {95--98},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1233912},
  abstract = {The gradual and noisy accumulation of evidence is a fundamental component of decision-making, with noise playing a key role as the source of variability and errors. However, the origins of this noise have never been determined. We developed decision-making tasks in which sensory evidence is delivered in randomly timed pulses, and analyzed the resulting data with models that use the richly detailed information of each trial's pulse timing to distinguish between different decision-making mechanisms. This analysis allowed measurement of the magnitude of noise in the accumulator's memory, separately from noise associated with incoming sensory evidence. In our tasks, the accumulator's memory was noiseless, for both rats and humans. In contrast, the addition of new sensory evidence was the primary source of variability. We suggest our task and modeling approach as a powerful method for revealing internal properties of decision-making processes.},
  file = {/Users/x0r/Zotero/storage/PEZD8U7J/Brunton et al. - 2013 - Rats and Humans Can Optimally Accumulate Evidence .pdf},
  journal = {Science},
  keywords = {To read},
  language = {en},
  number = {6128}
}

@article{Burr_etal17,
  ids = {burr2017},
  title = {Neuromorphic Computing Using Non-Volatile Memory},
  author = {Burr, Geoffrey W and Shelby, Robert M and Sebastian, Abu and Kim, Sangbum and Kim, Seyoung and Sidler, Severin and Virwani, Kumar and Ishii, Masatoshi and Narayanan, Pritish and Fumarola, Alessandro and others},
  year = {2017},
  volume = {2},
  pages = {89--124},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/EPS4JHTF/Burr et al_2017_Neuromorphic computing using non-volatile memory.pdf},
  journal = {Advances in Physics: X},
  keywords = {neuromorphic},
  number = {1}
}

@article{burr2010,
  title = {Phase Change Memory Technology},
  author = {Burr, Geoffrey W. and Breitwisch, Matthew J. and Franceschini, Michele and Garetto, Davide and Gopalakrishnan, Kailash and Jackson, Bryan and Kurdi, Bulent and Lam, Chung and Lastras, Luis A. and Padilla, Alvaro and Rajendran, Bipin and Raoux, Simone and Shenoy, Rohit S.},
  year = {2010},
  month = mar,
  volume = {28},
  pages = {223--262},
  issn = {2166-2746, 2166-2754},
  doi = {10.1116/1.3301579},
  abstract = {We survey the current state of phase change memory (PCM), a non-volatile solid-state memory technology built around the large electrical contrast between the highly-resistive amorphous and highly-conductive crystalline states in so-called phase change materials. PCM technology has made rapid progress in a short time, having passed older technologies in terms of both sophisticated demonstrations of scaling to small device dimensions, as well as integrated large-array demonstrators with impressive retention, endurance, performance and yield characteristics. We introduce the physics behind PCM technology, assess how its characteristics match up with various potential applications across the memory-storage hierarchy, and discuss its strengths including scalability and rapid switching speed. We then address challenges for the technology, including the design of PCM cells for low RESET current, the need to control device-to-device variability, and undesirable changes in the phase change material that can be induced by the fabrication procedure. We then turn to issues related to operation of PCM devices, including retention, device-to-device thermal crosstalk, endurance, and bias-polarity effects. Several factors that can be expected to enhance PCM in the future are addressed, including Multi-Level Cell technology for PCM (which offers higher density through the use of intermediate resistance states), the role of coding, and possible routes to an ultra-high density PCM technology.},
  archivePrefix = {arXiv},
  eprint = {1001.1164},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/SM4PVUYS/Burr et al_2010_Phase change memory technology.pdf},
  journal = {Journal of Vacuum Science \& Technology B, Nanotechnology and Microelectronics: Materials, Processing, Measurement, and Phenomena},
  keywords = {PCM},
  language = {en},
  number = {2}
}

@article{burr2014,
  title = {Access Devices for {{3D}} Crosspoint Memory},
  author = {Burr, Geoffrey W. and Shenoy, Rohit S. and Virwani, Kumar and Narayanan, Pritish and Padilla, Alvaro and Kurdi, B{\"u}lent and Hwang, Hyunsang},
  year = {2014},
  month = jul,
  volume = {32},
  pages = {040802},
  issn = {2166-2746, 2166-2754},
  doi = {10.1116/1.4889999},
  file = {/Users/x0r/Zotero/storage/VUZ4VUZC/Burr et al_2014_Access devices for 3D crosspoint memory.pdf},
  journal = {Journal of Vacuum Science \& Technology B, Nanotechnology and Microelectronics: Materials, Processing, Measurement, and Phenomena},
  keywords = {neuromorphic},
  language = {en},
  number = {4}
}

@inproceedings{burr2015,
  title = {Large-Scale Neural Networks Implemented with Non-Volatile Memory as the Synaptic Weight Element: {{Comparative}} Performance Analysis (Accuracy, Speed, and Power)},
  shorttitle = {Large-Scale Neural Networks Implemented with Non-Volatile Memory as the Synaptic Weight Element},
  author = {Burr, G. W. and Narayanan, P. and Shelby, R. M. and Sidler, S. and Boybat, I. and {di Nolfo}, C. and Leblebici, Y.},
  year = {2015},
  month = dec,
  pages = {4.4.1-4.4.4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2015.7409625},
  abstract = {We review our work towards achieving competitive performance (classification accuracies) for on-chip machine learning (ML) of large-scale artificial neural networks (ANN) using NonVolatile Memory (NVM)-based synapses, despite the inherent random and deterministic imperfections of such devices. We then show that such systems could potentially offer faster (up to 25\texttimes ) and lower-power (from 120\textendash 2850\texttimes ) ML training than GPU\textendash based hardware.},
  file = {/Users/x0r/Zotero/storage/NE4VJ6V8/Burr et al_2015_Large-scale neural networks implemented with non-volatile memory as the.pdf},
  isbn = {978-1-4673-9894-7},
  keywords = {backprop,neuromorphic},
  language = {en}
}

@article{burr2015a,
  title = {Experimental {{Demonstration}} and {{Tolerancing}} of a {{Large}}-{{Scale Neural Network}} (165 000 {{Synapses}}) {{Using Phase}}-{{Change Memory}} as the {{Synaptic Weight Element}}},
  author = {Burr, Geoffrey W. and Shelby, Robert M. and Sidler, Severin and {di Nolfo}, Carmelo and Jang, Junwoo and Boybat, Irem and Shenoy, Rohit S. and Narayanan, Pritish and Virwani, Kumar and Giacometti, Emanuele U. and Kurdi, Bulent N. and Hwang, Hyunsang},
  year = {2015},
  month = nov,
  volume = {62},
  pages = {3498--3507},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2015.2439635},
  abstract = {Using two phase-change memory devices per synapse, a three-layer perceptron network with 164 885 synapses is trained on a subset (5000 examples) of the MNIST database of handwritten digits using a backpropagation variant suitable for nonvolatile memory (NVM) + selector crossbar arrays, obtaining a training (generalization) accuracy of 82.2\% (82.9\%). Using a neural network simulator matched to the experimental demonstrator, extensive tolerancing is performed with respect to NVM variability, yield, and the stochasticity, linearity, and asymmetry of the NVM-conductance response. We show that a bidirectional NVM with a symmetric, linear conductance response of high dynamic range is capable of delivering the same high classification accuracies on this problem as a conventional, software-based implementation of this same network.},
  file = {/Users/x0r/Zotero/storage/AV5VX5TV/Burr et al_2015_Experimental Demonstration and Tolerancing of a Large-Scale Neural Network (165.pdf;/Users/x0r/Zotero/storage/GYYQEU39/Burr et al_2015_Experimental Demonstration and Tolerancing of a Large-Scale Neural Network (165.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {backprop,neuromorphic},
  language = {en},
  number = {11}
}

@article{burr2016,
  title = {Recent {{Progress}} in {{Phase}}-{{Change Memory Technology}}},
  shorttitle = {Recent {{Progress}} in {{Phase}}-{{Change}}},
  author = {Burr, Geoffrey W. and BrightSky, Matthew J. and Sebastian, Abu and Cheng, Huai-Yu and Wu, Jau-Yi and Kim, Sangbum and Sosa, Norma E. and Papandreou, Nikolaos and Lung, Hsiang-Lan and Pozidis, Haralampos and Eleftheriou, Evangelos and Lam, Chung H.},
  year = {2016},
  month = jun,
  volume = {6},
  pages = {146--162},
  issn = {2156-3357, 2156-3365},
  doi = {10.1109/JETCAS.2016.2547718},
  abstract = {We survey progress in the PCM field over the past five years, ranging from large-scale PCM demonstrations to materials improvements for high\textendash temperature retention and faster switching. Both materials and new cell designs that support lower-power switching are discussed, as well as higher reliability for long cycling endurance. Two paths towards higher density are discussed: through 3D integration by the combination of PCM and 3D-capable access devices, and through multiple bits per cell, by understanding and managing resistance drift caused by structural relaxation of the amorphous phase. We also briefly survey work in the nascent field of brain-inspired neuromorphic systems that use PCM to implement non-Von Neumann computing.},
  file = {/Users/x0r/Zotero/storage/W9W5K96L/Burr et al_2016_Recent Progress in Phase-Change Memory Technology.pdf},
  journal = {IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  keywords = {PCM},
  language = {en},
  number = {2}
}

@article{burr2019,
  title = {A Role for Analogue Memory in {{AI}} Hardware},
  author = {Burr, Geoffrey W.},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {10},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0007-y},
  abstract = {Memristor-based chips could lead the way to fast, energy-efficient AI hardware accelerators.},
  copyright = {2019 Springer Nature Limited},
  file = {/Users/x0r/Zotero/storage/TAPNA7T7/Burr_2019_A role for analogue memory in AI hardware.pdf},
  journal = {Nature Machine Intelligence},
  keywords = {neuromorphic},
  language = {En},
  number = {1}
}

@article{cai2019,
  title = {A Fully Integrated Reprogrammable Memristor\textendash{{CMOS}} System for Efficient Multiply\textendash Accumulate Operations},
  author = {Cai, Fuxi and Correll, Justin M. and Lee, Seung Hwan and Lim, Yong and Bothra, Vishishtha and Zhang, Zhengya and Flynn, Michael P. and Lu, Wei D.},
  year = {2019},
  month = jul,
  volume = {2},
  pages = {290--299},
  issn = {2520-1131},
  doi = {10.1038/s41928-019-0270-x},
  file = {/Users/x0r/Zotero/storage/WWVE856Y/Cai et al_2019_A fully integrated reprogrammable memristor–CMOS system for efficient.pdf},
  journal = {Nature Electronics},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {7}
}

@article{caravelli2018,
  title = {Memristors for the {{Curious Outsiders}}},
  author = {Caravelli, Francesco and Carbajal, Juan Pablo},
  year = {2018},
  month = dec,
  abstract = {We present both an overview and a perspective of recent experimental advances and proposed new approaches to performing computation using memristors. A memristor is a 2-terminal passive component with a dynamic resistance depending on an internal parameter. We provide an brief historical introduction, as well as an overview over the physical mechanism that lead to memristive behavior. This review is meant to guide nonpractitioners in the field of memristive circuits and their connection to machine learning and neural computation.},
  archivePrefix = {arXiv},
  eprint = {1812.03389},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/J3QM2FBE/Caravelli_Carbajal_2018_Memristors for the Curious Outsiders.pdf},
  journal = {arXiv:1812.03389 [cond-mat]},
  keywords = {neuromorphic},
  primaryClass = {cond-mat}
}

@article{carette2018,
  title = {Embracing the {{Laws}} of {{Physics}}: {{Three Reversible Models}} of {{Computation}}},
  shorttitle = {Embracing the {{Laws}} of {{Physics}}},
  author = {Carette, Jacques and James, Roshan P. and Sabry, Amr},
  year = {2018},
  month = nov,
  abstract = {Our main models of computation (the Turing Machine and the RAM) make fundamental assumptions about which primitive operations are realizable. The consensus is that these include logical operations like conjunction, disjunction and negation, as well as reading and writing to memory locations. This perspective conforms to a macro-level view of physics and indeed these operations are realizable using macro-level devices involving thousands of electrons. This point of view is however incompatible with quantum mechanics, or even elementary thermodynamics, as both imply that information is a conserved quantity of physical processes, and hence of primitive computational operations. Our aim is to re-develop foundational computational models that embraces the principle of conservation of information. We first define what conservation of information means in a computational setting. We emphasize that computations must be reversible transformations on data. One can think of data as modeled using topological spaces and programs as modeled by reversible deformations. We illustrate this idea using three notions of data. The first assumes unstructured finite data, i.e., discrete topological spaces. The corresponding notion of reversible computation is that of permutations. We then consider a structured notion of data based on the Curry-Howard correspondence; here reversible deformations, as a programming language for witnessing type isomorphisms, comes from proof terms for commutative semirings. We then "move up a level" to treat programs as data. The corresponding notion of reversible programs equivalences comes from the "higher dimensional" analog to commutative semirings: symmetric rig groupoids. The coherence laws for these are exactly the program equivalences we seek. We conclude with some generalizations inspired by homotopy type theory and survey directions for further research.},
  archivePrefix = {arXiv},
  eprint = {1811.03678},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/6ZLEYTP7/Carette et al_2018_Embracing the Laws of Physics.pdf},
  journal = {arXiv:1811.03678 [quant-ph]},
  primaryClass = {quant-ph}
}

@article{carleo2017,
  title = {Solving the Quantum Many-Body Problem with Artificial Neural Networks},
  author = {Carleo, Giuseppe and Troyer, Matthias},
  year = {2017},
  month = feb,
  volume = {355},
  pages = {602--606},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aag2302},
  file = {/Users/x0r/Zotero/storage/J7D8V8YT/Carleo_Troyer_2017_Solving the quantum many-body problem with artificial neural networks.pdf},
  journal = {Science},
  keywords = {dl,quantum},
  language = {en},
  number = {6325}
}

@article{caulfield2017,
  title = {Configurable {{Clouds}}},
  author = {Caulfield, Adrian M. and Chung, Eric S. and Putnam, Andrew and Angepat, Hari and Firestone, Daniel and Fowers, Jeremy and Haselman, Michael and Heil, Stephen and Humphrey, Matt and Kaur, Puneet and Kim, Joo-Young and Lo, Daniel and Massengill, Todd and Ovtcharov, Kalin and Papamichael, Michael and Woods, Lisa and Lanka, Sitaram and Chiou, Derek and Burger, Doug},
  year = {2017},
  volume = {37},
  pages = {52--61},
  issn = {0272-1732},
  doi = {10.1109/MM.2017.51},
  file = {/Users/x0r/Zotero/storage/93YTVR2T/Caulfield et al_2017_Configurable Clouds.pdf},
  journal = {IEEE Micro},
  language = {en},
  number = {3}
}

@article{chaitin2007,
  title = {Algorithmic Information Theory: {{Some}} Recollections},
  shorttitle = {Algorithmic Information Theory},
  author = {Chaitin, G. J.},
  year = {2007},
  month = jan,
  abstract = {Presents a history of the evolution of the author's ideas on program-size complexity and its applications to metamathematics over the course of more than four decades. Includes suggestions for further work.},
  archivePrefix = {arXiv},
  eprint = {math/0701164},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/TEYPIS6N/Chaitin - 2007 - Algorithmic information theory Some recollections.pdf},
  journal = {arXiv:math/0701164},
  keywords = {AGI,AIT,dl,To read},
  language = {en}
}

@article{Chakrabarti_etal17,
  ids = {chakrabarti2017},
  title = {A Multiply-Add Engine with Monolithically Integrated {{3D}} Memristor Crossbar/{{CMOS}} Hybrid Circuit},
  author = {Chakrabarti, Bhaswar and {Lastras-Monta{\~n}o}, Miguel Angel and Adam, Gina and Prezioso, Mirko and Hoskins, Brian and Payvand, M and Madhavan, A and Ghofrani, A and Theogarajan, L and Cheng, K-T and others},
  year = {2017},
  volume = {7},
  pages = {42429},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/SLGMRJDQ/Chakrabarti et al_2017_A multiply-add engine with monolithically integrated 3D memristor crossbar-CMOS.pdf},
  journal = {Scientific Reports},
  keywords = {neuromorphic}
}

@article{chakraborty2018,
  title = {Photonic {{Spiking Neural Networks}} - {{From Devices}} to {{Systems}}},
  author = {Chakraborty, Indranil and Saha, Gobinda and Roy, Kaushik},
  year = {2018},
  month = aug,
  abstract = {Spiking Neural Networks offer an event-driven and biologically realistic alternative to standard Artificial Neural Networks based on analog information processing which make them more suitable for energy-efficient hardware implementations of the functional units of the brain, namely, neurons and synapses. Despite extensive efforts to replicate the energy efficiency of the brain in standard von-Neumann architecture, the massive parallelism has remained elusive. This has led researchers to venture towards beyond von-Neumann computing or `in-memory' computing paradigms based on CMOS and post-CMOS technologies. However, implementations of such platforms in the electrical domain faces potential limitations of switching speed and interconnect losses. Integrated Photonics offers an welcome alternative to standard `microelectronic' platforms and have recently shown promise as a viable technology for spike-based neural processing systems. However, non-volatility has been identified as an important component of large-scale neuromorphic systems. Although the recent demonstrations of ultra-fast computing with phase-change materials (PCMs) show promise, the jump from standalone computing devices to parallel computing architecture is challenging. In this work, we utilize the optical properties of the PCM, Ge\$\_2\$Sb\$\_2\$Te\$\_5\$ (GST), to propose an all-Photonic Spiking Neural Network, comprising of a non-volatile synaptic array integrated seamlessly with previously explored `integrate-and-fire' neurons to realize an `in-memory' computing platform leveraging the inherent parallelism of wavelength-division-multiplexing (WDM). The proposed design not only bridges the gap between isolated computing devices and parallel large-scale implementation, but also paves the way for ultra-fast computing and localized on-chip learning.},
  archivePrefix = {arXiv},
  eprint = {1808.01241},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/K7LMDL26/Chakraborty et al_2018_Photonic Spiking Neural Networks - From Devices to Systems.pdf},
  journal = {arXiv:1808.01241 [cs]},
  primaryClass = {cs}
}

@incollection{chance1999,
  title = {Recurrent {{Cortical Amplification Produces Complex Cell Responses}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 11},
  author = {Chance, Frances S. and Nelson, Sacha B. and Abbott, L. F.},
  editor = {Kearns, M. J. and Solla, S. A. and Cohn, D. A.},
  year = {1999},
  pages = {90--96},
  publisher = {{MIT Press}},
  file = {/Users/x0r/Zotero/storage/BXIHQMY6/Chance et al. - 1999 - Recurrent Cortical Amplification Produces Complex .pdf;/Users/x0r/Zotero/storage/UEISLARC/1526-recurrent-cortical-amplification-produces-complex-cell-responses.html}
}

@article{chang2017,
  title = {Spontaneous Expression of Mirror Self-Recognition in Monkeys after Learning Precise Visual-Proprioceptive Association for Mirror Images},
  author = {Chang, Liangtang and Zhang, Shikun and Poo, Mu-ming and Gong, Neng},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {3258--3263},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1620764114},
  abstract = {Mirror self-recognition (MSR) is generally considered to be an intrinsic cognitive ability found only in humans and a few species of great apes. Rhesus monkeys do not spontaneously show MSR, but they have the ability to use a mirror as an instrument to find hidden objects. The mechanism underlying the transition from simple mirror use to MSR remains unclear. Here we show that rhesus monkeys could show MSR after learning precise visual-proprioceptive association for mirror images. We trained head-fixed monkeys on a chair in front of a mirror to touch with spatiotemporal precision a laser pointer light spot on an adjacent board that could only be seen in the mirror. After several weeks of training, when the same laser pointer light was projected to the monkey's face, a location not used in training, all three trained monkeys successfully touched the face area marked by the light spot in front of a mirror. All trained monkeys passed the standard face mark test for MSR both on the monkey chair and in their home cage. Importantly, distinct from untrained control monkeys, the trained monkeys showed typical mirror-induced self-directed behaviors in their home cage, such as using the mirror to explore normally unseen body parts. Thus, bodily self-consciousness may be a cognitive ability present in many more species than previously thought, and acquisition of precise visual-proprioceptive association for the images in the mirror is critical for revealing the MSR ability of the animal.},
  copyright = {\textcopyright{}  . http://www.pnas.org/site/misc/userlicense.xhtml},
  file = {/Users/x0r/Zotero/storage/SRHBRDL2/Chang et al_2017_Spontaneous expression of mirror self-recognition in monkeys after learning.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {12},
  pmid = {28193875}
}

@article{chang2019,
  title = {Convolutional {{Reservoir Computing}} for {{World Models}}},
  author = {Chang, Hanten and Futagami, Katsuya},
  year = {2019},
  month = jul,
  abstract = {Recently, reinforcement learning models have achieved great success, completing complex tasks such as mastering Go and other games with higher scores than human players. Many of these models collect considerable data on the tasks and improve accuracy by extracting visual and time-series features using convolutional neural networks (CNNs) and recurrent neural networks, respectively. However, these networks have very high computational costs because they need to be trained by repeatedly using a large volume of past playing data. In this study, we propose a novel practical approach called reinforcement learning with convolutional reservoir computing (RCRC) model. The RCRC model has several desirable features: 1. it can extract visual and time-series features very fast because it uses random fixed-weight CNN and the reservoir computing model; 2. it does not require the training data to be stored because it extracts features without training and decides action with evolution strategy. Furthermore, the model achieves state of the art score in the popular reinforcement learning task. Incredibly, we find the random weight-fixed simple networks like only one dense layer network can also reach high score in the RL task.},
  archivePrefix = {arXiv},
  eprint = {1907.08040},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/XYSD7CN8/Chang_Futagami_2019_Convolutional Reservoir Computing for World Models.pdf},
  journal = {arXiv:1907.08040 [cs, stat]},
  keywords = {convnet,neuromorphic,reservoir-computing},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chang2019a,
  title = {{{AI}} Hardware Acceleration with Analog Memory: Micro-Architectures for Low Energy at High Speed},
  shorttitle = {{{AI}} Hardware Acceleration with Analog Memory},
  author = {Chang, Hung-Yang and Narayanan, Pritish and Lewis, Scott C. and Farinha, Nathan C. P. and Hosokawa, Kohji and Mackin, Charles and Tsai, Hsinyu and Ambrogio, Stefano and Chen, An and Burr, Geoffrey William},
  year = {2019},
  pages = {1--1},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/JRD.2019.2934050},
  file = {/Users/x0r/Zotero/storage/R4QSQMRS/Chang et al. - 2019 - AI hardware acceleration with analog memory micro.pdf},
  journal = {IBM Journal of Research and Development},
  keywords = {neuromorphic,To read},
  language = {en}
}

@article{Chanthbouala_etal12,
  ids = {chanthbouala2012},
  title = {A Ferroelectric Memristor},
  author = {Chanthbouala, A. and Garcia, V. and Cherifi, R.O. and Bouzehouane, K. and Fusil, S. and Moya, X. and Xavier, S. and Yamada, H. and Deranlot, C. and Mathur, N.D. and Bibes, M. and Barth{\'e}l{\'e}my, A. and Grollier, J.},
  year = {2012},
  volume = {11},
  pages = {860--864},
  publisher = {{[object Object]}},
  doi = {10.1038/nmat3415},
  file = {/Users/x0r/Zotero/storage/D25N4GEL/Chanthbouala et al_2012_A ferroelectric memristor.pdf},
  journal = {Nature materials},
  keywords = {neuromorphic},
  number = {10}
}

@article{chen2016,
  title = {Size-Dependent and Tunable Crystallization of {{GeSbTe}} Phase-Change Nanoparticles},
  author = {Chen, Bin and {ten Brink}, Gert H. and Palasantzas, George and Kooi, Bart J.},
  year = {2016},
  month = dec,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep39546},
  file = {/Users/x0r/Zotero/storage/82SVAM93/Chen et al_2016_Size-dependent and tunable crystallization of GeSbTe phase-change nanoparticles.pdf},
  journal = {Scientific Reports},
  keywords = {GST},
  language = {en},
  number = {1}
}

@inproceedings{chen2016a,
  title = {Partition {{SRAM}} and {{RRAM}} Based Synaptic Arrays for Neuro-Inspired Computing},
  booktitle = {2016 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Chen, Pai-Yu and Yu, Shimeng},
  year = {2016},
  month = may,
  pages = {2310--2313},
  issn = {2379-447X},
  doi = {10.1109/ISCAS.2016.7539046},
  abstract = {Memory array architectures have been proposed for on-chip acceleration of weighted sum and weight update in the neuro-inspired machine learning algorithms. As the learning algorithms usually operate on a large weight matrix size, an efficient mapping of a large weight matrix on the hardware accelerator may require partitioning the matrix into multiple sub-arrays. In this work, we built a circuit-level macro simulator to evaluate the performance of partitioning a 512\texttimes 512 weight matrix into the SRAM and RRAM based accelerators. Generally, with more partitioning and finer granularity of the array architecture, the read/write latency and the dynamic read/write energy will decrease due to an increased computation parallelism at the expense of larger area and leakage power, as shown in the case of the SRAM accelerator. However, the RRAM accelerator does not improve the read latency and read energy beyond a certain partition point because the overhead due to multiple intermediate stages of adders and registers will dominate.},
  file = {/Users/x0r/Zotero/storage/5FR2DLY6/7539046.html}
}

@article{chen2019,
  title = {Learning to {{Plan}} via {{Neural Exploration}}-{{Exploitation Trees}}},
  author = {Chen, Binghong and Dai, Bo and Song, Le},
  year = {2019},
  month = feb,
  abstract = {Sampling-based algorithms such as RRT and its variants are powerful tools for path planning problems in high-dimensional continuous state and action spaces. While these algorithms perform systematic exploration of the state space, they do not fully exploit past planning experiences from similar environments. In this paper, we design a meta path planning algorithm, called \textbackslash emph\{Neural Exploration-Exploitation Trees\} (NEXT), which can exploit past experience to drastically reduce the sample requirement for solving new path planning problems. More specifically, NEXT contains a novel neural architecture which can learn from experiences the dependency between task structures and promising path search directions. Then this learned prior is integrated with a UCB-type algorithm to achieve an online balance between \textbackslash emph\{exploration\} and \textbackslash emph\{exploitation\} when solving a new problem. Empirically, we show that NEXT can complete the planning tasks with very small searching trees and significantly outperforms previous state-of-the-arts on several benchmark problems.},
  archivePrefix = {arXiv},
  eprint = {1903.00070},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/G6UPZ7VT/Chen et al_2019_Learning to Plan via Neural Exploration-Exploitation Trees.pdf},
  journal = {arXiv:1903.00070 [cs, stat]},
  keywords = {rl},
  primaryClass = {cs, stat}
}

@article{chevalier-boisvert2019,
  title = {{{BABYAI}}: {{A PLATFORM TO STUDY THE SAMPLE EFFI}}- {{CIENCY OF GROUNDED LANGUAGE LEARNING}}},
  author = {{Chevalier-Boisvert}, Maxime and Lahlou, Salem and Nguyen, Thien Huu and Bahdanau, Dzmitry and Willems, Lucas and Saharia, Chitwan and Bengio, Yoshua},
  year = {2019},
  pages = {19},
  abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons. Though, given the lack of sample efficiency in current learning methods, reaching this goal may require substantial research efforts. We introduce the BabyAI research platform, with the goal of supporting investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. Each level gradually leads the agent towards acquiring a combinatorially rich synthetic language, which is a proper subset of English. The platform also provides a hand-crafted bot agent, which simulates a human teacher. We report estimated amount of supervision required for training neural reinforcement and behavioral-cloning agents on some BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample-efficient in the context of learning a language with compositional properties.},
  file = {/Users/x0r/Zotero/storage/27LYT2FL/Chevalier-Boisvert et al. - 2019 - BABYAI A PLATFORM TO STUDY THE SAMPLE EFFI- CIENC.pdf},
  keywords = {To read},
  language = {en}
}

@article{Chicca_etal14,
  ids = {chicca2014},
  title = {Neuromorphic Electronic Circuits for Building Autonomous Cognitive Systems},
  author = {Chicca, E. and Stefanini, F. and Bartolozzi, C. and Indiveri, G.},
  year = {2014},
  month = sep,
  volume = {102},
  pages = {1367--1388},
  issn = {0018-9219},
  arxiv = {[object Object]},
  file = {/Users/x0r/Zotero/storage/BTV4MQ2U/Chicca et al_2014_Neuromorphic electronic circuits for building autonomous cognitive systems.pdf},
  journal = {Proceedings of the IEEE},
  keywords = {Buildings,Cognitive systems,Computational modeling,Computer architecture,Integrated circuit modeling,learning systems,neuromorphic,neuromorphic engineering,Neuromorphics,Neurons,real-time neuromorphic systems,SNN,spike-timing-dependent plasticity (STDP),spiking neural network architecture,subthreshold analog circuits,To read,very large-scale integration (VLSI),winner-take-all (WTA)},
  number = {9}
}

@article{choueiri2009,
  title = {New {{Dawn}} for {{Electric Rockets}}},
  author = {Choueiri, Edgar Y.},
  year = {2009},
  month = feb,
  volume = {300},
  pages = {58--65},
  issn = {0036-8733},
  doi = {10.1038/scientificamerican0209-58},
  file = {/Users/x0r/Zotero/storage/FEKSGUIW/Choueiri_2009_New Dawn for Electric Rockets.pdf},
  journal = {Scientific American},
  keywords = {rocket},
  language = {en},
  number = {2}
}

@article{chua2011,
  title = {Resistance Switching Memories Are Memristors},
  author = {Chua, Leon},
  year = {2011},
  month = mar,
  volume = {102},
  pages = {765--783},
  issn = {0947-8396, 1432-0630},
  doi = {10.1007/s00339-011-6264-9},
  abstract = {All 2-terminal non-volatile memory devices based on resistance switching are memristors, regardless of the device material and physical operating mechanisms. They all exhibit a distinctive ``fingerprint'' characterized by a pinched hysteresis loop confined to the first and the third quadrants of the v\textendash i plane whose contour shape in general changes with both the amplitude and frequency of any periodic ``sinewave-like'' input voltage source, or current source. In particular, the pinched hysteresis loop shrinks and tends to a straight line as frequency increases. Though numerous examples of voltage vs. current pinched hysteresis loops have been published in many unrelated fields, such as biology, chemistry, physics, etc., and observed from many unrelated phenomena, such as gas discharge arcs, mercury lamps, power conversion devices, earthquake conductance variations, etc., we restrict our examples in this tutorial to solidstate and/or nano devices where copious examples of published pinched hysteresis loops abound. In particular, we sampled arbitrarily, one example from each year between the years 2000 and 2010, to demonstrate that the memristor is a device that does not depend on any particular material, or physical mechanism. For example, we have shown that spin-transfer magnetic tunnel junctions are examples of memristors. We have also demonstrated that both bipolar and unipolar resistance switching devices are memristors.},
  file = {/Users/x0r/Zotero/storage/2QPNU5BW/Chua_2011_Resistance switching memories are memristors.pdf},
  journal = {Applied Physics A},
  keywords = {ReRAM},
  language = {en},
  number = {4}
}

@article{cichy2019,
  title = {Deep {{Neural Networks}} as {{Scientific Models}}},
  author = {Cichy, Radoslaw M. and Kaiser, Daniel},
  year = {2019},
  month = feb,
  issn = {13646613},
  doi = {10.1016/j.tics.2019.01.009},
  file = {/Users/x0r/Zotero/storage/4AE3H6D9/Cichy_Kaiser_2019_Deep Neural Networks as Scientific Models.pdf},
  journal = {Trends in Cognitive Sciences},
  keywords = {dl,To read},
  language = {en}
}

@article{cinar2015,
  title = {Three Dimensional Finite Element Modeling and Characterization of Intermediate States in Single Active Layer Phase Change Memory Devices},
  author = {Cinar, I. and Aslan, O. B. and Gokce, A. and Dincer, O. and Karakas, V. and Stipe, B. and Katine, J. A. and Aktas, G. and Ozatay, O.},
  year = {2015},
  month = jun,
  volume = {117},
  pages = {214302},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.4921827},
  file = {/Users/x0r/Zotero/storage/L2N4VND5/Cinar et al_2015_Three dimensional finite element modeling and characterization of intermediate.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {21}
}

@article{ciocchini2012,
  title = {Modeling of {{Threshold}}-{{Voltage Drift}} in {{Phase}}-{{Change Memory}} ({{PCM}}) {{Devices}}},
  author = {Ciocchini, Nicola and Cassinerio, Marco and Fugazza, Davide and Ielmini, Daniele},
  year = {2012},
  month = nov,
  volume = {59},
  pages = {3084--3090},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2012.2214784},
  abstract = {The amorphous phase of the chalcogenide material in phase-change memory (PCM) devices is sensitive to temperatureactivated crystallization and structural relaxation (SR). The latter leads to a change of device/material properties, such as the mobility band gap, the resistance, and the threshold voltage VT for threshold switching. In this paper, we present a VT drift model based on physical descriptions of the electrical transport, the threshold switching, and the SR. We introduce an analytical formula describing the relation between the drift slopes of resistance and VT via the subthreshold slope STS of the I\textendash V curve. A numerical model predicting the time evolution of VT for different programmed states in the PCM multilevel cell is finally presented and compared with experiments.},
  file = {/Users/x0r/Zotero/storage/9SJWSTG4/Ciocchini et al_2012_Modeling of Threshold-Voltage Drift in Phase-Change Memory (PCM) Devices.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {modeling,PCM,threshold-switching},
  language = {en},
  number = {11}
}

@article{clavera2018,
  title = {Model-{{Based Reinforcement Learning}} via {{Meta}}-{{Policy Optimization}}},
  author = {Clavera, Ignasi and Rothfuss, Jonas and Schulman, John and Fujita, Yasuhiro and Asfour, Tamim and Abbeel, Pieter},
  year = {2018},
  month = sep,
  abstract = {Model-based reinforcement learning approaches carry the promise of being data efficient. However, due to challenges in learning dynamics models that sufficiently match the real-world dynamics, they struggle to achieve the same asymptotic performance as model-free methods. We propose Model-Based MetaPolicy-Optimization (MB-MPO), an approach that foregoes the strong reliance on accurate learned dynamics models. Using an ensemble of learned dynamic models, MB-MPO meta-learns a policy that can quickly adapt to any model in the ensemble with one policy gradient step. This steers the meta-policy towards internalizing consistent dynamics predictions among the ensemble while shifting the burden of behaving optimally w.r.t. the model discrepancies towards the adaptation step. Our experiments show that MB-MPO is more robust to model imperfections than previous model-based approaches. Finally, we demonstrate that our approach is able to match the asymptotic performance of model-free methods while requiring significantly less experience.},
  archivePrefix = {arXiv},
  eprint = {1809.05214},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/BNIKUZ6F/Clavera et al_2018_Model-Based Reinforcement Learning via Meta-Policy Optimization.pdf},
  journal = {arXiv:1809.05214 [cs, stat]},
  keywords = {meta-rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{clune2019,
  title = {{{AI}}-{{GAs}}: {{AI}}-Generating Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence},
  shorttitle = {{{AI}}-{{GAs}}},
  author = {Clune, Jeff},
  year = {2019},
  month = may,
  abstract = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces required for intelligence, with the implicit assumption that some future group will complete the Herculean task of figuring out how to combine all of those pieces into a complex thinking machine. I call this the ``manual AI approach.'' This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend in machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. I argue that either approach could produce general AI first, and both are scientifically worthwhile irrespective of which is the fastest path. Because both are promising, yet the ML community is currently committed to the manual approach, I argue that our community should increase its research investment in the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss AI-GA-specific safety and ethical considerations. Because it it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general AI (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  archivePrefix = {arXiv},
  eprint = {1905.10985},
  eprinttype = {arxiv},
  file = {/Users/x0r/switchdrive/zotero/Clune_2019_AI-GAs.pdf;/Users/x0r/Zotero/storage/IXEZHP85/1905.html},
  journal = {arXiv:1905.10985 [cs]},
  primaryClass = {cs}
}

@article{clune2020,
  title = {{{AI}}-{{GAs}}: {{AI}}-Generating Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence},
  shorttitle = {{{AI}}-{{GAs}}},
  author = {Clune, Jeff},
  year = {2020},
  month = jan,
  abstract = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces that might be required for intelligence, with the implicit assumption that at some point in the future some group will complete the Herculean task of figuring out how to combine all of those pieces into an extremely complex machine. I call this the ``manual AI approach.'' This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend from the history of machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which itself automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. While work has begun on the first two pillars, little has been done on the third. Here I argue that either the manual or AI-GA approach could be the first to lead to general AI, and that both are worthwhile scientific endeavors irrespective of which is the fastest path. Because both approaches are roughly equally promising, and because the machine learning community is mostly committed to the engineered AI approach currently, I argue that our community should shift a substantial amount of its research investment to the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss the safety and ethical considerations unique to the AI-GA approach. Because it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general intelligence (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  archivePrefix = {arXiv},
  eprint = {1905.10985},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NVF7XGT9/Clune - 2020 - AI-GAs AI-generating algorithms, an alternate par.pdf},
  journal = {arXiv:1905.10985 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{cohen2012,
  title = {Neuron-Type Specific Signals for Reward and Punishment in the Ventral Tegmental Area},
  author = {Cohen, Jeremiah Y. and Haesler, Sebastian and Vong, Linh and Lowell, Bradford B. and Uchida, Naoshige},
  year = {2012},
  month = jan,
  volume = {482},
  pages = {85--88},
  issn = {0028-0836},
  doi = {10.1038/nature10754},
  abstract = {Dopamine plays a key role in motivation and reward. Dopaminergic neurons in the ventral tegmental area (VTA) signal the discrepancy between expected and actual rewards (i.e., reward prediction error, RPE)-, but how they compute such signals is unknown. We recorded the activity of VTA neurons while mice associated different odour cues with appetitive and aversive outcomes. We found three types of neurons based on responses to odours and outcomes: approximately half of the neurons (Type I, 52\%) showed phasic excitation after reward-predicting odours and rewards in a manner consistent with RPE coding. The other half of neurons showed persistent activity during the delay between odour and outcome, that was modulated positively (Type II, 31\%) or negatively (Type III, 17\%) by the value of outcomes. While the activity of Type I neurons was sensitive to actual outcomes (i.e., when the reward was delivered as expected vs. unexpectedly omitted), the activity of Types II and III neurons was determined predominantly by reward-predicting odours. We ``tagged'' dopaminergic and GABAergic neurons with the light-sensitive protein channelrhodopsin-2 (ChR2) and identified them based on their responses to optical stimulation while recording. All identified dopaminergic neurons were of Type I and all GABAergic neurons were of Type II. These results show that VTA GABAergic neurons signal expected reward, a key variable for dopaminergic neurons to calculate RPE.},
  file = {/Users/x0r/Zotero/storage/NYDQF3Z9/Cohen et al. - 2012 - Neuron-type specific signals for reward and punish.pdf},
  journal = {Nature},
  number = {7383},
  pmcid = {PMC3271183},
  pmid = {22258508}
}

@book{cole2018,
  title = {The Consciousness' Drive: Information Need and the Search for Meaning},
  shorttitle = {The Consciousness' Drive},
  author = {Cole, Charles},
  year = {2018},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  file = {/Users/x0r/Zotero/storage/49SUCCHH/Cole_2018_The consciousness' drive.pdf},
  isbn = {978-3-319-92455-7},
  keywords = {AGI},
  language = {en}
}

@article{collier2018,
  title = {Implementing {{Neural Turing Machines}}},
  author = {Collier, Mark and Beel, Joeran},
  year = {2018},
  month = jul,
  abstract = {Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural Networks, a new class of recurrent neural networks which decouple computation from memory by introducing an external memory unit. NTMs have demonstrated superior performance over Long Short-Term Memory Cells in several sequence learning tasks. A number of open source implementations of NTMs exist but are unstable during training and/or fail to replicate the reported performance of NTMs. This paper presents the details of our successful implementation of a NTM. Our implementation learns to solve three sequential learning tasks from the original NTM paper. We find that the choice of memory contents initialization scheme is crucial in successfully implementing a NTM. Networks with memory contents initialized to small constant values converge on average 2 times faster than the next best memory contents initialization scheme.},
  archivePrefix = {arXiv},
  eprint = {1807.08518},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GMJDI3VA/Collier_Beel_2018_Implementing Neural Turing Machines.pdf;/Users/x0r/Zotero/storage/T7EU3957/1807.html},
  journal = {arXiv:1807.08518 [cs, stat]},
  keywords = {dl,NTM},
  primaryClass = {cs, stat}
}

@article{comsa2019,
  title = {Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function},
  author = {Comsa, Iulia M. and Potempa, Krzysztof and Versari, Luca and Fischbacher, Thomas and Gesmundo, Andrea and Alakuijala, Jyrki},
  year = {2019},
  month = jul,
  abstract = {The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically-plausible alpha synaptic transfer function. Additionally, we use trainable synchronisation pulses that provide bias, add flexibility during training and exploit the decay part of the alpha function. We show that such networks can be trained successfully on noisy Boolean logic tasks and on the MNIST dataset encoded in time. The results show that the spiking neural network outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. We also find that the spiking network spontaneously discovers two operating regimes, mirroring the accuracy-speed trade-off observed in human decision-making: a slow regime, where a decision is taken after all hidden neurons have spiked and the accuracy is very high, and a fast regime, where a decision is taken very fast but the accuracy is lower. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks towards energy-efficient and more complex biologically-inspired neural architectures.},
  archivePrefix = {arXiv},
  eprint = {1907.13223},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/B92MCJEH/Comsa et al_2019_Temporal coding in spiking neural networks with alpha synaptic function.pdf;/Users/x0r/Zotero/storage/WTXE3WFX/1907.html},
  journal = {arXiv:1907.13223 [cs, q-bio]},
  keywords = {SNN},
  primaryClass = {cs, q-bio}
}

@article{conti2018,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty}}-{{Seeking Agents}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Such, Felipe Petroski and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2018},
  month = oct,
  abstract = {Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.},
  archivePrefix = {arXiv},
  eprint = {1712.06560},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JQIGY5MS/Conti et al. - 2018 - Improving Exploration in Evolution Strategies for .pdf;/Users/x0r/Zotero/storage/J3PTY2SX/1712.html},
  journal = {arXiv:1712.06560 [cs]},
  primaryClass = {cs}
}

@article{coon,
  title = {New {{Hardware}} for {{Massive Neural Networks}}},
  author = {Coon, Darryl D and Perera, A G Unil},
  pages = {10},
  abstract = {Transient phenomena associated with forward biased silicon p+ - n - n+ structures at 4.2K show remarkable similarities with biological neurons. The devices play a role similar to the two-terminal switching elements in Hodgkin-Huxley equivalent circuit diagrams. The devices provide simpler and more realistic neuron emulation than transistors or op-amps. They have such low power and current requirements that they could be used in massive neural networks. Some observed properties of simple circuits containing the devices include action potentials, refractory periods, threshold behavior, excitation, inhibition, summation over synaptic inputs, synaptic weights, temporal integration, memory, network connectivity modification based on experience, pacemaker activity, firing thresholds, coupling to sensors with graded signal outputs and the dependence of firing rate on input current. Transfer functions for simple artificial neurons with spiketrain inputs and spiketrain outputs have been measured and correlated with input coupling.},
  file = {/Users/x0r/Zotero/storage/8RYVQUJZ/Coon_Perera_New Hardware for Massive Neural Networks.pdf},
  language = {en}
}

@article{cortese2019,
  title = {The Neural and Cognitive Architecture for Learning from a Small Sample},
  author = {Cortese, Aurelio and De Martino, Benedetto and Kawato, Mitsuo},
  year = {2019},
  month = apr,
  volume = {55},
  pages = {133--141},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2019.02.011},
  abstract = {Artificial intelligence algorithms are capable of fantastic exploits, yet they are still grossly inefficient compared with the brain's ability to learn from few exemplars or solve problems that have not been explicitly defined. What is the secret that the evolution of human intelligence has unlocked? Generalization is one answer, but there is more to it. The brain does not directly solve difficult problems, it is able to recast them into new and more tractable problems. Here, we propose a model whereby higher cognitive functions profoundly interact with reinforcement learning to drastically reduce the degrees of freedom of the search space, simplifying complex problems, and fostering more efficient learning.},
  file = {/Users/x0r/Zotero/storage/65IFLUH9/Cortese et al_2019_The neural and cognitive architecture for learning from a small sample.pdf},
  journal = {Current Opinion in Neurobiology},
  keywords = {neuromorphic,neuroscience},
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}}
}

@article{courbariaux2015,
  title = {{{BinaryConnect}}: {{Training Deep Neural Networks}} with Binary Weights during Propagations},
  shorttitle = {{{BinaryConnect}}},
  author = {Courbariaux, Matthieu and Bengio, Yoshua and David, Jean-Pierre},
  year = {2015},
  month = nov,
  abstract = {Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide range of tasks, with the best results obtained with large training sets and large models. In the past, GPUs enabled these breakthroughs because of their greater computational speed. In the future, faster computation at both training and test time is likely to be crucial for further progress and for consumer applications on low-power devices. As a result, there is much interest in research and development of dedicated hardware for Deep Learning (DL). Binary weights, i.e., weights which are constrained to only two possible values (e.g. -1 or 1), would bring great benefits to specialized DL hardware by replacing many multiply-accumulate operations by simple accumulations, as multipliers are the most space and powerhungry components of the digital implementation of neural networks. We introduce BinaryConnect, a method which consists in training a DNN with binary weights during the forward and backward propagations, while retaining precision of the stored weights in which gradients are accumulated. Like other dropout schemes, we show that BinaryConnect acts as regularizer and we obtain near state-of-the-art results with BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.},
  archivePrefix = {arXiv},
  eprint = {1511.00363},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/687RX36E/Courbariaux et al_2015_BinaryConnect.pdf},
  journal = {arXiv:1511.00363 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{Covi_etal16,
  ids = {covi2016},
  title = {Analog Memristive Synapse in Spiking Networks Implementing Unsupervised Learning},
  author = {Covi, Erika and Brivio, Stefano and Serb, Alexander and Prodromakis, Themis and Fanciulli, Marco and Spiga, Sabina},
  year = {2016},
  volume = {10},
  pages = {1--13},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/UY958NTN/Covi et al_2016_Analog Memristive Synapse in Spiking Networks Implementing Unsupervised Learning.pdf},
  journal = {Frontiers in neuroscience},
  keywords = {neuromorphic,SNN},
  number = {482}
}

@article{covi2018,
  title = {Spike-Driven Threshold-Based Learning with Memristive Synapses and Neuromorphic Silicon Neurons},
  author = {Covi, E and George, R and Frascaroli, J and Brivio, S and Mayr, C and Mostafa, H and Indiveri, G and Spiga, S},
  year = {2018},
  month = aug,
  volume = {51},
  pages = {344003},
  issn = {0022-3727, 1361-6463},
  doi = {10.1088/1361-6463/aad361},
  abstract = {Biologically plausible neuromorphic computing systems are attracting considerable attention due to their low latency, massively parallel information processing abilities, and their high energy efficiency. To achieve these features neuromorphic silicon neuron circuits need to be integrated with plastic synapse circuits capable of on-line learning and storage of synaptic weights. Within this context, memristive devices play a key role thanks to their nonvolatility, scalability, and compatibility with the complementary metal\textendash oxide\textendash semiconductor fabrication process. However, neuro-memristive systems are still facing difficult challenges for implementing efficient learning protocols. Here, we propose and demonstrate in hardware a spike-driven threshold-based learning rule which goes beyond conventional spike-timing dependent plasticity mechanisms, by also taking into account the neuron membrane potential and its firing rate. The mixed memristive\textendash neuromorphic system we demonstrate comprises an oxide-based memristive synapse device placed between two silicon neurons implemented on a neuromorphic chip that comprises the proper interfacing and spike-based learning circuits designed to drive the memristive elements. We show how the system is able to emulate in real-time weight dependent post-synaptic activity and drive synaptic weight updates at the memristive synapse level following the spike-driven learning rule presented. We validate this spike-based learning mechanism with experimental results and quantify the system performance with basic learning experiments.},
  file = {/Users/x0r/Zotero/storage/YYGYLJPC/Covi et al_2018_Spike-driven threshold-based learning with memristive synapses and neuromorphic.pdf},
  journal = {Journal of Physics D: Applied Physics},
  keywords = {neuromorphic},
  language = {en},
  number = {34}
}

@article{cristiano2018,
  title = {Perspective on Training Fully Connected Networks with Resistive Memories: {{Device}} Requirements for Multiple Conductances of Varying Significance},
  shorttitle = {Perspective on Training Fully Connected Networks with Resistive Memories},
  author = {Cristiano, Giorgio and Giordano, Massimo and Ambrogio, Stefano and Romero, Louis P. and Cheng, Christina and Narayanan, Pritish and Tsai, Hsinyu and Shelby, Robert M. and Burr, Geoffrey W.},
  year = {2018},
  month = oct,
  volume = {124},
  pages = {151901},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.5042462},
  file = {/Users/x0r/Zotero/storage/MM4F8M9U/Cristiano et al. - 2018 - Perspective on training fully connected networks w.pdf},
  journal = {Journal of Applied Physics},
  keywords = {neuromorphic},
  language = {en},
  number = {15}
}

@article{cuccu2018,
  title = {Playing {{Atari}} with {{Six Neurons}}},
  author = {Cuccu, Giuseppe and Togelius, Julian and {Cudre-Mauroux}, Philippe},
  year = {2018},
  month = jun,
  abstract = {Deep reinforcement learning, applied to vision-based problems like Atari games, maps pixels directly to actions; internally, the deep neural network bears the responsibility of both extracting useful information and making decisions based on it. By separating the image processing from decision-making, one could better understand the complexity of each task, as well as potentially find smaller policy representations that are easier for humans to understand and may generalize better. To this end, we propose a new method for learning policies and compact state representations separately but simultaneously for policy approximation in reinforcement learning. State representations are generated by an encoder based on two novel algorithms: Increasing Dictionary Vector Quantization makes the encoder capable of growing its dictionary size over time, to address new observations as they appear in an open-ended online-learning context; Direct Residuals Sparse Coding encodes observations by disregarding reconstruction error minimization, and aiming instead for highest information inclusion. The encoder autonomously selects observations online to train on, in order to maximize code sparsity. As the dictionary size increases, the encoder produces increasingly larger inputs for the neural network: this is addressed by a variation of the Exponential Natural Evolution Strategies algorithm which adapts its probability distribution dimensionality along the run. We test our system on a selection of Atari games using tiny neural networks of only 6 to 18 neurons (depending on the game's controls). These are still capable of achieving results comparable\textemdash and occasionally superior\textemdash to state-of-the-art techniques which use two orders of magnitude more neurons.},
  archivePrefix = {arXiv},
  eprint = {1806.01363},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/534G8BP7/Cuccu et al_2018_Playing Atari with Six Neurons.pdf},
  journal = {arXiv:1806.01363 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{cueva2018,
  title = {{{EMERGENCE OF GRID}}-{{LIKE REPRESENTATIONS BY TRAINING RECURRENT NEURAL NETWORKS TO PERFORM SPATIAL LOCALIZATION}}},
  author = {Cueva, Christopher J and Wei, Xue-Xin},
  year = {2018},
  pages = {19},
  abstract = {Decades of research on the neural code underlying spatial navigation have revealed a diverse set of neural response properties. The Entorhinal Cortex (EC) of the mammalian brain contains a rich set of spatial correlates, including grid cells which encode space using tessellating patterns. However, the mechanisms and functional significance of these spatial representations remain largely mysterious. As a new way to understand these neural representations, we trained recurrent neural networks (RNNs) to perform navigation tasks in 2D arenas based on velocity inputs. Surprisingly, we find that grid-like spatial response patterns emerge in trained networks, along with units that exhibit other spatial correlates, including border cells and band-like cells. All these different functional types of neurons have been observed experimentally. The order of the emergence of grid-like and border cells is also consistent with observations from developmental studies. Together, our results suggest that grid cells, border cells and others as observed in EC may be a natural solution for representing space efficiently given the predominant recurrent connections in the neural circuits.},
  file = {/Users/x0r/Zotero/storage/8XHRUQ82/Cueva and Wei - 2018 - EMERGENCE OF GRID-LIKE REPRESENTATIONS BY TRAINING.pdf},
  keywords = {hippocampus,neuroscience},
  language = {en}
}

@article{dabney2017,
  title = {Distributional {{Reinforcement Learning}} with {{Quantile Regression}}},
  author = {Dabney, Will and Rowland, Mark and Bellemare, Marc G. and Munos, R{\'e}mi},
  year = {2017},
  month = oct,
  abstract = {In reinforcement learning an agent interacts with the environment by taking actions and observing the next state and reward. When sampled probabilistically, these state transitions, rewards, and actions can all induce randomness in the observed long-term return. Traditionally, reinforcement learning algorithms average over this randomness to estimate the value function. In this paper, we build on recent work advocating a distributional approach to reinforcement learning in which the distribution over returns is modeled explicitly instead of only estimating the mean. That is, we examine methods of learning the value distribution instead of the value function. We give results that close a number of gaps between the theoretical and algorithmic results given by Bellemare, Dabney, and Munos (2017). First, we extend existing results to the approximate distribution setting. Second, we present a novel distributional reinforcement learning algorithm consistent with our theoretical formulation. Finally, we evaluate this new algorithm on the Atari 2600 games, observing that it significantly outperforms many of the recent improvements on DQN, including the related distributional algorithm C51.},
  archivePrefix = {arXiv},
  eprint = {1710.10044},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JA2TFENV/Dabney et al_2017_Distributional Reinforcement Learning with Quantile Regression.pdf},
  journal = {arXiv:1710.10044 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{dabney2020,
  title = {A Distributional Code for Value in Dopamine-Based Reinforcement Learning},
  author = {Dabney, Will and {Kurth-Nelson}, Zeb and Uchida, Naoshige and Starkweather, Clara Kwon and Hassabis, Demis and Munos, R{\'e}mi and Botvinick, Matthew},
  year = {2020},
  month = jan,
  volume = {577},
  pages = {671--675},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1924-6},
  file = {/Users/x0r/Zotero/storage/DJ9EU8ZV/Dabney et al. - 2020 - A distributional code for value in dopamine-based .pdf},
  journal = {Nature},
  keywords = {To read},
  language = {en},
  number = {7792}
}

@inproceedings{dalgaty2019,
  title = {Hybrid {{CMOS}}-{{RRAM Neurons}} with {{Intrinsic Plasticity}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Dalgaty, Thomas and Payvand, Melika and De Salvo, Barbara and Casas, Jerome and Lama, Giusy and Nowak, Etienne and Indiveri, Giacomo and Vianello, Elisa},
  year = {2019},
  month = may,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Sapporo, Japan}},
  doi = {10.1109/ISCAS.2019.8702603},
  abstract = {Brain-inspired architectures in neuromorphic hardware are currently subject to intensive research as an alternative to the limits of traditional computer organisation. The remarkable computing performance and efficiency of biological nervous systems are widely attributed to the co-localisation of memory and computation spatially throughout the structure. Moreover, it appears that a number of local self-organising neural mechanisms play their part in efficient biological computation. An example is neuronal intrinsic plasticity, where a neuron adapts its parameters to maximise its information capacity based on the statistical properties of its input while minimising the power it consumes. CMOS circuits implementing neuron models have been proposed but require their parameters to be set by biases originating from a centralised memory. In this work, we propose a hybrid CMOS-RRAM circuit that addresses this problem through storing neuron parameters within programmable nonvolatile resistive memories incorporated into the CMOS neuron. Additional circuits exploit the stochastic switching properties of resisitive memories to map a local intrinsic plasticity algorithm onto the proposed neuron. We demonstrate the computational advantages of this algorithm through simulation, calibrated on experimental data, whereby the neuron maximises its information capacity while minimising its power consumption, as is the case for biological neurons.},
  file = {/Users/x0r/Zotero/storage/4YJCLY5Z/Dalgaty et al_2019_Hybrid CMOS-RRAM Neurons with Intrinsic Plasticity.pdf},
  isbn = {978-1-72810-397-6},
  keywords = {neuromorphic},
  language = {en}
}

@article{danihelka2016,
  title = {Associative {{Long Short}}-{{Term Memory}}},
  author = {Danihelka, Ivo and Wayne, Greg and Uria, Benigno and Kalchbrenner, Nal and Graves, Alex},
  year = {2016},
  month = feb,
  abstract = {We investigate a new method to augment recurrent neural networks with extra memory without increasing the number of network parameters. The system has an associative memory based on complex-valued vectors and is closely related to Holographic Reduced Representations and Long Short-Term Memory networks. Holographic Reduced Representations have limited capacity: as they store more information, each retrieval becomes noisier due to interference. Our system in contrast creates redundant copies of stored information, which enables retrieval with reduced noise. Experiments demonstrate faster learning on multiple memorization tasks.},
  archivePrefix = {arXiv},
  eprint = {1602.03032},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/7FSLLRRF/Danihelka et al_2016_Associative Long Short-Term Memory.pdf},
  journal = {arXiv:1602.03032 [cs]},
  keywords = {dl,RNN},
  language = {en},
  primaryClass = {cs}
}

@article{dasgupta2017,
  title = {A Neural Algorithm for a Fundamental Computing Problem},
  author = {Dasgupta, Sanjoy and Stevens, Charles F and Navlakha, Saket},
  year = {2017},
  pages = {5},
  file = {/Users/x0r/Zotero/storage/MXHRH3Q4/Dasgupta et al. - 2017 - A neural algorithm for a fundamental computing pro.pdf},
  keywords = {neuroscience,SNN},
  language = {en}
}

@article{davies2019,
  title = {Benchmarks for Progress in Neuromorphic Computing},
  author = {Davies, Mike},
  year = {2019},
  month = sep,
  volume = {1},
  pages = {386--388},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0097-1},
  abstract = {In order for the neuromorphic research field to advance into the mainstream of computing, it needs to start quantifying gains, standardize on benchmarks and focus on feasible application challenges.},
  copyright = {2019 Springer Nature Limited},
  file = {/Users/x0r/Zotero/storage/W7UMQN4T/s42256-019-0097-1.html},
  journal = {Nature Machine Intelligence},
  keywords = {neuromorphic},
  language = {en},
  number = {9}
}

@article{davies2019a,
  title = {Benchmarks for Progress in Neuromorphic Computing},
  author = {Davies, Mike},
  year = {2019},
  month = sep,
  volume = {1},
  pages = {386--388},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0097-1},
  file = {/Users/x0r/Zotero/storage/4IWTITUE/Davies - 2019 - Benchmarks for progress in neuromorphic computing.pdf},
  journal = {Nature Machine Intelligence},
  language = {en},
  number = {9}
}

@article{dayan1993,
  title = {Improving {{Generalization}} for {{Temporal Difference Learning}}: {{The Successor Representation}}},
  shorttitle = {Improving {{Generalization}} for {{Temporal Difference Learning}}},
  author = {Dayan, Peter},
  year = {1993},
  month = jul,
  volume = {5},
  pages = {613--624},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1993.5.4.613},
  file = {/Users/x0r/Zotero/storage/ERNC4HJH/Dayan_1993_Improving Generalization for Temporal Difference Learning.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{degiuli2019,
  title = {Random {{Language Model}}},
  author = {DeGiuli, E.},
  year = {2019},
  month = mar,
  volume = {122},
  pages = {128301},
  doi = {10.1103/PhysRevLett.122.128301},
  abstract = {Many complex generative systems use languages to create structured objects. We consider a model of random languages, defined by weighted context-free grammars. As the distribution of grammar weights broadens, a transition is found from a random phase, in which sentences are indistinguishable from noise, to an organized phase in which nontrivial information is carried. This marks the emergence of deep structure in the language, and can be understood by a competition between energy and entropy.},
  journal = {Physical Review Letters},
  number = {12}
}

@article{deneve2017,
  title = {The {{Brain}} as an {{Efficient}} and {{Robust Adaptive Learner}}},
  author = {Den{\`e}ve, Sophie and Alemi, Alireza and Bourdoukan, Ralph},
  year = {2017},
  month = jun,
  volume = {94},
  pages = {969--977},
  issn = {08966273},
  doi = {10.1016/j.neuron.2017.05.016},
  file = {/Users/x0r/Zotero/storage/J346CTJV/Denève et al. - 2017 - The Brain as an Efficient and Robust Adaptive Lear.pdf},
  journal = {Neuron},
  keywords = {To read},
  language = {en},
  number = {5}
}

@inproceedings{derchangkau2009,
  title = {A Stackable Cross Point {{Phase Change Memory}}},
  author = {{DerChang Kau} and Tang, Stephen and Karpov, Ilya V. and Dodge, Rick and Klehn, Brett and Kalb, Johannes A. and Strand, Jonathan and Diaz, Aleshandre and Leung, Nelson and Wu, Jack and {Sean Lee} and Langtry, Tim and {Kuo-wei Chang} and Papagianni, Christina and {Jinwook Lee} and Hirst, Jeremy and Erra, Swetha and Flores, Eddie and Righos, Nick and Castro, Hernan and Spadini, Gianpaolo},
  year = {2009},
  month = dec,
  pages = {1--4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2009.5424263},
  abstract = {A novel scalable and stackable nonvolatile memory technology suitable for building fast and dense memory devices is discussed. The memory cell is built by layering a storage element and a selector. The storage element is a Phase Change Memory (PCM) cell [1] and the selector is an Ovonic Threshold Switch (OTS) [2]. The vertically integrated memory cell of one PCM and one OTS (PCMS) is embedded in a true cross point array. Arrays are stacked on top of CMOS circuits for decoding, sensing and logic functions. A RESET speed of 9 nsec and endurance of 106 cycles are achieved. One volt of dynamic range delineating SET vs. RESET is also demonstrated.},
  file = {/Users/x0r/Zotero/storage/EHMZ5YQ8/DerChang Kau et al_2009_A stackable cross point Phase Change Memory.pdf},
  isbn = {978-1-4244-5639-0},
  keywords = {neuromorphic,PCM},
  language = {en}
}

@article{deverett2019,
  title = {Interval Timing in Deep Reinforcement Learning Agents},
  author = {Deverett, Ben and Faulkner, Ryan and Fortunato, Meire and Wayne, Greg and Leibo, Joel Z.},
  year = {2019},
  month = may,
  abstract = {The measurement of time is central to intelligent behavior. We know that both animals and artificial agents can successfully use temporal dependencies to select actions. In artificial agents, little work has directly addressed (1) which architectural components are necessary for successful development of this ability, (2) how this timing ability comes to be represented in the units and actions of the agent, and (3) whether the resulting behavior of the system converges on solutions similar to those of biology. Here we studied interval timing abilities in deep reinforcement learning agents trained end-to-end on an interval reproduction paradigm inspired by experimental literature on mechanisms of timing. We characterize the strategies developed by recurrent and feedforward agents, which both succeed at temporal reproduction using distinct mechanisms, some of which bear specific and intriguing similarities to biological systems. These findings advance our understanding of how agents come to represent time, and they highlight the value of experimentally inspired approaches to characterizing agent abilities.},
  archivePrefix = {arXiv},
  eprint = {1905.13469},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LS73NFZE/Deverett et al. - 2019 - Interval timing in deep reinforcement learning age.pdf},
  journal = {arXiv:1905.13469 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{dickson2015,
  title = {Long-Wave Infrared Polarimetric Cluster-Based Vehicle Detection},
  author = {Dickson, Christopher N. and Wallace, Andrew M. and Kitchin, Matthew and Connor, Barry},
  year = {2015},
  month = dec,
  volume = {32},
  pages = {2307--2315},
  issn = {1520-8532},
  doi = {10.1364/JOSAA.32.002307},
  abstract = {The sensory perception of other vehicles in cluttered environments is an essential component of situational awareness for a mobile vehicle. However, vehicle detection is normally applied to visible imagery sequences, while in this paper we investigate how polarized, infrared imagery can add additional discriminatory power. Using knowledge about the properties of the objects of interest and the scene environment, we have developed a polarimetric cluster-based descriptor to detect vehicles using long-wave infrared radiation in the range of 8\&\#x2013;12\&\#xA0;\&\#x3BC;m. Our approach outperforms both intensity and polarimetric image histogram descriptors applied to the infrared data. For example, at a false positive rate of 0.01 per detection window, our cluster approach results in a true positive rate of 0.63 compared to a rate of 0.05 for a histogram of gradient descriptor trained and tested on the same dataset. In conclusion, we discuss the potential of this new approach in comparison with state-of-the-art infrared and conventional video detection.},
  copyright = {\&\#169; 2015 Optical Society of America},
  file = {/Users/x0r/Zotero/storage/3WTZPDN6/Dickson et al_2015_Long-wave infrared polarimetric cluster-based vehicle detection.pdf},
  journal = {JOSA A},
  keywords = {dl,polarcam},
  language = {EN},
  number = {12}
}

@article{diehl,
  title = {Unsupervised {{Learning}} of {{Digit Recognition Using Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  author = {Diehl, Peter U},
  pages = {6},
  abstract = {The recent development of power-efficient neuromorphic hardware offers great opportunities for applications where power consumption is a main concern, ranging from mobile platforms to server farms. However, it remains a challenging task to design spiking neural networks (SNN) to do pattern recognition on such hardware. We present a SNN for digit recognition which relies on mechanisms commonly used on neuromorphic hardware, i.e. exponential synapses with spiketiming-dependent plasticity, lateral inhibition, and an adaptive threshold. Unlike most other approaches, we do not present any class labels to the network; the network uses unsupervised learning. The performance of our network scales well with the number of neurons used. Intuitively, the used algorithm is comparable to k-means and competitive learning algorithms such as vector quantization and self-organizing maps, each neuron learns a representation of a part of the input space, similar to a centroid in k-means. Our architecture achieves 95\% accuracy on the MNIST benchmark, which outperforms other unsupervised learning methods for SNNs. The fact that we used no domainspecific knowledge points toward a more general applicability of the network design.},
  file = {/Users/x0r/Zotero/storage/3AP9DWRV/Diehl_Unsupervised Learning of Digit Recognition Using Spike-Timing-Dependent.pdf},
  keywords = {STDP},
  language = {en}
}

@inproceedings{Diehl_etal15,
  ids = {diehl2015},
  title = {Fast-Classifying, High-Accuracy Spiking Deep Networks through Weight and Threshold Balancing},
  booktitle = {2015 International Joint Conference on Neural Networks ({{IJCNN}})},
  author = {Diehl, Peter U and Neil, Daniel and Binas, Jonathan and Cook, Matthew and Liu, Shih-Chii and Pfeiffer, Michael},
  year = {2015},
  pages = {1--8},
  file = {/Users/x0r/Zotero/storage/KMY7F29I/Diehl et al_2015_Fast-classifying, high-accuracy spiking deep networks through weight and.pdf},
  keywords = {backprop,dl,neuromorphic,SNN},
  organization = {{[object Object]}}
}

@article{donati2019,
  title = {Discrimination of {{EMG Signals Using}} a {{Neuromorphic Implementation}} of a {{Spiking Neural Network}}},
  author = {Donati, Elisa and Payvand, Melika and Risi, Nicoletta and Krause, Renate and Indiveri, Giacomo},
  year = {2019},
  month = oct,
  volume = {13},
  pages = {795--803},
  issn = {1940-9990},
  doi = {10.1109/TBCAS.2019.2925454},
  abstract = {An accurate description of muscular activity plays an important role in the clinical diagnosis and rehabilitation research. The electromyography (EMG) is the most used technique to make accurate descriptions of muscular activity. The EMG is associated with the electrical changes generated by the activity of the motor neurons. Typically, to decode the muscular activation during different movements, a large number of individual motor neurons are monitored simultaneously, producing large amounts of data to be transferred and processed by the computing devices. In this paper, we follow an alternative approach that can be deployed locally on the sensor side. We propose a neuromorphic implementation of a spiking neural network (SNN) to extract spatio-temporal information of EMG signals locally and classify hand gestures with very low power consumption. We present experimental results on the input data stream using a mixed-signal analog/digital neuromorphic processor. We performed a thorough investigation on the performance of the SNN implemented on the chip, by: first, calculating PCA on the activity of the silicon neurons at the input and the hidden layers to show how the network helps in separating the samples of different classes; second, performing classification of the data using state-of-the-art SVM and logistic regression methods and a hardware-friendly spike-based read-out. The traditional algorithm achieved a classification rate of [Formula: see text] and [Formula: see text], respectively, and the spiking learning method achieved [Formula: see text]. The power consumption of the SNN is [Formula: see text], showing the potential of this approach for ultra-low power processing.},
  file = {/Users/x0r/Zotero/storage/9J8YQIXL/Donati et al. - 2019 - Discrimination of EMG Signals Using a Neuromorphic.pdf},
  journal = {IEEE transactions on biomedical circuits and systems},
  language = {eng},
  number = {5},
  pmid = {31251192}
}

@article{douglas1989,
  title = {A {{Canonical Microcircuit}} for {{Neocortex}}},
  author = {Douglas, Rodney J. and Martin, Kevan A.C. and Whitteridge, David},
  year = {1989},
  month = dec,
  volume = {1},
  pages = {480--488},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.480},
  file = {/Users/x0r/Zotero/storage/TVS5PBXD/Douglas et al. - 1989 - A Canonical Microcircuit for Neocortex.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{doya,
  title = {Temporal {{Difference Learning}} in {{Continuous Time}} and {{Space}}},
  author = {Doya, Kenji},
  pages = {7},
  abstract = {A continuous-time, continuous-state version of the temporal difference (TD) algorithm is derived in order to facilitate the application of reinforcement learning to real-world control tasks and neurobiological modeling. An optimal nonlinear feedback control law was also derived using the derivatives of the value function. The performance of the algorithms was tested in a task of swinging up a pendulum with limited torque. Both the "critic" that specifies the paths to the upright position and the "actor" that works as a nonlinear feedback controller were successfully implemented by radial basis function (RBF) networks.},
  file = {/Users/x0r/Zotero/storage/KF9Y3YAL/Doya - Temporal Difference Learning in Continuous Time an.pdf},
  keywords = {neuromorphic,rl},
  language = {en}
}

@article{doya1999,
  title = {What Are the Computations of the Cerebellum, the Basal Ganglia and the Cerebral Cortex?},
  author = {Doya, K.},
  year = {1999},
  month = oct,
  volume = {12},
  pages = {961--974},
  issn = {08936080},
  doi = {10.1016/S0893-6080(99)00046-5},
  abstract = {The classical notion that the cerebellum and the basal ganglia are dedicated to motor control is under dispute given increasing evidence of their involvement in non-motor functions. Is it then impossible to characterize the functions of the cerebellum, the basal ganglia and the cerebral cortex in a simplistic manner? This paper presents a novel view that their computational roles can be characterized not by asking what are the ``goals'' of their computation, such as motor or sensory, but by asking what are the ``methods'' of their computation, specifically, their learning algorithms. There is currently enough anatomical, physiological, and theoretical evidence to support the hypotheses that the cerebellum is a specialized organism for supervised learning, the basal ganglia are for reinforcement learning, and the cerebral cortex is for unsupervised learning.},
  file = {/Users/x0r/Zotero/storage/5PHRETRW/Doya_1999_What are the computations of the cerebellum, the basal ganglia and the cerebral.pdf},
  journal = {Neural Networks},
  keywords = {AGI,neuroscience,To read},
  language = {en},
  number = {7-8}
}

@article{du2017,
  title = {Reservoir Computing Using Dynamic Memristors for Temporal Information Processing},
  author = {Du, Chao and Cai, Fuxi and Zidan, Mohammed A. and Ma, Wen and Lee, Seung Hwan and Lu, Wei D.},
  year = {2017},
  month = dec,
  volume = {8},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02337-y},
  file = {/Users/x0r/Zotero/storage/C93KGYGN/Du et al_2017_Reservoir computing using dynamic memristors for temporal information processing.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic,reservoir-computing},
  language = {en},
  number = {1}
}

@article{duarte2017,
  title = {Synaptic Patterning and the Timescales of Cortical Dynamics},
  author = {Duarte, Renato and Seeholzer, Alexander and Zilles, Karl and Morrison, Abigail},
  year = {2017},
  month = apr,
  volume = {43},
  pages = {156--165},
  issn = {09594388},
  doi = {10.1016/j.conb.2017.02.007},
  file = {/Users/x0r/Zotero/storage/QPCXWEA7/Duarte et al. - 2017 - Synaptic patterning and the timescales of cortical.pdf},
  journal = {Current Opinion in Neurobiology},
  language = {en}
}

@article{dupont2019,
  title = {Augmented {{Neural ODEs}}},
  author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
  year = {2019},
  month = apr,
  abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
  archivePrefix = {arXiv},
  eprint = {1904.01681},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/EDLH5LK4/Dupont et al_2019_Augmented Neural ODEs.pdf},
  journal = {arXiv:1904.01681 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{dwivedi2014,
  title = {Characterization of {{Phase}}-{{Change Material Using Verilog}}-{{A}} and Its {{Validation}} as a {{Memory Element}}},
  author = {Dwivedi, Amit Krishna and Kumari, Smriti and Islam, Aminul},
  year = {2014},
  month = sep,
  pages = {1--5},
  publisher = {{IEEE}},
  doi = {10.1109/ICDCCom.2014.7024702},
  abstract = {This paper presents SPICE modeling of Phase-change Random Access Memory (PCRAM). Different models of PCRAM have been already proposed but those models still lack capability to exactly model the behavior of PCRAM. In this paper we have introduced various physical parameters in the programming, to accurately model PCRAM behavior. The modeling of PCRAM cell has been done in Verilog-A and simulation results have been extensively verified using SPICE. Further, we have integrated the proposed model with the modified CNFET based read-write circuit. This paper emerges with the successful modeling of R-I characteristics of PCRAM. Read and write operations with proper programming pulses have also been demonstrated in this paper.},
  file = {/Users/x0r/Zotero/storage/ESA9RLPJ/Dwivedi et al_2014_Characterization of Phase-Change Material Using Verilog-A and its Validation as.pdf},
  isbn = {978-1-4799-6052-1},
  keywords = {PCM},
  language = {en}
}

@article{eichenbaum2014,
  title = {Time Cells in the Hippocampus: A New Dimension for Mapping Memories},
  shorttitle = {Time Cells in the Hippocampus},
  author = {Eichenbaum, Howard},
  year = {2014},
  month = nov,
  volume = {15},
  pages = {732--744},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn3827},
  abstract = {Recent studies have revealed the existence of hippocampal neurons that fire at successive moments in temporally structured experiences. Several studies have shown that such temporal coding is not attributable to external events, specific behaviours or spatial dimensions of an experience. Instead, these cells represent the flow of time in specific memories and have therefore been dubbed `time cells'. The firing properties of time cells parallel those of hippocampal place cells; time cells thus provide an additional dimension that is integrated with spatial mapping. The robust representation of both time and space in the hippocampus suggests a fundamental mechanism for organizing the elements of experience into coherent memories.},
  file = {/Users/x0r/Zotero/storage/N6QXZIG8/Eichenbaum - 2014 - Time cells in the hippocampus a new dimension for.pdf},
  journal = {Nature Reviews Neuroscience},
  keywords = {hippocampus,neuroscience},
  language = {en},
  number = {11}
}

@article{ekman2017,
  title = {Time-Compressed Preplay of Anticipated Events in Human Primary Visual Cortex},
  author = {Ekman, Matthias and Kok, Peter and {de Lange}, Floris P.},
  year = {2017},
  month = may,
  volume = {8},
  pages = {15276},
  issn = {2041-1723},
  doi = {10.1038/ncomms15276},
  file = {/Users/x0r/Zotero/storage/45MSBTZC/Ekman et al_2017_Time-compressed preplay of anticipated events in human primary visual cortex.pdf},
  journal = {Nature Communications},
  keywords = {neuroscience},
  language = {en}
}

@article{el-boustani2018,
  title = {Locally Coordinated Synaptic Plasticity of Visual Cortex Neurons in Vivo},
  author = {{El-Boustani}, Sami and Ip, Jacque P K and {Breton-Provencher}, Vincent and Knott, Graham W and Okuno, Hiroyuki and Bito, Haruhiko and Sur, Mriganka},
  year = {2018},
  pages = {7},
  file = {/Users/x0r/Zotero/storage/ED7F4248/El-Boustani et al_2018_Locally coordinated synaptic plasticity of visual cortex neurons in vivo.pdf},
  keywords = {neuroscience},
  language = {en}
}

@article{Eliasmith_etal12,
  ids = {eliasmith2012},
  title = {A Large-Scale Model of the Functioning Brain},
  author = {Eliasmith, C. and Stewart, T.C. and Choo, X. and Bekolay, T. and DeWolf, T. and Tang, Y. and Rasmussen, D.},
  year = {2012},
  volume = {338},
  pages = {1202--1205},
  doi = {10.1126/science.1225266},
  file = {/Users/x0r/Zotero/storage/E6NP3V62/Eliasmith et al_2012_A Large-Scale Model of the Functioning Brain.pdf},
  journal = {Science},
  keywords = {neuroscience,To read},
  number = {6111}
}

@article{ellefsen2019,
  title = {How Do {{Mixture Density RNNs Predict}} the {{Future}}?},
  author = {Ellefsen, Kai Olav and Martin, Charles Patrick and Torresen, Jim},
  year = {2019},
  month = jan,
  abstract = {Gaining a better understanding of how and what machine learning systems learn is important to increase confidence in their decisions and catalyze further research. In this paper, we analyze the predictions made by a specific type of recurrent neural network, mixture density RNNs (MD-RNNs). These networks learn to model predictions as a combination of multiple Gaussian distributions, making them particularly interesting for problems where a sequence of inputs may lead to several distinct future possibilities. An example is learning internal models of an environment, where different events may or may not occur, but where the average over different events is not meaningful. By analyzing the predictions made by trained MD-RNNs, we find that their different Gaussian components have two complementary roles: 1) Separately modeling different stochastic events and 2) Separately modeling scenarios governed by different rules. These findings increase our understanding of what is learned by predictive MD-RNNs, and open up new research directions for further understanding how we can benefit from their self-organizing model decomposition.},
  archivePrefix = {arXiv},
  eprint = {1901.07859},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2GRFYETF/Ellefsen et al_2019_How do Mixture Density RNNs Predict the Future.pdf},
  journal = {arXiv:1901.07859 [cs, stat]},
  keywords = {rl,RNN},
  primaryClass = {cs, stat}
}

@article{erren2007,
  title = {Ten {{Simple Rules}} for {{Doing Your Best Research}}, {{According}} to {{Hamming}}},
  author = {Erren, Thomas C. and Cullen, Paul and Erren, Michael and Bourne, Philip E.},
  year = {2007},
  volume = {3},
  pages = {e213},
  issn = {1553-734X, 1553-7358},
  doi = {10.1371/journal.pcbi.0030213},
  file = {/Users/x0r/Zotero/storage/29X7LLRB/Erren et al_2007_Ten Simple Rules for Doing Your Best Research, According to Hamming.pdf;/Users/x0r/Zotero/storage/KN6W4Z3V/Erren et al_2007_Ten Simple Rules for Doing Your Best Research, According to Hamming.pdf},
  journal = {PLoS Computational Biology},
  keywords = {research},
  language = {en},
  number = {10}
}

@article{eryilmaz2014,
  title = {Brain-like Associative Learning Using a Nanoscale Non-Volatile Phase Change Synaptic Device Array},
  author = {Eryilmaz, Sukru B. and Kuzum, Duygu and Jeyasingh, Rakesh and Kim, SangBum and BrightSky, Matthew and Lam, Chung and Wong, H.-S. Philip},
  year = {2014},
  month = jul,
  volume = {8},
  issn = {1662-453X},
  doi = {10.3389/fnins.2014.00205},
  abstract = {Recent advances in neuroscience together with nanoscale electronic device technology have resulted in huge interests in realizing brain-like computing hardwares using emerging nanoscale memory devices as synaptic elements. Although there has been experimental work that demonstrated the operation of nanoscale synaptic element at the single device level, network level studies have been limited to simulations. In this work, we demonstrate, using experiments, array level associative learning using phase change synaptic devices connected in a grid like configuration similar to the organization of the biological brain. Implementing Hebbian learning with phase change memory cells, the synaptic grid was able to store presented patterns and recall missing patterns in an associative brain-like fashion. We found that the system is robust to device variations, and large variations in cell resistance states can be accommodated by increasing the number of training epochs. We illustrated the tradeoff between variation tolerance of the network and the overall energy consumption, and found that energy consumption is decreased significantly for lower variation tolerance.},
  file = {/Users/x0r/Zotero/storage/BCZBEQEK/Eryilmaz et al_2014_Brain-like associative learning using a nanoscale non-volatile phase change.pdf;/Users/x0r/Zotero/storage/WTF6EGCU/Eryilmaz et al_2014_Brain-like associative learning using a nanoscale non-volatile phase change.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic,PCM},
  language = {en}
}

@article{eslami2016,
  title = {Attend, {{Infer}}, {{Repeat}}: {{Fast Scene Understanding}} with {{Generative Models}}},
  shorttitle = {Attend, {{Infer}}, {{Repeat}}},
  author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
  year = {2016},
  month = mar,
  abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects \textendash{} counting, locating and classifying the elements of a scene \textendash without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
  archivePrefix = {arXiv},
  eprint = {1603.08575},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VB2M7S3P/Eslami et al_2016_Attend, Infer, Repeat.pdf},
  journal = {arXiv:1603.08575 [cs]},
  keywords = {attention,dl,GAN},
  language = {en},
  primaryClass = {cs}
}

@article{eslami2018,
  title = {Neural Scene Representation and Rendering},
  author = {Eslami, S. M. Ali and Rezende, Danilo Jimenez and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2018},
  month = jun,
  volume = {360},
  pages = {1204--1210},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar6170},
  abstract = {A scene-internalizing computer program
To train a computer to ``recognize'' elements of a scene supplied by its visual sensors, computer scientists typically use millions of images painstakingly labeled by humans. Eslami et al. developed an artificial vision system, dubbed the Generative Query Network (GQN), that has no need for such labeled data. Instead, the GQN first uses images taken from different viewpoints and creates an abstract description of the scene, learning its essentials. Next, on the basis of this representation, the network predicts what the scene would look like from a new, arbitrary viewpoint.
Science, this issue p. 1204
Scene representation\textemdash the process of converting visual sensory data into concise descriptions\textemdash is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.
A computer vision system predicts how a 3D scene looks from any viewpoint after just a few 2D views from other viewpoints.
A computer vision system predicts how a 3D scene looks from any viewpoint after just a few 2D views from other viewpoints.},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/x0r/Zotero/storage/TBDCRR9Y/Eslami et al_2018_Neural scene representation and rendering.pdf},
  journal = {Science},
  keywords = {dl},
  language = {en},
  number = {6394},
  pmid = {29903970}
}

@article{esser,
  title = {Backpropagation for {{Energy}}-{{Efficient Neuromorphic Computing}}},
  author = {Esser, Steve K and Appuswamy, Rathinakumar and Merolla, Paul and Arthur, John V and Modha, Dharmendra S},
  pages = {9},
  abstract = {Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets. For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses. Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of 64), we achieve 99.42\% accuracy at 108 \textmu J per image, and with a high efficiency network (ensemble of 1) we achieve 92.7\% accuracy at 0.268 \textmu J per image.},
  file = {/Users/x0r/Zotero/storage/VKUGGXM2/Esser et al_Backpropagation for Energy-Efficient Neuromorphic Computing.pdf},
  keywords = {neuromorphic},
  language = {en}
}

@article{esser2016,
  title = {Convolutional Networks for Fast, Energy-Efficient Neuromorphic Computing},
  author = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Andreopoulos, Alexander and Berg, David J. and McKinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and {di Nolfo}, Carmelo and Datta, Pallab and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
  year = {2016},
  month = oct,
  volume = {113},
  pages = {11441--11446},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1604850113},
  file = {/Users/x0r/Zotero/storage/9GLWIBDT/Esser et al_2016_Convolutional networks for fast, energy-efficient neuromorphic computing.pdf;/Users/x0r/Zotero/storage/RWRGXUYV/Esser et al_2016_Convolutional networks for fast, energy-efficient neuromorphic computing.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {convnet,neuromorphic},
  language = {en},
  number = {41}
}

@article{evans2017,
  title = {Learning {{Explanatory Rules}} from {{Noisy Data}}},
  author = {Evans, Richard and Grefenstette, Edward},
  year = {2017},
  month = nov,
  abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data\textemdash which is not necessarily easily obtained\textemdash that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
  archivePrefix = {arXiv},
  eprint = {1711.04574},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Y3N5SEHL/Evans_Grefenstette_2017_Learning Explanatory Rules from Noisy Data.pdf},
  journal = {arXiv:1711.04574 [cs, math]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, math}
}

@article{evans2018,
  title = {{{CAN NEURAL NETWORKS UNDERSTAND LOGICAL ENTAILMENT}}?},
  author = {Evans, Richard and Saxton, David and Amos, David and Kohli, Pushmeet and Grefenstette, Edward},
  year = {2018},
  pages = {15},
  abstract = {We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class\textemdash PossibleWorldNets\textemdash which computes entailment as a ``convolution over possible worlds''. Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, treestructured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.},
  file = {/Users/x0r/Zotero/storage/TV28IWU4/Evans et al_2018_CAN NEURAL NETWORKS UNDERSTAND LOGICAL ENTAILMENT.pdf},
  journal = {ICLR 2018},
  keywords = {dl},
  language = {en}
}

@article{eysenbach2019,
  title = {Search on the {{Replay Buffer}}: {{Bridging Planning}} and {{Reinforcement Learning}}},
  shorttitle = {Search on the {{Replay Buffer}}},
  author = {Eysenbach, Benjamin and Salakhutdinov, Ruslan and Levine, Sergey},
  year = {2019},
  month = jun,
  abstract = {The history of learning for control has been an exciting back and forth between two broad classes of algorithms: planning and reinforcement learning. Planning algorithms effectively reason over long horizons, but assume access to a local policy and distance metric over collision-free paths. Reinforcement learning excels at learning policies and the relative values of states, but fails to plan over long horizons. Despite the successes of each method in various domains, tasks that require reasoning over long horizons with limited feedback and high-dimensional observations remain exceedingly challenging for both planning and reinforcement learning algorithms. Frustratingly, these sorts of tasks are potentially the most useful, as they are simple to design (a human only need to provide an example goal state) and avoid reward shaping, which can bias the agent towards finding a sub-optimal solution. We introduce a general control algorithm that combines the strengths of planning and reinforcement learning to effectively solve these tasks. Our aim is to decompose the task of reaching a distant goal state into a sequence of easier tasks, each of which corresponds to reaching a subgoal. Planning algorithms can automatically find these waypoints, but only if provided with suitable abstractions of the environment -- namely, a graph consisting of nodes and edges. Our main insight is that this graph can be constructed via reinforcement learning, where a goal-conditioned value function provides edge weights, and nodes are taken to be previously seen observations in a replay buffer. Using graph search over our replay buffer, we can automatically generate this sequence of subgoals, even in image-based environments. Our algorithm, search on the replay buffer (SoRB), enables agents to solve sparse reward tasks over one hundred steps, and generalizes substantially better than standard RL algorithms.},
  archivePrefix = {arXiv},
  eprint = {1906.05253},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LEGQQT2J/Eysenbach et al_2019_Search on the Replay Buffer.pdf;/Users/x0r/Zotero/storage/LQNDK7H8/1906.html},
  journal = {arXiv:1906.05253 [cs]},
  keywords = {rl},
  primaryClass = {cs}
}

@article{fantini2012,
  title = {Band Gap Widening with Time Induced by Structural Relaxation in Amorphous {{Ge}} {\textsubscript{2}} {{Sb}} {\textsubscript{2}} {{Te}} {\textsubscript{5}} Films},
  author = {Fantini, P. and Brazzelli, S. and Cazzini, E. and Mani, A.},
  year = {2012},
  month = jan,
  volume = {100},
  pages = {013505},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.3674311},
  file = {/Users/x0r/Zotero/storage/6JCXE9J7/Fantini et al_2012_Band gap widening with time induced by structural relaxation in amorphous Ge.pdf},
  journal = {Applied Physics Letters},
  keywords = {GST},
  language = {en},
  number = {1}
}

@article{Farabet_etal13,
  ids = {farabet2013},
  title = {Learning Hierarchical Features for Scene Labeling},
  author = {Farabet, C. and Couprie, C. and Najman, L. and Le Cun, Y.},
  year = {2013},
  month = aug,
  volume = {35},
  pages = {1915--1929},
  doi = {10.1109/TPAMI.2012.231},
  file = {/Users/x0r/Zotero/storage/2TLHKL3X/Farabet et al_2013_Learning Hierarchical Features for Scene Labeling.pdf},
  journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  keywords = {Accuracy,Barcelona dataset,Context,contextual information capturing,Convolutional networks,deep learning,dense feature vector extraction,dl,feature extraction,Feature extraction,hierarchical feature learning,image classification,Image edge detection,image labeling,image pixel labeling,image segmentation,Image segmentation,image texture,Labeling,multiple size region encoding,multiscale convolutional network,near-record accuracy,object category,scene labeling,scene parsing,segmentation components,segmentation tree,shape information capturing,shape recognition,SIFT flow dataset,Stanford background dataset,texture information capturing,transforms,trees (mathematics),Vectors},
  number = {8}
}

@article{feinberg2018,
  title = {Model-{{Based Value Estimation}} for {{Efficient Model}}-{{Free Reinforcement Learning}}},
  author = {Feinberg, Vladimir and Wan, Alvin and Stoica, Ion and Jordan, Michael I. and Gonzalez, Joseph E. and Levine, Sergey},
  year = {2018},
  month = feb,
  abstract = {Recent model-free reinforcement learning algorithms have proposed incorporating learned dynamics models as a source of additional data with the intention of reducing sample complexity. Such methods hold the promise of incorporating imagined data coupled with a notion of model uncertainty to accelerate the learning of continuous control tasks. Unfortunately, they rely on heuristics that limit usage of the dynamics model. We present model-based value expansion, which controls for uncertainty in the model by only allowing imagination to fixed depth. By enabling wider use of learned dynamics models within a model-free reinforcement learning algorithm, we improve value estimation, which, in turn, reduces the sample complexity of learning.},
  archivePrefix = {arXiv},
  eprint = {1803.00101},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ZMQIDTM4/Feinberg et al_2018_Model-Based Value Estimation for Efficient Model-Free Reinforcement Learning.pdf},
  journal = {arXiv:1803.00101 [cs, stat]},
  keywords = {rl,spinning-up},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fernando2017,
  title = {{{PathNet}}: {{Evolution Channels Gradient Descent}} in {{Super Neural Networks}}},
  shorttitle = {{{PathNet}}},
  author = {Fernando, Chrisantha and Banarse, Dylan and Blundell, Charles and Zwols, Yori and Ha, David and Rusu, Andrei A. and Pritzel, Alexander and Wierstra, Daan},
  year = {2017},
  month = jan,
  abstract = {For artificial general intelligence (AGI) it would be efficient if multiple users trained the same giant neural network, permitting parameter reuse, without catastrophic forgetting. PathNet is a first step in this direction. It is a neural network algorithm that uses agents embedded in the neural network whose task is to discover which parts of the network to re-use for new tasks. Agents are pathways (views) through the network which determine the subset of parameters that are used and updated by the forwards and backwards passes of the backpropogation algorithm. During learning, a tournament selection genetic algorithm is used to select pathways through the neural network for replication and mutation. Pathway fitness is the performance of that pathway measured according to a cost function. We demonstrate successful transfer learning; fixing the parameters along a path learned on task A and re-evolving a new population of paths for task B, allows task B to be learned faster than it could be learned from scratch or after fine-tuning. Paths evolved on task B re-use parts of the optimal path evolved on task A. Positive transfer was demonstrated for binary MNIST, CIFAR, and SVHN supervised learning classification tasks, and a set of Atari and Labyrinth reinforcement learning tasks, suggesting PathNets have general applicability for neural network training. Finally, PathNet also significantly improves the robustness to hyperparameter choices of a parallel asynchronous reinforcement learning algorithm (A3C).},
  archivePrefix = {arXiv},
  eprint = {1701.08734},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JGLIM3BM/Fernando et al_2017_PathNet.pdf},
  journal = {arXiv:1701.08734 [cs]},
  keywords = {dl,evolution},
  language = {en},
  primaryClass = {cs}
}

@techreport{feulner2020,
  title = {Neural Manifold under Plasticity in a Goal Driven Learning Behaviour},
  author = {Feulner, Barbara and Clopath, Claudia},
  year = {2020},
  month = feb,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.02.21.959163},
  abstract = {Neural activity is often low dimensional and dominated by only a few prominent neural covariation patterns. It has been hypothesised that these covariation patterns could form the building blocks used for fast and flexible motor control. Supporting this idea, recent experiments have shown that monkeys can learn to adapt their neural activity in motor cortex on a timescale of minutes, given that the change lies within the original low-dimensional subspace, also called neural manifold. However, the neural mechanism underlying this within-manifold adaptation remains unknown. Here, we show in a computational model that modification of recurrent weights, driven by a learned feedback signal, can account for the observed behavioural difference between within- and outside-manifold learning. Our findings give a new perspective, showing that recurrent weight changes do not necessarily lead to change in the neural manifold. On the contrary, successful learning is naturally constrained to a common subspace.},
  file = {/Users/x0r/Zotero/storage/7X2KM67W/Feulner and Clopath - 2020 - Neural manifold under plasticity in a goal driven .pdf},
  language = {en},
  type = {Preprint}
}

@article{fischer2014,
  title = {Serial Dependence in Visual Perception},
  author = {Fischer, Jason and Whitney, David},
  year = {2014},
  month = may,
  volume = {17},
  pages = {738--743},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3689},
  file = {/Users/x0r/Zotero/storage/A3HWELW6/Fischer_Whitney_2014_Serial dependence in visual perception.pdf},
  journal = {Nature Neuroscience},
  keywords = {neuroscience,vision},
  language = {en},
  number = {5}
}

@article{fleck2016,
  title = {Uniting {{Gradual}} and {{Abrupt}} Set {{Processes}} in {{Resistive Switching Oxides}}},
  author = {Fleck, Karsten and La Torre, Camilla and Aslam, Nabeel and {Hoffmann-Eifert}, Susanne and B{\"o}ttger, Ulrich and Menzel, Stephan},
  year = {2016},
  month = dec,
  volume = {6},
  issn = {2331-7019},
  doi = {10.1103/PhysRevApplied.6.064015},
  file = {/Users/x0r/Zotero/storage/B8BT3TU9/Fleck et al_2016_Uniting Gradual and Abrupt set Processes in Resistive Switching Oxides.pdf},
  journal = {Physical Review Applied},
  keywords = {modeling,ReRAM},
  language = {en},
  number = {6}
}

@article{foerster2016,
  title = {Learning to {{Communicate}} with {{Deep Multi}}-{{Agent Reinforcement Learning}}},
  author = {Foerster, Jakob N. and Assael, Yannis M. and {de Freitas}, Nando and Whiteson, Shimon},
  year = {2016},
  month = may,
  abstract = {We consider the problem of multiple agents sensing and acting in environments with the goal of maximising their shared utility. In these environments, agents must learn communication protocols in order to share information that is needed to solve the tasks. By embracing deep neural networks, we are able to demonstrate endto-end learning of protocols in complex environments inspired by communication riddles and multi-agent computer vision problems with partial observability. We propose two approaches for learning in these domains: Reinforced Inter-Agent Learning (RIAL) and Differentiable Inter-Agent Learning (DIAL). The former uses deep Q-learning, while the latter exploits the fact that, during learning, agents can backpropagate error derivatives through (noisy) communication channels. Hence, this approach uses centralised learning but decentralised execution. Our experiments introduce new environments for studying the learning of communication protocols and present a set of engineering innovations that are essential for success in these domains.},
  archivePrefix = {arXiv},
  eprint = {1605.06676},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/G9YK4VVS/Foerster et al_2016_Learning to Communicate with Deep Multi-Agent Reinforcement Learning.pdf},
  journal = {arXiv:1605.06676 [cs]},
  keywords = {AGI,rl,To read},
  language = {en},
  primaryClass = {cs}
}

@article{fortunato2019,
  title = {Generalization of {{Reinforcement Learners}} with {{Working}} and {{Episodic Memory}}},
  author = {Fortunato, Meire and Tan, Melissa and Faulkner, Ryan and Hansen, Steven and Badia, Adri{\`a} Puigdom{\`e}nech and Buttimore, Gavin and Deck, Charlie and Leibo, Joel Z. and Blundell, Charles},
  year = {2019},
  month = oct,
  abstract = {Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models. However, little progress has been made in understanding when specific memory systems help more than others and how well they generalize. The field also has yet to see a prevalent consistent and rigorous approach for evaluating agent performance on holdout data. In this paper, we aim to develop a comprehensive methodology to test different kinds of memory in an agent and assess how well the agent can apply what it learns in training to a holdout set that differs from the training set along dimensions that we suggest are relevant for evaluating memory-specific generalization. To that end, we first construct a diverse set of memory tasks1 that allow us to evaluate test-time generalization across multiple dimensions. Second, we develop and perform multiple ablations on an agent architecture that combines multiple memory systems, observe its baseline models, and investigate its performance against the task suite.},
  archivePrefix = {arXiv},
  eprint = {1910.13406},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NRQMGBWY/Fortunato et al. - 2019 - Generalization of Reinforcement Learners with Work.pdf},
  journal = {arXiv:1910.13406 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fortunato2020,
  title = {Generalization of {{Reinforcement Learners}} with {{Working}} and {{Episodic Memory}}},
  author = {Fortunato, Meire and Tan, Melissa and Faulkner, Ryan and Hansen, Steven and Badia, Adri{\`a} Puigdom{\`e}nech and Buttimore, Gavin and Deck, Charlie and Leibo, Joel Z. and Blundell, Charles},
  year = {2020},
  month = feb,
  abstract = {Memory is an important aspect of intelligence and plays a role in many deep reinforcement learning models. However, little progress has been made in understanding when specific memory systems help more than others and how well they generalize. The field also has yet to see a prevalent consistent and rigorous approach for evaluating agent performance on holdout data. In this paper, we aim to develop a comprehensive methodology to test different kinds of memory in an agent and assess how well the agent can apply what it learns in training to a holdout set that differs from the training set along dimensions that we suggest are relevant for evaluating memory-specific generalization. To that end, we first construct a diverse set of memory tasks1 that allow us to evaluate test-time generalization across multiple dimensions. Second, we develop and perform multiple ablations on an agent architecture that combines multiple memory systems, observe its baseline models, and investigate its performance against the task suite.},
  archivePrefix = {arXiv},
  eprint = {1910.13406},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GQMK8HG6/Fortunato et al. - 2020 - Generalization of Reinforcement Learners with Work.pdf},
  journal = {arXiv:1910.13406 [cs, stat]},
  keywords = {To read},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fouda2019,
  title = {Spiking {{Neural Networks}} for {{Inference}} and {{Learning}}: {{A Memristor}}-Based {{Design Perspective}}},
  shorttitle = {Spiking {{Neural Networks}} for {{Inference}} and {{Learning}}},
  author = {Fouda, M. E. and Kurdahi, F. and Eltawil, A. and Neftci, E.},
  year = {2019},
  month = oct,
  abstract = {On metrics of density and power efficiency, neuromorphic technologies have the potential to surpass mainstream computing technologies in tasks where real-time functionality, adaptability, and autonomy are essential. While algorithmic advances in neuromorphic computing are proceeding successfully, the potential of memristors to improve neuromorphic computing have not yet born fruit, primarily because they are often used as a drop-in replacement to conventional memory. However, interdisciplinary approaches anchored in machine learning theory suggest that multifactor plasticity rules matching neural and synaptic dynamics to the device capabilities can take better advantage of memristor dynamics and its stochasticity. Furthermore, such plasticity rules generally show much higher performance than that of classical Spike Time Dependent Plasticity (STDP) rules. This chapter reviews the recent development in learning with spiking neural network models and their possible implementation with memristor-based hardware.},
  archivePrefix = {arXiv},
  eprint = {1909.01771},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/MXF2TZIA/Fouda et al. - 2019 - Spiking Neural Networks for Inference and Learning.pdf},
  journal = {arXiv:1909.01771 [cs]},
  keywords = {neuromorphic,SNN},
  language = {en},
  primaryClass = {cs}
}

@article{fraccaro2016,
  title = {Sequential {{Neural Models}} with {{Stochastic Layers}}},
  author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
  year = {2016},
  month = may,
  abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
  archivePrefix = {arXiv},
  eprint = {1605.07571},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/HMX87J94/Fraccaro et al_2016_Sequential Neural Models with Stochastic Layers.pdf},
  journal = {arXiv:1605.07571 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{francois-lavet2018,
  title = {An {{Introduction}} to {{Deep Reinforcement Learning}}},
  author = {{Francois-Lavet}, Vincent and Henderson, Peter and Islam, Riashat and Bellemare, Marc G. and Pineau, Joelle},
  year = {2018},
  volume = {11},
  pages = {219--354},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000071},
  abstract = {Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decisionmaking tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.},
  archivePrefix = {arXiv},
  eprint = {1811.12560},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Y2677ZC9/Francois-Lavet et al_2018_An Introduction to Deep Reinforcement Learning.pdf},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  keywords = {rl},
  language = {en},
  number = {3-4}
}

@article{frascaroli2018,
  title = {Evidence of Soft Bound Behaviour in Analogue Memristive Devices for Neuromorphic Computing},
  author = {Frascaroli, Jacopo and Brivio, Stefano and Covi, Erika and Spiga, Sabina},
  year = {2018},
  month = dec,
  volume = {8},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-25376-x},
  file = {/Users/x0r/Zotero/storage/5F23C54J/Frascaroli et al_2018_Evidence of soft bound behaviour in analogue memristive devices for.pdf},
  journal = {Scientific Reports},
  keywords = {neuromorphic,ReRAM},
  language = {en},
  number = {1}
}

@article{fredens2019,
  title = {Total Synthesis of {{Escherichia}} Coli with a Recoded Genome},
  author = {Fredens, Julius and Wang, Kaihang and {de la Torre}, Daniel and Funke, Louise F. H. and Robertson, Wesley E. and Christova, Yonka and Chia, Tiongsun and Schmied, Wolfgang H. and Dunkelmann, Daniel L. and Ber{\'a}nek, V{\'a}clav and Uttamapinant, Chayasith and Llamazares, Andres Gonzalez and Elliott, Thomas S. and Chin, Jason W.},
  year = {2019},
  month = may,
  volume = {569},
  pages = {514--518},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1192-5},
  file = {/Users/x0r/Zotero/storage/TTDQRP7D/Fredens et al_2019_Total synthesis of Escherichia coli with a recoded genome.pdf},
  journal = {Nature},
  keywords = {biology,genetics},
  language = {en},
  number = {7757}
}

@article{fremaux2010,
  title = {Functional {{Requirements}} for {{Reward}}-{{Modulated Spike}}-{{Timing}}-{{Dependent Plasticity}}},
  author = {Fremaux, N. and Sprekeler, H. and Gerstner, W.},
  year = {2010},
  month = oct,
  volume = {30},
  pages = {13326--13337},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.6249-09.2010},
  file = {/Users/x0r/Zotero/storage/REJ56XHF/Fremaux et al_2010_Functional Requirements for Reward-Modulated Spike-Timing-Dependent Plasticity.pdf},
  journal = {Journal of Neuroscience},
  keywords = {SNN},
  language = {en},
  number = {40}
}

@article{fremaux2013,
  title = {Reinforcement {{Learning Using}} a {{Continuous Time Actor}}-{{Critic Framework}} with {{Spiking Neurons}}},
  author = {Fr{\'e}maux, Nicolas and Sprekeler, Henning and Gerstner, Wulfram},
  editor = {Graham, Lyle J.},
  year = {2013},
  month = apr,
  volume = {9},
  pages = {e1003024},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1003024},
  abstract = {Animals repeat rewarded behaviors, but the physiological basis of reward-based learning has only been partially elucidated. On one hand, experimental evidence shows that the neuromodulator dopamine carries information about rewards and affects synaptic plasticity. On the other hand, the theory of reinforcement learning provides a framework for reward-based learning. Recent models of reward-modulated spike-timing-dependent plasticity have made first steps towards bridging the gap between the two approaches, but faced two problems. First, reinforcement learning is typically formulated in a discrete framework, ill-adapted to the description of natural situations. Second, biologically plausible models of reward-modulated spike-timing-dependent plasticity require precise calculation of the reward prediction error, yet it remains to be shown how this can be computed by neurons. Here we propose a solution to these problems by extending the continuous temporal difference (TD) learning of Doya (2000) to the case of spiking neurons in an actor-critic network operating in continuous time, and with continuous state and action representations. In our model, the critic learns to predict expected future rewards in real time. Its activity, together with actual rewards, conditions the delivery of a neuromodulatory TD signal to itself and to the actor, which is responsible for action choice. In simulations, we show that such an architecture can solve a Morris water-maze-like navigation task, in a number of trials consistent with reported animal performance. We also use our model to solve the acrobot and the cartpole problems, two complex motor control tasks. Our model provides a plausible way of computing reward prediction error in the brain. Moreover, the analytically derived learning rule is consistent with experimental evidence for dopamine-modulated spike-timing-dependent plasticity.},
  file = {/Users/x0r/Zotero/storage/IXS5TZN3/Frémaux et al. - 2013 - Reinforcement Learning Using a Continuous Time Act.pdf},
  journal = {PLoS Computational Biology},
  language = {en},
  number = {4}
}

@article{fremaux2016,
  title = {Neuromodulated {{Spike}}-{{Timing}}-{{Dependent Plasticity}}, and {{Theory}} of {{Three}}-{{Factor Learning Rules}}},
  author = {Fr{\'e}maux, Nicolas and Gerstner, Wulfram},
  year = {2016},
  month = jan,
  volume = {9},
  issn = {1662-5110},
  doi = {10.3389/fncir.2015.00085},
  abstract = {Classical Hebbian learning puts the emphasis on joint pre- and postsynaptic activity, but neglects the potential role of neuromodulators. Since neuromodulators convey information about novelty or reward, the influence of neuromodulators on synaptic plasticity is useful not just for action learning in classical conditioning, but also to decide ``when'' to create new memories in response to a flow of sensory stimuli. In this review, we focus on timing requirements for pre- and postsynaptic activity in conjunction with one or several phasic neuromodulatory signals. While the emphasis of the text is on conceptual models and mathematical theories, we also discuss some experimental evidence for neuromodulation of Spike-Timing-Dependent Plasticity. We highlight the importance of synaptic mechanisms in bridging the temporal gap between sensory stimulation and neuromodulatory signals, and develop a framework for a class of neo-Hebbian three-factor learning rules that depend on presynaptic activity, postsynaptic variables as well as the influence of neuromodulators.},
  file = {/Users/x0r/Zotero/storage/J6FPZCUI/Frémaux_Gerstner_2016_Neuromodulated Spike-Timing-Dependent Plasticity, and Theory of Three-Factor.pdf},
  journal = {Frontiers in Neural Circuits},
  keywords = {neuromorphic,neuroscience,STDP},
  language = {en}
}

@article{frenkel1938,
  title = {On {{Pre}}-{{Breakdown Phenomena}} in {{Insulators}} and {{Electronic Semi}}-{{Conductors}}},
  author = {Frenkel, J.},
  year = {1938},
  month = oct,
  volume = {54},
  pages = {647--648},
  issn = {0031-899X},
  doi = {10.1103/PhysRev.54.647},
  file = {/Users/x0r/Zotero/storage/53WJLUNS/Frenkel_1938_On Pre-Breakdown Phenomena in Insulators and Electronic Semi-Conductors.pdf},
  journal = {Physical Review},
  keywords = {material},
  language = {en},
  number = {8}
}

@article{friedrich2010,
  title = {Learning {{Spike}}-{{Based Population Codes}} by {{Reward}} and {{Population Feedback}}},
  author = {Friedrich, Johannes and Urbanczik, Robert and Senn, Walter},
  year = {2010},
  month = jul,
  volume = {22},
  pages = {1698--1717},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.2010.05-09-1010},
  file = {/Users/x0r/Zotero/storage/53VTR5PS/Friedrich et al_2010_Learning Spike-Based Population Codes by Reward and Population Feedback.pdf},
  journal = {Neural Computation},
  keywords = {neuroscience,SNN,STDP},
  language = {en},
  number = {7}
}

@article{friston2016,
  title = {Active Inference and Learning},
  author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and O⿿Doherty, John and Pezzulo, Giovanni},
  year = {2016},
  month = sep,
  volume = {68},
  pages = {862--879},
  issn = {01497634},
  doi = {10.1016/j.neubiorev.2016.06.022},
  abstract = {This paper offers an active inference account of choice behaviour and learning. It focuses on the distinction between goal-directed and habitual behaviour and how they contextualise each other. We show that habits emerge naturally (and autodidactically) from sequential policy optimisation when agents are equipped with state-action policies. In active inference, behaviour has explorative (epistemic) and exploitative (pragmatic) aspects that are sensitive to ambiguity and risk respectively, where epistemic (ambiguity-resolving) behaviour enables pragmatic (reward-seeking) behaviour and the subsequent emergence of habits. Although goal-directed and habitual policies are usually associated with model-based and model-free schemes, we find the more important distinction is between belief-free and belief-based schemes. The underlying (variational) belief updating provides a comprehensive (if metaphorical) process theory for several phenomena, including the transfer of dopamine responses, reversal learning, habit formation and devaluation. Finally, we show that active inference reduces to a classical (Bellman) scheme, in the absence of ambiguity.},
  file = {/Users/x0r/Zotero/storage/L87I325I/Friston et al_2016_Active inference and learning.pdf},
  journal = {Neuroscience \& Biobehavioral Reviews},
  language = {en}
}

@article{friston2017,
  title = {Active {{Inference}}: {{A Process Theory}}},
  shorttitle = {Active {{Inference}}},
  author = {Friston, Karl and FitzGerald, Thomas and Rigoli, Francesco and Schwartenbeck, Philipp and Pezzulo, Giovanni},
  year = {2017},
  month = jan,
  volume = {29},
  pages = {1--49},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00912},
  file = {/Users/x0r/Zotero/storage/NVEWIU8V/Friston et al_2017_Active Inference.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {1}
}

@article{froese2009,
  title = {Enactive Artificial Intelligence: {{Investigating}} the Systemic Organization of Life and Mind},
  shorttitle = {Enactive Artificial Intelligence},
  author = {Froese, Tom and Ziemke, Tom},
  year = {2009},
  month = mar,
  volume = {173},
  pages = {466--500},
  issn = {00043702},
  doi = {10.1016/j.artint.2008.12.001},
  abstract = {The embodied and situated approach to artificial intelligence (AI) has matured and become a viable alternative to traditional computationalist approaches with respect to the practical goal of building artificial agents, which can behave in a robust and flexible manner under changing real-world conditions. Nevertheless, some concerns have recently been raised with regard to the sufficiency of current embodied AI for advancing our scientific understanding of intentional agency. While from an engineering or computer science perspective this limitation might not be relevant, it is of course highly relevant for AI researchers striving to build accurate models of natural cognition. We argue that the biological foundations of enactive cognitive science can provide the conceptual tools that are needed to diagnose more clearly the shortcomings of current embodied AI. In particular, taking an enactive perspective points to the need for AI to take seriously the organismic roots of autonomous agency and sense-making. We identify two necessary systemic requirements, namely constitutive autonomy and adaptivity, which lead us to introduce two design principles of enactive AI. It is argued that the development of such enactive AI poses a significant challenge to current methodologies. However, it also provides a promising way of eventually overcoming the current limitations of embodied AI, especially in terms of providing fuller models of natural embodied cognition. Finally, some practical implications and examples of the two design principles of enactive AI are also discussed.},
  file = {/Users/x0r/Zotero/storage/5TICEXQG/Froese_Ziemke_2009_Enactive artificial intelligence.pdf;/Users/x0r/Zotero/storage/7XZIQJKN/Froese_Ziemke_2009_Enactive artificial intelligence.pdf},
  journal = {Artificial Intelligence},
  keywords = {AGI,rl,To read},
  language = {en},
  number = {3-4}
}

@article{fu2019,
  title = {From {{Language}} to {{Goals}}: {{Inverse Reinforcement Learning}} for {{Vision}}-{{Based Instruction Following}}},
  shorttitle = {From {{Language}} to {{Goals}}},
  author = {Fu, Justin and Korattikara, Anoop and Levine, Sergey and Guadarrama, Sergio},
  year = {2019},
  month = feb,
  abstract = {Reinforcement learning is a promising framework for solving control problems,
but its use in practical situations is hampered by the fact that reward
functions are often difficult to engineer. Specifying goals and tasks for
autonomous machines, such as robots, is a significant challenge:
conventionally, reward functions and goal states have been used to communicate
objectives. But people can communicate objectives to each other simply by
describing or demonstrating them. How can we build learning algorithms that
will allow us to tell machines what we want them to do? In this work, we
investigate the problem of grounding language commands as reward functions
using inverse reinforcement learning, and argue that language-conditioned
rewards are more transferable than language-conditioned policies to new
environments. We propose language-conditioned reward learning (LC-RL), which
grounds language commands as a reward function represented by a deep neural
network. We demonstrate that our model learns rewards that transfer to novel
tasks and environments on realistic, high-dimensional visual environments with
natural language commands, whereas directly learning a language-conditioned
policy leads to poor performance.},
  file = {/Users/x0r/Zotero/storage/8FXQJ7DR/Fu et al_2019_From Language to Goals.pdf},
  keywords = {AGI,rl},
  language = {en}
}

@inproceedings{fugazza2010,
  title = {Temperature- and Time-Dependent Conduction Controlled by Activation Energy in {{PCM}}},
  author = {Fugazza, D. and Ielmini, D. and Montemurro, G. and Lacaita, A. L.},
  year = {2010},
  month = dec,
  pages = {29.3.1-29.3.4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2010.5703443},
  abstract = {A deep insight in the conduction processes in phase-change memory (PCM) devices is needed for a reliable size scaling of the technology. In this paper we discuss and model the temperature and time dependences of the programmed cell resistance. The non-Arrhenius behavior of the resistance is interpreted by temperature-dependent current localization in the frame of the distributed Poole-Frenkel model. Experimental evidence is shown for current noise and drift being dictated by changes of the activation energy for hopping conduction. Models for current noise and drift are developed and used for projections as a function of the PCM technology node.},
  file = {/Users/x0r/Zotero/storage/WSYJ3QK2/Fugazza et al_2010_Temperature- and time-dependent conduction controlled by activation energy in.pdf},
  isbn = {978-1-4424-7418-5},
  keywords = {neuromorphic,PCM},
  language = {en}
}

@article{fujimoto2018,
  title = {Addressing {{Function Approximation Error}} in {{Actor}}-{{Critic Methods}}},
  author = {Fujimoto, Scott and {van Hoof}, Herke and Meger, David},
  year = {2018},
  month = feb,
  abstract = {In value-based reinforcement learning methods such as deep Q-learning, function approximation errors are known to lead to overestimated value estimates and suboptimal policies. We show that this problem persists in an actor-critic setting and propose novel mechanisms to minimize its effects on both the actor and the critic. Our algorithm builds on Double Q-learning, by taking the minimum value between a pair of critics to limit overestimation. We draw the connection between target networks and overestimation bias, and suggest delaying policy updates to reduce per-update error and further improve performance. We evaluate our method on the suite of OpenAI gym tasks, outperforming the state of the art in every environment tested.},
  archivePrefix = {arXiv},
  eprint = {1802.09477},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/BLQ44DT6/Fujimoto et al_2018_Addressing Function Approximation Error in Actor-Critic Methods.pdf},
  journal = {arXiv:1802.09477 [cs, stat]},
  keywords = {rl,spinning-up},
  primaryClass = {cs, stat}
}

@article{Fusi_etal05,
  ids = {fusi2005},
  title = {Cascade Models of Synaptically Stored Memories},
  author = {Fusi, S. and Drew, P.J. and Abbott, L.F.},
  year = {2005},
  volume = {45},
  pages = {599--611},
  file = {/Users/x0r/Zotero/storage/EQDYBK9C/Fusi et al_2005_Cascade Models of Synaptically Stored Memories.pdf},
  journal = {Neuron},
  keywords = {neuroscience}
}

@article{gabardi2015,
  title = {Microscopic Origin of Resistance Drift in the Amorphous State of the Phase-Change Compound {{GeTe}}},
  author = {Gabardi, S. and Caravati, S. and Sosso, G. C. and Behler, J. and Bernasconi, M.},
  year = {2015},
  month = aug,
  volume = {92},
  issn = {1098-0121, 1550-235X},
  doi = {10.1103/PhysRevB.92.054201},
  file = {/Users/x0r/Zotero/storage/JVVNKMAC/Gabardi et al_2015_Microscopic origin of resistance drift in the amorphous state of the.pdf},
  journal = {Physical Review B},
  keywords = {modeling,PCM},
  language = {en},
  number = {5}
}

@article{gaier2019,
  title = {Weight {{Agnostic Neural Networks}}},
  author = {Gaier, Adam and Ha, David},
  year = {2019},
  month = sep,
  abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  archivePrefix = {arXiv},
  eprint = {1906.04358},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NUZ6F8HD/Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf;/Users/x0r/Zotero/storage/WZX8YPXE/1906.html},
  journal = {arXiv:1906.04358 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gallo2015,
  title = {Subthreshold Electrical Transport in Amorphous Phase-Change Materials},
  author = {Gallo, Manuel Le and Kaes, Matthias and Sebastian, Abu and Krebs, Daniel},
  year = {2015},
  month = sep,
  volume = {17},
  pages = {093035},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/17/9/093035},
  file = {/Users/x0r/Zotero/storage/I69KDX8B/Gallo et al_2015_Subthreshold electrical transport in amorphous phase-change materials.pdf},
  journal = {New Journal of Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {9}
}

@article{gangwani2017,
  title = {Policy {{Optimization}} by {{Genetic Distillation}}},
  author = {Gangwani, Tanmay and Peng, Jian},
  year = {2017},
  month = nov,
  abstract = {Genetic algorithms have been widely used in many practical optimization problems. Inspired by natural selection, operators, including mutation, crossover and selection, provide effective heuristics for search and black-box optimization. However, they have not been shown useful for deep reinforcement learning, possibly due to the catastrophic consequence of parameter crossovers of neural networks. Here, we present Genetic Policy Optimization (GPO), a new genetic algorithm for sample-efficient deep policy optimization. GPO uses imitation learning for policy crossover in the state space and applies policy gradient methods for mutation. Our experiments on MuJoCo tasks show that GPO as a genetic algorithm is able to provide superior performance over the state-of-the-art policy gradient methods and achieves comparable or higher sample efficiency.},
  archivePrefix = {arXiv},
  eprint = {1711.01012},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/69GR8YDI/Gangwani_Peng_2017_Policy Optimization by Genetic Distillation.pdf;/Users/x0r/Zotero/storage/9456SPUQ/1711.html},
  journal = {arXiv:1711.01012 [cs, stat]},
  keywords = {evolution,rl},
  primaryClass = {cs, stat}
}

@article{gao2015,
  title = {On Simplicity and Complexity in the Brave New World of Large-Scale Neuroscience},
  author = {Gao, Peiran and Ganguli, Surya},
  year = {2015},
  month = jun,
  volume = {32},
  pages = {148--155},
  issn = {09594388},
  doi = {10.1016/j.conb.2015.04.003},
  file = {/Users/x0r/Zotero/storage/SETFPZ4H/Gao and Ganguli - 2015 - On simplicity and complexity in the brave new worl.pdf},
  journal = {Current Opinion in Neurobiology},
  keywords = {To read},
  language = {en}
}

@article{geffner,
  title = {Model-Free, {{Model}}-Based, and {{General Intelligence}}},
  author = {Geffner, Hector},
  pages = {8},
  abstract = {During the 60s and 70s, AI researchers explored intuitions about intelligence by writing programs that displayed intelligent behavior. Many good ideas came out from this work but programs written by hand were not robust or general. After the 80s, research increasingly shifted to the development of learners capable of inferring behavior and functions from experience and data, and solvers capable of tackling well-defined but intractable models like SAT, classical planning, Bayesian networks, and POMDPs. The learning approach has achieved considerable success but results in black boxes that do not have the flexibility, transparency, and generality of their model-based counterparts. Modelbased approaches, on the other hand, require models and scalable algorithms. Model-free learners and model-based solvers have close parallels with Systems 1 and 2 in current theories of the human mind: the first, a fast, opaque, and inflexible intuitive mind; the second, a slow, transparent, and flexible analytical mind. In this paper, I review developments in AI and draw on these theories to discuss the gap between model-free learners and model-based solvers, a gap that needs to be bridged in order to have intelligent systems that are robust and general.},
  file = {/Users/x0r/Zotero/storage/M3L4UIQ5/Geffner_Model-free, Model-based, and General Intelligence.pdf},
  keywords = {AGI,To read},
  language = {en}
}

@article{gemici2017,
  title = {Generative {{Temporal Models}} with {{Memory}}},
  author = {Gemici, Mevlana and Hung, Chia-Chun and Santoro, Adam and Wayne, Greg and Mohamed, Shakir and Rezende, Danilo J. and Amos, David and Lillicrap, Timothy},
  year = {2017},
  month = feb,
  abstract = {We consider the general problem of modeling temporal data with long-range dependencies, wherein new observations are fully or partially predictable based on temporally-distant, past observations. A sufficiently powerful temporal model should separate predictable elements of the sequence from unpredictable elements, express uncertainty about those unpredictable elements, and rapidly identify novel elements that may help to predict the future. To create such models, we introduce Generative Temporal Models augmented with external memory systems. They are developed within the variational inference framework, which provides both a practical training methodology and methods to gain insight into the models' operation. We show, on a range of problems with sparse, long-term temporal dependencies, that these models store information from early in a sequence, and reuse this stored information efficiently. This allows them to perform substantially better than existing models based on well-known recurrent neural networks, like LSTMs.},
  archivePrefix = {arXiv},
  eprint = {1702.04649},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/B8VV396W/Gemici et al_2017_Generative Temporal Models with Memory.pdf},
  journal = {arXiv:1702.04649 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gershman2017,
  title = {Reinforcement {{Learning}} and {{Episodic Memory}} in {{Humans}} and {{Animals}}: {{An Integrative Framework}}},
  shorttitle = {Reinforcement {{Learning}} and {{Episodic Memory}} in {{Humans}} and {{Animals}}},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  year = {2017},
  month = jan,
  volume = {68},
  pages = {101--128},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122414-033625},
  abstract = {We review the psychology and neuroscience of reinforcement learning (RL), which has experienced significant progress in the past two decades, enabled by the comprehensive experimental study of simple learning and decisionmaking tasks. However, one challenge in the study of RL is computational: The simplicity of these tasks ignores important aspects of reinforcement learning in the real world: (a) State spaces are high-dimensional, continuous, and partially observable; this implies that (b) data are relatively sparse and, indeed, precisely the same situation may never be encountered twice; furthermore, (c) rewards depend on the long-term consequences of actions in ways that violate the classical assumptions that make RL tractable. A seemingly distinct challenge is that, cognitively, theories of RL have largely involved procedural and semantic memory, the way in which knowledge about action values or world models extracted gradually from many experiences can drive choice. This focus on semantic memory leaves out many aspects of memory, such as episodic memory, related to the traces of individual events. We suggest that these two challenges are related. The computational challenge can be dealt with, in part, by endowing RL systems with episodic memory, allowing them to (a) efficiently approximate value functions over complex state spaces, (b) learn with very little data, and (c) bridge long-term dependencies between actions and rewards. We review the computational theory underlying this proposal and the empirical evidence to support it. Our proposal suggests that the ubiquitous and diverse roles of memory in RL may function as part of an integrated learning system.},
  file = {/Users/x0r/Zotero/storage/4RC3PZXN/Gershman_Daw_2017_Reinforcement Learning and Episodic Memory in Humans and Animals.pdf},
  journal = {Annual Review of Psychology},
  keywords = {neuroscience,rl},
  language = {en},
  number = {1}
}

@article{gerstner2009,
  title = {How {{Good Are Neuron Models}}?},
  author = {Gerstner, W. and Naud, R.},
  year = {2009},
  month = oct,
  volume = {326},
  pages = {379--380},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1181936},
  file = {/Users/x0r/Zotero/storage/M84RPZQL/Gerstner_Naud_2009_How Good Are Neuron Models.pdf},
  journal = {Science},
  keywords = {neuroscience},
  language = {en},
  number = {5951}
}

@book{gerstner2014,
  title = {Neuronal {{Dynamics}}: {{From Single Neurons}} to {{Networks}} and {{Models}} of {{Cognition}}},
  shorttitle = {Neuronal {{Dynamics}}},
  author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781107447615},
  file = {/Users/x0r/Zotero/storage/44DR6KYA/Gerstner et al_2014_Neuronal Dynamics.pdf},
  isbn = {978-1-107-44761-5},
  keywords = {neuroscience,SNN},
  language = {en}
}

@article{gerstner2018,
  title = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}: {{Experimental Support}} of {{NeoHebbian Three}}-{{Factor Learning Rules}}},
  shorttitle = {Eligibility {{Traces}} and {{Plasticity}} on {{Behavioral Time Scales}}},
  author = {Gerstner, Wulfram and Lehmann, Marco and Liakoni, Vasiliki and Corneil, Dane and Brea, Johanni},
  year = {2018},
  month = jul,
  volume = {12},
  issn = {1662-5110},
  doi = {10.3389/fncir.2018.00053},
  abstract = {Most elementary behaviors such as moving the arm to grasp an object or walking into the next room to explore a museum evolve on the time scale of seconds; in contrast, neuronal action potentials occur on the time scale of a few milliseconds. Learning rules of the brain must therefore bridge the gap between these two different time scales. Modern theories of synaptic plasticity have postulated that the co-activation of pre- and postsynaptic neurons sets a flag at the synapse, called an eligibility trace, that leads to a weight change only if an additional factor is present while the flag is set. This third factor, signaling reward, punishment, surprise, or novelty, could be implemented by the phasic activity of neuromodulators or specific neuronal inputs signaling special events. While the theoretical framework has been developed over the last decades, experimental evidence in support of eligibility traces on the time scale of seconds has been collected only during the last few years. Here we review, in the context of three-factor rules of synaptic plasticity, four key experiments that support the role of synaptic eligibility traces in combination with a third factor as a biological implementation of neoHebbian three-factor learning rules.},
  file = {/Users/x0r/Zotero/storage/5ADF3PKL/Gerstner et al_2018_Eligibility Traces and Plasticity on Behavioral Time Scales.pdf},
  journal = {Frontiers in Neural Circuits},
  keywords = {neuroscience,SNN},
  language = {en}
}

@article{gidon2020,
  title = {Dendritic Action Potentials and Computation in Human Layer 2/3 Cortical Neurons},
  author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
  year = {2020},
  month = jan,
  volume = {367},
  pages = {83--87},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax6239},
  abstract = {Dendritic action potentials extend the repertoire of computations available to human neurons.
Dendritic action potentials extend the repertoire of computations available to human neurons.},
  chapter = {Report},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/x0r/Zotero/storage/LJKXNYE6/Gidon et al. - 2020 - Dendritic action potentials and computation in hum.pdf;/Users/x0r/Zotero/storage/ZULK5V78/aax6239_Gidon_SM.pdf;/Users/x0r/Zotero/storage/NZHTRKIP/tab-pdf.html},
  journal = {Science},
  language = {en},
  number = {6473},
  pmid = {31896716}
}

@article{gildea,
  title = {Automatic {{Labeling}} of {{Semantic Roles}}},
  author = {Gildea, Daniel and Jurafsky, Daniel},
  volume = {28},
  pages = {44},
  file = {/Users/x0r/Zotero/storage/222B5M97/Gildea_Jurafsky_Automatic Labeling of Semantic Roles.pdf},
  journal = {Computational Linguistics},
  keywords = {nlp},
  language = {en},
  number = {3}
}

@article{gilra2017,
  title = {Predicting Non-Linear Dynamics by Stable Local Learning in a Recurrent Spiking Neural Network},
  author = {Gilra, Aditya and Gerstner, Wulfram},
  year = {2017},
  month = nov,
  volume = {6},
  issn = {2050-084X},
  doi = {10.7554/eLife.28295},
  abstract = {Brains need to predict how the body reacts to motor commands. It is an open question how networks of spiking neurons can learn to reproduce the non-linear body dynamics caused by motor commands, using local, online and stable learning rules. Here, we present a supervised learning scheme for the feedforward and recurrent connections in a network of heterogeneous spiking neurons. The error in the output is fed back through fixed random connections with a negative gain, causing the network to follow the desired dynamics, while an online and local rule changes the weights. The rule for Feedback-based Online Local Learning Of Weights (FOLLOW) is local in the sense that weight changes depend on the presynaptic activity and the error signal projected onto the postsynaptic neuron. We provide examples of learning linear, non-linear and chaotic dynamics, as well as the dynamics of a two-link arm. Using the Lyapunov method, and under reasonable assumptions and approximations, we show that FOLLOW learning is stable uniformly, with the error going to zero asymptotically.},
  archivePrefix = {arXiv},
  eprint = {1702.06463},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2Q3W8RBX/Gilra_Gerstner_2017_Predicting non-linear dynamics by stable local learning in a recurrent spiking.pdf},
  journal = {eLife},
  keywords = {RNN,SNN},
  language = {en}
}

@article{gokmen2016,
  title = {Acceleration of {{Deep Neural Network Training}} with {{Resistive Cross}}-{{Point Devices}}: {{Design Considerations}}},
  shorttitle = {Acceleration of {{Deep Neural Network Training}} with {{Resistive Cross}}-{{Point Devices}}},
  author = {Gokmen, Tayfun and Vlasov, Yurii},
  year = {2016},
  month = jul,
  volume = {10},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00333},
  abstract = {In recent years, deep neural networks (DNN) have demonstrated significant business impact in large scale analysis and classification tasks such as speech recognition, visual object detection, pattern extraction, etc. Training of large DNNs, however, is universally considered as time consuming and computationally intensive task that demands datacenter-scale computational resources recruited for many days. Here we propose a concept of resistive processing unit (RPU) devices that can potentially accelerate DNN training by orders of magnitude while using much less power. The proposed RPU device can store and update the weight values locally thus minimizing data movement during training and allowing to fully exploit the locality and the parallelism of the training algorithm. We evaluate the effect of various RPU device features/non-idealities and system parameters on performance in order to derive the device and system level specifications for implementation of an accelerator chip for DNN training in a realistic CMOS-compatible technology. For large DNNs with about 1 billion weights this massively parallel RPU architecture can achieve acceleration factors of 30, 000\texttimes{} compared to state-of-the-art microprocessors while providing power efficiency of 84, 000 GigaOps/s/W. Problems that currently require days of training on a datacenter-size cluster with thousands of machines can be addressed within hours on a single RPU accelerator. A system consisting of a cluster of RPU accelerators will be able to tackle Big Data problems with trillions of parameters that is impossible to address today like, for example, natural speech recognition and translation between all world languages, real-time analytics on large streams of business and scientific data, integration, and analysis of multimodal sensory data flows from a massive number of IoT (Internet of Things) sensors.},
  file = {/Users/x0r/Zotero/storage/8DRS9RTH/Gokmen_Vlasov_2016_Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices.pdf;/Users/x0r/Zotero/storage/BNZCVMTL/Gokmen_Vlasov_2016_Acceleration of Deep Neural Network Training with Resistive Cross-Point Devices.pdf;/Users/x0r/Zotero/storage/P56NUEMQ/1603.html},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic},
  language = {en}
}

@article{gokmen2017,
  title = {Training {{Deep Convolutional Neural Networks}} with {{Resistive Cross}}-{{Point Devices}}},
  author = {Gokmen, Tayfun and Onen, O. Murat and Haensch, Wilfried},
  year = {2017},
  month = may,
  abstract = {In a previous work we have detailed the requirements to obtain a maximal performance benefit by implementing fully connected deep neural networks (DNN) in form of arrays of resistive devices for deep learning. This concept of Resistive Processing Unit (RPU) devices we extend here towards convolutional neural networks (CNNs). We show how to map the convolutional layers to RPU arrays such that the parallelism of the hardware can be fully utilized in all three cycles of the backpropagation algorithm. We find that the noise and bound limitations imposed due to analog nature of the computations performed on the arrays effect the training accuracy of the CNNs. Noise and bound management techniques are presented that mitigate these problems without introducing any additional complexity in the analog circuits and can be addressed by the digital circuits. In addition, we discuss digitally programmable update management and device variability reduction techniques that can be used selectively for some of the layers in a CNN. We show that combination of all those techniques enables a successful application of the RPU concept for training CNNs. The techniques discussed here are more general and can be applied beyond CNN architectures and therefore enables applicability of RPU approach for large class of neural network architectures.},
  archivePrefix = {arXiv},
  eprint = {1705.08014},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VTIKEYSJ/Gokmen et al_2017_Training Deep Convolutional Neural Networks with Resistive Cross-Point Devices.pdf;/Users/x0r/Zotero/storage/7HGZY4SG/1705.html},
  journal = {arXiv:1705.08014 [cs, stat]},
  keywords = {dl,neuromorphic},
  primaryClass = {cs, stat}
}

@article{gomez-marin2019,
  title = {The {{Life}} of {{Behavior}}},
  author = {{Gomez-Marin}, Alex and Ghazanfar, Asif A.},
  year = {2019},
  month = oct,
  volume = {104},
  pages = {25--36},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.09.017},
  file = {/Users/x0r/Zotero/storage/IFB7A2BL/Gomez-Marin and Ghazanfar - 2019 - The Life of Behavior.pdf},
  journal = {Neuron},
  keywords = {To read},
  language = {en},
  number = {1}
}

@article{gong2018,
  title = {Signal and Noise Extraction from Analog Memory Elements for Neuromorphic Computing},
  author = {Gong, N. and Id{\'e}, T. and Kim, S. and Boybat, I. and Sebastian, A. and Narayanan, V. and Ando, T.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {2102},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04485-1},
  file = {/Users/x0r/Zotero/storage/SVLYCCG9/Gong et al. - 2018 - Signal and noise extraction from analog memory ele.pdf},
  journal = {Nature Communications},
  keywords = {PCM},
  language = {en},
  number = {1}
}

@book{Goodfellow_etal16,
  ids = {goodfellow2016},
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT press}},
  file = {/Users/x0r/Zotero/storage/H9NU3G8U/Goodfellow et al_2016_Deep learning.pdf}
}

@article{goodwill2017,
  title = {Electro-{{Thermal Model}} of {{Threshold Switching}} in {{TaO}} {\textsubscript{ {\emph{x}} }} -{{Based Devices}}},
  author = {Goodwill, Jonathan M. and Sharma, Abhishek A. and Li, Dasheng and Bain, James A. and Skowronski, Marek},
  year = {2017},
  month = apr,
  volume = {9},
  pages = {11704--11710},
  issn = {1944-8244, 1944-8252},
  doi = {10.1021/acsami.6b16559},
  file = {/Users/x0r/Zotero/storage/T5NVGQPQ/Goodwill et al_2017_Electro-Thermal Model of Threshold Switching in TaO sub ix-i -sub.pdf},
  journal = {ACS Applied Materials \& Interfaces},
  keywords = {threshold-switching},
  language = {en},
  number = {13}
}

@article{gordon2019,
  title = {Depth from {{Videos}} in the {{Wild}}: {{Unsupervised Monocular Depth Learning}} from {{Unknown Cameras}}},
  shorttitle = {Depth from {{Videos}} in the {{Wild}}},
  author = {Gordon, Ariel and Li, Hanhan and Jonschkowski, Rico and Angelova, Anelia},
  year = {2019},
  month = apr,
  abstract = {We present a novel method for simultaneous learning of depth, egomotion, object motion, and camera intrinsics from monocular videos, using only consistency across neighboring video frames as supervision signal. Similarly to prior work, our method learns by applying differentiable warping to frames and comparing the result to adjacent ones, but it provides several improvements: We address occlusions geometrically and differentiably, directly using the depth maps as predicted during training. We introduce randomized layer normalization, a novel powerful regularizer, and we account for object motion relative to the scene. To the best of our knowledge, our work is the first to learn the camera intrinsic parameters, including lens distortion, from video in an unsupervised manner, thereby allowing us to extract accurate depth and motion from arbitrary videos of unknown origin at scale. We evaluate our results on the Cityscapes, KITTI and EuRoC datasets, establishing new state of the art on depth prediction and odometry, and demonstrate qualitatively that depth prediction can be learned from a collection of YouTube videos.},
  archivePrefix = {arXiv},
  eprint = {1904.04998},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/MR36GBVT/Gordon et al_2019_Depth from Videos in the Wild.pdf},
  journal = {arXiv:1904.04998 [cs]},
  keywords = {dl,self-supervision},
  language = {en},
  primaryClass = {cs}
}

@book{graham,
  title = {On {{LISP}}},
  author = {Graham, Paul},
  file = {/Users/x0r/Zotero/storage/FAPPR9PU/onlisp.pdf}
}

@article{graves2016,
  title = {Stochastic {{Backpropagation}} through {{Mixture Density Distributions}}},
  author = {Graves, Alex},
  year = {2016},
  month = jul,
  abstract = {The ability to backpropagate stochastic gradients through continuous latent distributions has been crucial to the emergence of variational autoencoders [4, 6, 7, 3] and stochastic gradient variational Bayes [2, 5, 1]. The key ingredient is an unbiased and low-variance way of estimating gradients with respect to distribution parameters from gradients evaluated at distribution samples. The ``reparameterization trick'' [6] provides a class of transforms yielding such estimators for many continuous distributions, including the Gaussian and other members of the location-scale family. However the trick does not readily extend to mixture density models, due to the difficulty of reparameterizing the discrete distribution over mixture weights. This report describes an alternative transform, applicable to any continuous multivariate distribution with a differentiable density function from which samples can be drawn, and uses it to derive an unbiased estimator for mixture density weight derivatives. Combined with the reparameterization trick applied to the individual mixture components, this estimator makes it straightforward to train variational autoencoders with mixture-distributed latent variables, or to perform stochastic variational inference with a mixture density variational posterior.},
  archivePrefix = {arXiv},
  eprint = {1607.05690},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AI6H4TRB/Graves_2016_Stochastic Backpropagation through Mixture Density Distributions.pdf},
  journal = {arXiv:1607.05690 [cs]},
  keywords = {backprop},
  language = {en},
  primaryClass = {cs}
}

@article{green2007,
  title = {A 160-Kilobit Molecular Electronic Memory Patterned at 1011 Bits per Square Centimetre},
  author = {Green, Jonathan E. and Wook Choi, Jang and Boukai, Akram and Bunimovich, Yuri and {Johnston-Halperin}, Ezekiel and DeIonno, Erica and Luo, Yi and Sheriff, Bonnie A. and Xu, Ke and Shik Shin, Young and Tseng, Hsian-Rong and Stoddart, J. Fraser and Heath, James R.},
  year = {2007},
  month = jan,
  volume = {445},
  pages = {414--417},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature05462},
  file = {/Users/x0r/Zotero/storage/3NZFG4KA/Green et al_2007_A 160-kilobit molecular electronic memory patterned at 1011 bits per square.pdf},
  journal = {Nature},
  keywords = {neuromorphic},
  language = {en},
  number = {7126}
}

@article{gregor2016,
  title = {Towards {{Conceptual Compression}}},
  author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
  year = {2016},
  month = apr,
  abstract = {We introduce a simple recurrent variational autoencoder architecture that significantly improves image modeling. The system represents the stateof-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality `conceptual compression'.},
  archivePrefix = {arXiv},
  eprint = {1604.08772},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2GMZ4CRR/Gregor et al_2016_Towards Conceptual Compression.pdf},
  journal = {arXiv:1604.08772 [cs, stat]},
  keywords = {GAN,RNN},
  language = {en},
  primaryClass = {cs, stat}
}

@article{grill,
  title = {Blazing the Trails before Beating the Path: {{Sample}}-Efficient {{Monte}}-{{Carlo}} Planning},
  author = {Grill, Jean-Bastien and Valko, Michal and Munos, Remi},
  pages = {9},
  abstract = {You are a robot and you live in a Markov decision process (MDP) with a finite or an infinite number of transitions from state-action to next states. You got brains and so you plan before you act. Luckily, your roboparents equipped you with a generative model to do some Monte-Carlo planning. The world is waiting for you and you have no time to waste. You want your planning to be efficient. Sample-efficient. Indeed, you want to exploit the possible structure of the MDP by exploring only a subset of states reachable by following near-optimal policies. You want guarantees on sample complexity that depend on a measure of the quantity of near-optimal states. You want something, that is an extension of Monte-Carlo sampling (for estimating an expectation) to problems that alternate maximization (over actions) and expectation (over next states). But you do not want to StOP with exponential running time, you want something simple to implement and computationally efficient. You want it all and you want it now. You want TrailBlazer.},
  file = {/Users/x0r/Zotero/storage/SS565DEX/Grill et al_Blazing the trails before beating the path.pdf},
  keywords = {rl},
  language = {en}
}

@article{grossman2017,
  title = {Noninvasive {{Deep Brain Stimulation}} via {{Temporally Interfering Electric Fields}}},
  author = {Grossman, Nir and Bono, David and Dedic, Nina and Kodandaramaiah, Suhasa B. and Rudenko, Andrii and Suk, Ho-Jun and Cassara, Antonino M. and Neufeld, Esra and Kuster, Niels and Tsai, Li-Huei and {Pascual-Leone}, Alvaro and Boyden, Edward S.},
  year = {2017},
  month = jun,
  volume = {169},
  pages = {1029-1041.e16},
  issn = {00928674},
  doi = {10.1016/j.cell.2017.05.024},
  abstract = {We report a noninvasive strategy for electrically stimulating neurons at depth. By delivering to the brain multiple electric fields at frequencies too high to recruit neural firing, but which differ by a frequency within the dynamic range of neural firing, we can electrically stimulate neurons throughout a region where interference between the multiple fields results in a prominent electric field envelope modulated at the difference frequency. We validated this temporal interference (TI) concept via modeling and physics experiments, and verified that neurons in the living mouse brain could follow the electric field envelope. We demonstrate the utility of TI stimulation by stimulating neurons in the hippocampus of living mice without recruiting neurons of the overlying cortex. Finally, we show that by altering the currents delivered to a set of immobile electrodes, we can steerably evoke different motor patterns in living mice.},
  file = {/Users/x0r/Zotero/storage/T3X7UW69/Grossman et al_2017_Noninvasive Deep Brain Stimulation via Temporally Interfering Electric Fields.pdf},
  journal = {Cell},
  language = {en},
  number = {6}
}

@article{gruslys2016,
  title = {Memory-{{Efficient Backpropagation Through Time}}},
  author = {Gruslys, Audr{\=u}nas and Munos, Remi and Danihelka, Ivo and Lanctot, Marc and Graves, Alex},
  year = {2016},
  month = jun,
  abstract = {We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95\% of memory usage while using only one third more time per iteration than the standard BPTT.},
  archivePrefix = {arXiv},
  eprint = {1606.03401},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NYWC5HAF/Gruslys et al_2016_Memory-Efficient Backpropagation Through Time.pdf},
  journal = {arXiv:1606.03401 [cs]},
  keywords = {backprop,perf},
  language = {en},
  primaryClass = {cs}
}

@article{guez2019,
  title = {An Investigation of Model-Free Planning},
  author = {Guez, Arthur and Mirza, Mehdi and Gregor, Karol and Kabra, Rishabh and Racani{\`e}re, S{\'e}bastien and Weber, Th{\'e}ophane and Raposo, David and Santoro, Adam and Orseau, Laurent and Eccles, Tom and Wayne, Greg and Silver, David and Lillicrap, Timothy},
  year = {2019},
  month = jan,
  abstract = {The field of reinforcement learning (RL) is facing increasingly challenging domains with combinatorial complexity. For an RL agent to address these challenges, it is essential that it can plan effectively. Prior work has typically utilized an explicit model of the environment, combined with a specific planning algorithm (such as tree search). More recently, a new family of methods have been proposed that learn how to plan, by providing the structure for planning via an inductive bias in the function approximator (such as a tree structured neural network), trained end-to-end by a model-free RL algorithm. In this paper, we go even further, and demonstrate empirically that an entirely model-free approach, without special structure beyond standard neural network components such as convolutional networks and LSTMs, can learn to exhibit many of the characteristics typically associated with a model-based planner. We measure our agent's effectiveness at planning in terms of its ability to generalize across a combinatorial and irreversible state space, its data efficiency, and its ability to utilize additional thinking time. We find that our agent has many of the characteristics that one might expect to find in a planning algorithm. Furthermore, it exceeds the state-of-the-art in challenging combinatorial domains such as Sokoban and outperforms other model-free approaches that utilize strong inductive biases toward planning.},
  archivePrefix = {arXiv},
  eprint = {1901.03559},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/PXIA2Z3Y/Guez et al_2019_An investigation of model-free planning.pdf},
  journal = {arXiv:1901.03559 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{guizilini2019,
  title = {{{PackNet}}-{{SfM}}: {{3D Packing}} for {{Self}}-{{Supervised Monocular Depth Estimation}}},
  shorttitle = {{{PackNet}}-{{SfM}}},
  author = {Guizilini, Vitor and Ambrus, Rares and Pillai, Sudeep and Gaidon, Adrien},
  year = {2019},
  month = may,
  abstract = {Densely estimating the depth of a scene from a single image is an ill-posed inverse problem that is seeing exciting progress with self-supervision from strong geometric cues, in particular from training using stereo imagery. In this work, we investigate the more challenging structure-from-motion (SfM) setting, learning purely from monocular videos. We propose PackNet - a novel deep architecture that leverages new 3D packing and unpacking blocks to effectively capture fine details in monocular depth map predictions. Additionally, we propose a novel velocity supervision loss that allows our model to predict metrically accurate depths, thus alleviating the need for test-time ground-truth scaling. We show that our proposed scale-aware architecture achieves state-of-the-art results on the KITTI benchmark, significantly improving upon any approach trained on monocular video, and even achieves competitive performance to stereo-trained methods.},
  archivePrefix = {arXiv},
  eprint = {1905.02693},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/WDXJ8YQ9/Guizilini et al_2019_PackNet-SfM.pdf},
  journal = {arXiv:1905.02693 [cs]},
  keywords = {depth,dl,self-supervision},
  primaryClass = {cs}
}

@inproceedings{gupta2009,
  title = {Hebbian Learning with Winner Take All for Spiking Neural Networks},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Gupta, Ankur and Long, Lyle N.},
  year = {2009},
  month = jun,
  pages = {1054--1060},
  publisher = {{IEEE}},
  address = {{Atlanta, Ga, USA}},
  doi = {10.1109/IJCNN.2009.5178751},
  file = {/Users/x0r/Zotero/storage/AB7CLH23/Gupta_Long_2009_Hebbian learning with winner take all for spiking neural networks.pdf},
  isbn = {978-1-4244-3548-7},
  keywords = {neuromorphic,SNN},
  language = {en}
}

@article{gupta2016,
  title = {Real-Time Encoding and Compression of Neuronal Spikes by Metal-Oxide Memristors},
  author = {Gupta, Isha and Serb, Alexantrou and Khiat, Ali and Zeitler, Ralf and Vassanelli, Stefano and Prodromakis, Themistoklis},
  year = {2016},
  month = sep,
  volume = {7},
  pages = {12805},
  issn = {2041-1723},
  doi = {10.1038/ncomms12805},
  file = {/Users/x0r/Zotero/storage/V6YGSRFF/Gupta et al_2016_Real-time encoding and compression of neuronal spikes by metal-oxide memristors.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic,SNN},
  language = {en}
}

@inproceedings{guzdial2017,
  title = {Game {{Engine Learning}} from {{Video}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Guzdial, Matthew and Li, Boyang and Riedl, Mark O.},
  year = {2017},
  month = aug,
  pages = {3707--3713},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/518},
  abstract = {Intelligent agents need to be able to make predictions about their environment. In this work we present a novel approach to learn a forward simulation model via simple search over pixel input. We make use of a video game, Super Mario Bros., as an initial test of our approach as it represents a physics system that is significantly less complex than reality. We demonstrate the significant improvement of our approach in predicting future states compared with a baseline CNN and apply the learned model to train a game playing agent. Thus we evaluate the algorithm in terms of the accuracy and value of its output model.},
  file = {/Users/x0r/Zotero/storage/M56T6C9V/Guzdial et al_2017_Game Engine Learning from Video.pdf},
  isbn = {978-0-9992411-0-3},
  keywords = {dl},
  language = {en}
}

@article{ha2018,
  title = {World {{Models}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = mar,
  doi = {10.5281/zenodo.1207631},
  abstract = {We explore building generative neural network models of popular reinforcement learning environments. Our world model can be trained quickly in an unsupervised manner to learn a compressed spatial and temporal representation of the environment. By using features extracted from the world model as inputs to an agent, we can train a very compact and simple policy that can solve the required task. We can even train our agent entirely inside of its own hallucinated dream generated by its world model, and transfer this policy back into the actual environment. An interactive version of this paper is available at https://worldmodels.github.io/},
  archivePrefix = {arXiv},
  eprint = {1803.10122},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/UGT4VYUD/Ha_Schmidhuber_2018_World Models.pdf},
  journal = {arXiv:1803.10122 [cs, stat]},
  keywords = {rl,spinning-up},
  primaryClass = {cs, stat}
}

@article{ha2018a,
  title = {Recurrent {{World Models Facilitate Policy Evolution}}},
  author = {Ha, David and Schmidhuber, J{\"u}rgen},
  year = {2018},
  month = sep,
  abstract = {A generative recurrent neural network is quickly trained in an unsupervised manner to model popular reinforcement learning environments through compressed spatio-temporal representations. The world model's extracted features are fed into compact and simple policies trained by evolution, achieving state of the art results in various environments. We also train our agent entirely inside of an environment generated by its own internal world model, and transfer this policy back into the actual environment. Interactive version of paper at https://worldmodels.github.io},
  archivePrefix = {arXiv},
  eprint = {1809.01999},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/WXZ3HGUH/Ha_Schmidhuber_2018_Recurrent World Models Facilitate Policy Evolution.pdf},
  journal = {arXiv:1809.01999 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{haarnoja2018,
  title = {Soft {{Actor}}-{{Critic}}: {{Off}}-{{Policy Maximum Entropy Deep Reinforcement Learning}} with a {{Stochastic Actor}}},
  shorttitle = {Soft {{Actor}}-{{Critic}}},
  author = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  year = {2018},
  month = jan,
  abstract = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  archivePrefix = {arXiv},
  eprint = {1801.01290},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/8D4I26LX/Haarnoja et al_2018_Soft Actor-Critic.pdf},
  journal = {arXiv:1801.01290 [cs, stat]},
  keywords = {rl,spinning-up},
  primaryClass = {cs, stat}
}

@article{hafner2018,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2018},
  month = nov,
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from pixels and chooses actions through online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this problem using a latent dynamics model with both deterministic and stochastic transition function and a generalized variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards. PlaNet uses significantly fewer episodes and reaches final performance close to and sometimes higher than top model-free algorithms.},
  archivePrefix = {arXiv},
  eprint = {1811.04551},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9KN6GVHW/Hafner et al_2018_Learning Latent Dynamics for Planning from Pixels.pdf},
  journal = {arXiv:1811.04551 [cs, stat]},
  keywords = {AGI,rl},
  primaryClass = {cs, stat}
}

@article{hafner2019,
  title = {Learning {{Latent Dynamics}} for {{Planning}} from {{Pixels}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  year = {2019},
  month = jun,
  abstract = {Planning has been very successful for control tasks with known environment dynamics. To leverage planning in unknown environments, the agent needs to learn the dynamics from interactions with the world. However, learning dynamics models that are accurate enough for planning has been a long-standing challenge, especially in image-based domains. We propose the Deep Planning Network (PlaNet), a purely model-based agent that learns the environment dynamics from images and chooses actions through fast online planning in latent space. To achieve high performance, the dynamics model must accurately predict the rewards ahead for multiple time steps. We approach this using a latent dynamics model with both deterministic and stochastic transition components. Moreover, we propose a multi-step variational inference objective that we name latent overshooting. Using only pixel observations, our agent solves continuous control tasks with contact dynamics, partial observability, and sparse rewards, which exceed the difficulty of tasks that were previously solved by planning with learned models. PlaNet uses substantially fewer episodes and reaches final performance close to and sometimes higher than strong model-free algorithms.},
  archivePrefix = {arXiv},
  eprint = {1811.04551},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/D4IT3BYS/Hafner et al. - 2019 - Learning Latent Dynamics for Planning from Pixels.pdf},
  journal = {arXiv:1811.04551 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hafner2020,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  year = {2020},
  month = mar,
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  archivePrefix = {arXiv},
  eprint = {1912.01603},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/T8N3HJKE/Hafner et al. - 2020 - Dream to Control Learning Behaviors by Latent Ima.pdf},
  journal = {arXiv:1912.01603 [cs]},
  keywords = {To read},
  language = {en},
  primaryClass = {cs}
}

@article{hajian2018,
  title = {Tailoring Far-Infrared Surface Plasmon Polaritons of a Single-Layer Graphene Using Plasmon-Phonon Hybridization in Graphene-{{LiF}} Heterostructures},
  author = {Hajian, Hodjat and Serebryannikov, Andriy E. and Ghobadi, Amir and Demirag, Yigit and Butun, Bayram and Vandenbosch, Guy A. E. and Ozbay, Ekmel},
  year = {2018},
  month = sep,
  volume = {8},
  pages = {13209},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-31049-6},
  abstract = {Being one-atom thick and tunable simultaneously, graphene plays the revolutionizing role in many areas. The focus of this paper is to investigate the modal characteristics of surface waves in structures with graphene in the far-infrared (far-IR) region. We discuss the effects exerted by substrate permittivity on propagation and localization characteristics of surface-plasmon-polaritons (SPPs) in single-layer graphene and theoretically investigate characteristics of the hybridized surface-phonon-plasmon-polaritons (SPPPs) in graphene/LiF/glass heterostructures. First, it is shown how high permittivity of substrate may improve characteristics of graphene SPPs. Next, the possibility of optimization for surface-phonon-polaritons (SPhPs) in waveguides based on LiF, a polar dielectric with a wide polaritonic gap (Reststrahlen band) and a wide range of permittivity variation, is demonstrated. Combining graphene and LiF in one heterostructure allows to keep the advantages of both, yielding tunable hybridized SPPPs which can be either forwardly or backwardly propagating. Owing to high permittivity of LiF below the gap, an almost 3.2-fold enhancement in the figure of merit (FoM), ratio of normalized propagation length to localization length of the modes, can be obtained for SPPPs at 5\textendash 9\,THz, as compared with SPPs of graphene on conventional glass substrate. The enhancement is efficiently tunable by varying the chemical potential of graphene. SPPPs with characteristics which strongly differ inside and around the polaritonic gap are found.},
  copyright = {2018 The Author(s)},
  file = {/Users/x0r/Zotero/storage/S5AURHTB/Hajian et al_2018_Tailoring far-infrared surface plasmon polaritons of a single-layer graphene.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{hakhamaneshi2019,
  title = {{{BagNet}}: {{Berkeley Analog Generator}} with {{Layout Optimizer Boosted}} with {{Deep Neural Networks}}},
  shorttitle = {{{BagNet}}},
  author = {Hakhamaneshi, Kourosh and Werblun, Nick and Abbeel, Pieter and Stojanovic, Vladimir},
  year = {2019},
  month = jul,
  abstract = {The discrepancy between post-layout and schematic simulation results continues to widen in analog design due in part to the domination of layout parasitics. This paradigm shift is forcing designers to adopt design methodologies that seamlessly integrate layout effects into the standard design flow. Hence, any simulation-based optimization framework should take into account time-consuming post-layout simulation results. This work presents a learning framework that learns to reduce the number of simulations of evolutionary-based combinatorial optimizers, using a DNN that discriminates against generated samples, before running simulations. Using this approach, the discriminator achieves at least two orders of magnitude improvement on sample efficiency for several large circuit examples including an optical link receiver layout.},
  archivePrefix = {arXiv},
  eprint = {1907.10515},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2TEWLA23/Hakhamaneshi et al. - 2019 - BagNet Berkeley Analog Generator with Layout Opti.pdf;/Users/x0r/Zotero/storage/HV6VA2A5/1907.html},
  journal = {arXiv:1907.10515 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{hall2014,
  title = {The {{Kardashian}} Index: A Measure of Discrepant Social Media Profile for Scientists},
  shorttitle = {The {{Kardashian}} Index},
  author = {Hall, Neil},
  year = {2014},
  month = jul,
  volume = {15},
  issn = {1474-760X},
  doi = {10.1186/s13059-014-0424-0},
  abstract = {In the era of social media there are now many different ways that a scientist can build their public profile; the publication of high-quality scientific papers being just one. While social media is a valuable tool for outreach and the sharing of ideas, there is a danger that this form of communication is gaining too high a value and that we are losing sight of key metrics of scientific value, such as citation indices. To help quantify this, I propose the `Kardashian Index', a measure of discrepancy between a scientist's social media profile and publication record based on the direct comparison of numbers of citations and Twitter followers.},
  file = {/Users/x0r/Zotero/storage/CUPFCMCS/Hall_2014_The Kardashian index.pdf;/Users/x0r/Zotero/storage/SVSA7ERL/Hall_2014_The Kardashian index.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {7}
}

@article{hamilton2010,
  title = {Dopamine Modulates Synaptic Plasticity in Dendrites of Rat and Human Dentate Granule Cells},
  author = {Hamilton, Trevor J. and Wheatley, B. Matthew and Sinclair, D. Barry and Bachmann, Madeline and Larkum, Matthew E. and Colmers, William F.},
  year = {2010},
  month = oct,
  volume = {107},
  pages = {18185--18190},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1011558107},
  abstract = {The mechanisms underlying memory formation in the hippocampal network remain a major unanswered aspect of neuroscience. Although high-frequency activity appears essential for plasticity, salience for memory formation is also provided by activity in ventral tegmental area (VTA) dopamine projections. Here, we report that activation of dopamine D1 receptors in dentate granule cells (DGCs) can preferentially increase dendritic excitability to both high-frequency afferent activity and high-frequency trains of backpropagating action potentials. Using whole-cell patch clamp recordings, calcium imaging, and neuropeptide Y to inhibit postsynaptic calcium influx, we found that activation of dendritic voltage-dependent calcium channels (VDCCs) is essential for dopamine-induced long-term potentiation (LTP), both in rat and human dentate gyrus (DG). Moreover, we demonstrate previously unreported spike-timing\textendash dependent plasticity in the human hippocampus. These results suggest that when dopamine is released in the dentate gyrus with concurrent high-frequency activity there is an increased probability that synapses will be strengthened and reward-associated spatial memories will be formed.},
  chapter = {Biological Sciences},
  file = {/Users/x0r/switchdrive/zotero/Hamilton et al_2010_Dopamine modulates synaptic plasticity in dendrites of rat and human dentate.pdf;/Users/x0r/Zotero/storage/7R6MTVHG/18185.html},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {42},
  pmid = {20921404}
}

@article{hancock2019,
  title = {Learning from {{Dialogue}} after {{Deployment}}: {{Feed Yourself}}, {{Chatbot}}!},
  shorttitle = {Learning from {{Dialogue}} after {{Deployment}}},
  author = {Hancock, Braden and Bordes, Antoine and Mazare, Pierre-Emmanuel and Weston, Jason},
  year = {2019},
  month = jan,
  abstract = {The majority of conversations a dialogue agent sees over its lifetime occur after it has already been trained and deployed, leaving a vast store of potential training signal untapped. In this work, we propose the self-feeding chatbot, a dialogue agent with the ability to extract new training examples from the conversations it participates in. As our agent engages in conversation, it also estimates user satisfaction in its responses. When the conversation appears to be going well, the user's responses become new training examples to imitate. When the agent believes it has made a mistake, it asks for feedback; learning to predict the feedback that will be given improves the chatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with over 131k training examples, we find that learning from dialogue with a self-feeding chatbot significantly improves performance, regardless of the amount of traditional supervision.},
  archivePrefix = {arXiv},
  eprint = {1901.05415},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/C7A48UMW/Hancock et al_2019_Learning from Dialogue after Deployment.pdf},
  journal = {arXiv:1901.05415 [cs, stat]},
  keywords = {dl,nlp},
  primaryClass = {cs, stat}
}

@article{hansen2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  shorttitle = {The {{CMA Evolution Strategy}}},
  author = {Hansen, Nikolaus},
  year = {2016},
  month = apr,
  abstract = {This tutorial introduces the CMA Evolution Strategy (ES), where CMA stands for Covariance Matrix Adaptation. The CMA-ES is a stochastic, or randomized, method for real-parameter (continuous domain) optimization of non-linear, non-convex functions. We try to motivate and derive the algorithm from intuitive concepts and from requirements of non-linear, non-convex search in continuous domain.},
  archivePrefix = {arXiv},
  eprint = {1604.00772},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GH2WBNR2/Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf;/Users/x0r/Zotero/storage/RVJNQED5/1604.html},
  journal = {arXiv:1604.00772 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hardtdegen2018,
  title = {Improved {{Switching Stability}} and the {{Effect}} of an {{Internal Series Resistor}} in {{HfO2}}/{{TiO2 Bilayer ReRAM Cells}}},
  author = {Hardtdegen, Alexander and La Torre, Camilla and Cuppers, Felix and Menzel, Stephan and Waser, Rainer and {Hoffmann-Eifert}, Susanne},
  year = {2018},
  month = aug,
  volume = {65},
  pages = {3229--3236},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2018.2849872},
  abstract = {Bipolar redox-based resistive random-access memory cells are intensively studied for new storage class memory and beyond von Neumann computing applications. However, the considerable variability of the resistance values in ON and OFF state as well as of the SET voltage remains challenging. In this paper, we discuss the physical origin of the significant reduction in the switching variability of HfO2-based devices achieved by the insertion of a thin TiOx layer between the HfO2 layer and the oxygen exchange metal layer. Typically, HfO2 single layer cells exhibit an abrupt SET process, which is difficult to control. In contrast, selfcompliance effects in the HfO2/TiOx bilayer devices lead to an increased stability of SET voltages and OFF-state resistances. The SET process is gradual and the RESET becomes abrupt for higher switching currents. Comparison of the experimental data with simulation results achieved from a physics-based compact model for the full description of the switching behavior of the single layer and bilayer devices clearly reveal three major effects. The TiOx layer affects the temperature distribution during switching (by modifying the heat dissipation), forms an additional series resistance and changes the current conduction mechanism in the OFF state of the bilayer device compared to the single layer device.},
  file = {/Users/x0r/Zotero/storage/EUU427Q5/Hardtdegen et al_2018_Improved Switching Stability and the Effect of an Internal Series Resistor in.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {ReRAM},
  language = {en},
  number = {8}
}

@article{harris,
  title = {Digital {{Design}} and {{Computer Architecture}}},
  author = {Harris, Sarah and Harris, David},
  pages = {721},
  file = {/Users/x0r/Zotero/storage/5WW3BY5N/Harris_Harris_Digital Design and Computer Architecture.pdf},
  keywords = {digital-design},
  language = {en}
}

@article{Hassabis_etal17,
  ids = {hassabis2017},
  title = {Neuroscience-Inspired Artificial Intelligence},
  author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
  year = {2017},
  volume = {95},
  pages = {245--258},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2017.06.011},
  abstract = {The fields of neuroscience and artificial intelligence (AI) have a long and intertwined history. In more recent times, however, communication and collaboration between the two fields has become less commonplace. In this article, we argue that better understanding biological brains could play a vital role in building intelligent machines. We survey historical interactions between the AI and neuroscience fields and emphasize current advances in AI that have been inspired by the study of neural computation in humans and other animals. We conclude by highlighting shared themes that may be key for advancing future research in both fields.},
  file = {/Users/x0r/Zotero/storage/IN7VPGQC/Hassabis et al_2017_Neuroscience-Inspired Artificial Intelligence.pdf},
  journal = {Neuron},
  keywords = {AGI,artificial intelligence,brain,cognition,learning,neural network,neuroscience},
  number = {2}
}

@article{hasson,
  title = {Direct-Fit to Nature: An Evolutionary Perspective on Biological (and Artificial) Neural Networks},
  author = {Hasson, Uri and Nastase, Samuel A and Goldstein, Ariel},
  pages = {32},
  abstract = {Evolution is a blind fitting process by which organisms, over generations, adapt to the niches of an ever-changing environment. Does the mammalian brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in training deep neural networks has exposed the power of optimizing millions of synaptic weights to map millions of observations along ecologically relevant objective functions. This class of models has dramatically outstripped simpler, more intuitive models, operating robustly in real-life contexts spanning perception, language, and action coordination. These models do not learn an explicit, human-interpretable representation of the underlying structure of the data; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Furthermore, counterintuitively, over-parameterized models, similarly to evolutionary processes, can be simple and parsimonious as they provide a versatile, robust solution for learning a diverse set of functions. In contrast to traditional scientific models, where the ultimate goal is interpretability, over-parameterized models eschew interpretability in favor of solving real-life problems or tasks. We contend that over-parameterized blind fitting presents a radical challenge to many of the underlying assumptions and practices in computational neuroscience and cognitive psychology. At the same time, this shift in perspective informs longstanding debates and establishes unexpected links with evolution, ecological psychology, and artificial life.},
  file = {/Users/x0r/Zotero/storage/3TCT5E4W/Hasson et al. - Direct-fit to nature an evolutionary perspective .pdf},
  keywords = {To read},
  language = {en}
}

@article{haviv2019,
  title = {Understanding and {{Controlling Memory}} in {{Recurrent Neural Networks}}},
  author = {Haviv, Doron and Rivkind, Alexander and Barak, Omri},
  year = {2019},
  month = feb,
  abstract = {To be effective in sequential data processing, Recurrent Neural Networks (RNNs) are required to keep track of past events by creating memories. While the relation between memories and the network's hidden state dynamics was established over the last decade, previous works in this direction were of a predominantly descriptive nature focusing mainly on locating the dynamical objects of interest. In particular, it remained unclear how dynamical observables affect the performance, how they form and whether they can be manipulated. Here, we utilize different training protocols, datasets and architectures to obtain a range of networks solving a delayed classification task with similar performance, alongside substantial differences in their ability to extrapolate for longer delays. We analyze the dynamics of the network's hidden state, and uncover the reasons for this difference. Each memory is found to be associated with a nearly steady state of the dynamics which we refer to as a 'slow point'. Slow point speeds predict extrapolation performance across all datasets, protocols and architectures tested. Furthermore, by tracking the formation of the slow points we are able to understand the origin of differences between training protocols. Finally, we propose a novel regularization technique that is based on the relation between hidden state speeds and memory longevity. Our technique manipulates these speeds, thereby leading to a dramatic improvement in memory robustness over time, and could pave the way for a new class of regularization methods.},
  archivePrefix = {arXiv},
  eprint = {1902.07275},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/67SU5HER/Haviv et al_2019_Understanding and Controlling Memory in Recurrent Neural Networks.pdf},
  journal = {arXiv:1902.07275 [cs, stat]},
  keywords = {rl},
  primaryClass = {cs, stat}
}

@article{he2018,
  title = {Streaming {{End}}-to-End {{Speech Recognition For Mobile Devices}}},
  author = {He, Yanzhang and Sainath, Tara N. and Prabhavalkar, Rohit and McGraw, Ian and Alvarez, Raziel and Zhao, Ding and Rybach, David and Kannan, Anjuli and Wu, Yonghui and Pang, Ruoming and Liang, Qiao and Bhatia, Deepti and Shangguan, Yuan and Li, Bo and Pundak, Golan and Sim, Khe Chai and Bagby, Tom and Chang, Shuo-yiin and Rao, Kanishka and Gruenstein, Alexander},
  year = {2018},
  month = nov,
  abstract = {End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recognizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.},
  archivePrefix = {arXiv},
  eprint = {1811.06621},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Z4U7LPJR/He et al_2018_Streaming End-to-end Speech Recognition For Mobile Devices.pdf},
  journal = {arXiv:1811.06621 [cs]},
  keywords = {dl},
  primaryClass = {cs}
}

@article{hegedus2008,
  title = {Microscopic Origin of the Fast Crystallization Ability of {{Ge}}\textendash{{Sb}}\textendash{{Te}} Phase-Change Memory Materials},
  author = {Heged{\"u}s, J. and Elliott, S. R.},
  year = {2008},
  month = may,
  volume = {7},
  pages = {399--405},
  issn = {1476-1122, 1476-4660},
  doi = {10.1038/nmat2157},
  file = {/Users/x0r/Zotero/storage/KVB3FTXL/Hegedüs_Elliott_2008_Microscopic origin of the fast crystallization ability of Ge–Sb–Te phase-change.pdf},
  journal = {Nature Materials},
  keywords = {GST,PCM},
  language = {en},
  number = {5}
}

@article{hein2020,
  title = {An {{Algorithmic Approach}} to {{Natural Behavior}}},
  author = {Hein, Andrew M. and Altshuler, Douglas L. and Cade, David E. and Liao, James C. and Martin, Benjamin T. and Taylor, Graham K.},
  year = {2020},
  month = jun,
  volume = {30},
  pages = {R663-R675},
  issn = {09609822},
  doi = {10.1016/j.cub.2020.04.018},
  file = {/Users/x0r/Zotero/storage/VNDCV9Z4/Hein et al. - 2020 - An Algorithmic Approach to Natural Behavior.pdf},
  journal = {Current Biology},
  language = {en},
  number = {11}
}

@article{henaff2019,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {H{\'e}naff, Olivier J. and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and van den Oord, Aaron},
  year = {2019},
  month = may,
  abstract = {Large scale deep learning excels when labeled images are abundant, yet data-efficient learning remains a longstanding challenge. While biological vision is thought to leverage vast amounts of unlabeled data to solve classification problems with limited supervision, computer vision has so far not succeeded in this `semi-supervised' regime. Our work tackles this challenge with Contrastive Predictive Coding, an unsupervised objective which extracts stable structure from still images. The result is a representation which, equipped with a simple linear classifier, separates ImageNet categories better than all competing methods, and surpasses the performance of a fully-supervised AlexNet model. When given a small number of labeled images (as few as 13 per class), this representation retains a strong classification performance, outperforming state-of-the-art semi-supervised methods by 10\% Top-5 accuracy and supervised methods by 20\%. Finally, we find our unsupervised representation to serve as a useful substrate for image detection on the PASCAL-VOC 2007 dataset, approaching the performance of representations trained with a fully annotated ImageNet dataset. We expect these results to open the door to pipelines that use scalable unsupervised representations as a drop-in replacement for supervised ones for real-world vision tasks where labels are scarce.},
  archivePrefix = {arXiv},
  eprint = {1905.09272},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/B7Q86XIB/Hénaff et al_2019_Data-Efficient Image Recognition with Contrastive Predictive Coding.pdf},
  journal = {arXiv:1905.09272 [cs]},
  keywords = {dl,neuromorphic},
  primaryClass = {cs}
}

@article{hermann2015,
  title = {Teaching {{Machines}} to {{Read}} and {{Comprehend}}},
  author = {Hermann, Karl Moritz and Ko{\v c}isk{\'y}, Tom{\'a}{\v s} and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  year = {2015},
  month = jun,
  abstract = {Teaching machines to read natural language documents remains an elusive challenge. Machine reading systems can be tested on their ability to answer questions posed on the contents of documents that they have seen, but until now large scale training and test datasets have been missing for this type of evaluation. In this work we define a new methodology that resolves this bottleneck and provides large scale supervised reading comprehension data. This allows us to develop a class of attention based deep neural networks that learn to read real documents and answer complex questions with minimal prior knowledge of language structure.},
  archivePrefix = {arXiv},
  eprint = {1506.03340},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VKB2HQDU/Hermann et al_2015_Teaching Machines to Read and Comprehend.pdf},
  journal = {arXiv:1506.03340 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{herzog2012,
  title = {Perceptual Learning, Roving and the Unsupervised Bias},
  author = {Herzog, Michael H. and Aberg, Kristoffer C. and Fr{\'e}maux, Nicolas and Gerstner, Wulfram and Sprekeler, Henning},
  year = {2012},
  month = may,
  volume = {61},
  pages = {95--99},
  issn = {00426989},
  doi = {10.1016/j.visres.2011.11.001},
  abstract = {Perceptual learning improves perception through training. Perceptual learning improves with most stimulus types but fails when certain stimulus types are mixed during training (roving). This result is surprising because classical supervised and unsupervised neural network models can cope easily with roving conditions. What makes humans so inferior compared to these models? As experimental and conceptual work has shown, human perceptual learning is neither supervised nor unsupervised but reward-based learning. Reward-based learning suffers from the so-called unsupervised bias, i.e., to prevent synaptic ``drift'', the average reward has to be exactly estimated. However, this is impossible when two or more stimulus types with different rewards are presented during training (and the reward is estimated by a running average). For this reason, we propose no learning occurs in roving conditions. However, roving hinders perceptual learning only for combinations of similar stimulus types but not for dissimilar ones. In this latter case, we propose that a critic can estimate the reward for each stimulus type separately. One implication of our analysis is that the critic cannot be located in the visual system.},
  file = {/Users/x0r/Zotero/storage/7NCBS3WD/Herzog et al_2012_Perceptual learning, roving and the unsupervised bias.pdf},
  journal = {Vision Research},
  keywords = {neuroscience,vision},
  language = {en}
}

@article{hestness2017,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1712.00409},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/I3JEV2LG/Hestness et al_2017_Deep Learning Scaling is Predictable, Empirically.pdf},
  journal = {arXiv:1712.00409 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hill1971,
  title = {Hopping Conduction in Amorphous Solids},
  author = {Hill, Robert M.},
  year = {1971},
  month = dec,
  volume = {24},
  pages = {1307--1325},
  issn = {0031-8086},
  doi = {10.1080/14786437108217414},
  file = {/Users/x0r/Zotero/storage/6X2S44A7/Hill_1971_Hopping conduction in amorphous solids.pdf},
  journal = {Philosophical Magazine},
  keywords = {material},
  language = {en},
  number = {192}
}

@article{hill2017,
  title = {An {{Evolutionary Theory}} for the {{Variability Hypothesis}}},
  author = {Hill, Theodore P.},
  year = {2017},
  month = mar,
  abstract = {An elementary mathematical theory based on ``selectivity'' is proposed to address a question raised by Charles Darwin, namely, how one gender of a sexually dimorphic species might tend to evolve with greater variability than the other gender. Briefly, the theory says that if one sex is relatively selective then from one generation to the next, more variable subpopulations of the opposite sex will tend to prevail over those with lesser variability; and conversely, if a sex is relatively non-selective, then less variable subpopulations of the opposite sex will tend to prevail over those with greater variability. This theory makes no assumptions about differences in means between the sexes, nor does it presume that one sex is selective and the other non-selective. Two mathematical models are presented: a discrete-time one-step statistical model using normally distributed fitness values; and a continuous-time deterministic model using exponentially distributed fitness levels.},
  archivePrefix = {arXiv},
  eprint = {1703.04184},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/CCTTMUG2/Hill_2017_An Evolutionary Theory for the Variability Hypothesis.pdf},
  journal = {arXiv:1703.04184 [q-bio]},
  language = {en},
  primaryClass = {q-bio}
}

@misc{hinton,
  title = {Aetherial {{Symbols}}},
  author = {Hinton, Geoffrey},
  file = {/Users/x0r/Zotero/storage/GHKVW5ZD/Hinton_Aetherial Symbols.pdf},
  keywords = {AGI}
}

@article{hinton2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}: {{The Shared Views}} of {{Four Research Groups}}},
  shorttitle = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  month = nov,
  volume = {29},
  pages = {82--97},
  issn = {1053-5888},
  doi = {10.1109/MSP.2012.2205597},
  file = {/Users/x0r/Zotero/storage/XG9XQDFV/Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Spee.pdf},
  journal = {IEEE Signal Processing Magazine},
  keywords = {dl},
  language = {en},
  number = {6}
}

@inproceedings{hochreiter2001,
  title = {Learning to {{Learn Using Gradient Descent}}},
  booktitle = {Artificial {{Neural Networks}} \textemdash{} {{ICANN}} 2001},
  author = {Hochreiter, Sepp and Younger, A. Steven and Conwell, Peter R.},
  year = {2001},
  month = aug,
  pages = {87--94},
  publisher = {{Springer, Berlin, Heidelberg}},
  doi = {10.1007/3-540-44668-0_13},
  abstract = {This paper introduces the application of gradient descent methods to meta-learning. The concept of ``meta-learning'', i.e. of a system that improves or discovers a learning algorithm, has been of interest in machine learning for decades because of its appealing applications. Previous meta-learning approaches have been based on evolutionary methods and, therefore, have been restricted to small models with few free parameters. We make meta-learning in large systems feasible by using recurrent neural networks with their attendant learning routines as meta-learning systems. Our system derived complex well performing learning algorithms from scratch. In this paper we also show that our approach performs non-stationary time series prediction.},
  file = {/Users/x0r/Zotero/storage/M6EBQ55U/Hochreiter et al_2001_Learning to Learn Using Gradient Descent.pdf},
  isbn = {978-3-540-42486-4 978-3-540-44668-2},
  keywords = {meta-learning},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{holm2019,
  title = {In Defense of the Black Box},
  author = {Holm, Elizabeth A.},
  year = {2019},
  month = apr,
  volume = {364},
  pages = {26--27},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aax0162},
  file = {/Users/x0r/Zotero/storage/E2YXBQW8/Holm_2019_In defense of the black box.pdf},
  journal = {Science},
  keywords = {dl},
  language = {en},
  number = {6435}
}

@inproceedings{horowitz2014,
  title = {1.1 {{Computing}}'s Energy Problem (and What We Can Do about It)},
  booktitle = {2014 {{IEEE International Solid}}-{{State Circuits Conference Digest}} of {{Technical Papers}} ({{ISSCC}})},
  author = {Horowitz, Mark},
  year = {2014},
  month = feb,
  pages = {10--14},
  issn = {2376-8606},
  doi = {10.1109/ISSCC.2014.6757323},
  abstract = {Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices.},
  file = {/Users/x0r/Zotero/storage/WBVA9ZRB/Horowitz - 2014 - 1.1 Computing's energy problem (and what we can do.pdf;/Users/x0r/Zotero/storage/JV3IRE5R/6757323.html}
}

@book{horowitz2015,
  title = {The Art of Electronics},
  author = {Horowitz, Paul},
  year = {2015},
  edition = {Third edition},
  publisher = {{Cambridge University Press}},
  address = {{New York, NY}},
  file = {/Users/x0r/Zotero/storage/IXZ8GDCG/Horowitz_2015_The art of electronics.pdf},
  isbn = {978-0-521-80926-9},
  lccn = {TK7815 .H67 2015}
}

@article{houthooft2016,
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  shorttitle = {{{VIME}}},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  year = {2016},
  month = may,
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  archivePrefix = {arXiv},
  eprint = {1605.09674},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/TU64UEB7/Houthooft et al_2016_VIME.pdf},
  journal = {arXiv:1605.09674 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hu2013,
  title = {Investigation of the Phase Transition of {{Ge2Sb2Te5}} Films Using Internal Friction Method},
  author = {Hu, D.Z. and Fan, D.H. and Pan, F.M. and Fan, J.Y.},
  year = {2013},
  month = oct,
  volume = {378},
  pages = {139--143},
  issn = {00223093},
  doi = {10.1016/j.jnoncrysol.2013.06.028},
  abstract = {The phase transition of Ge2Sb2Te5 has been studied by internal friction measurement. Two Q-1 peaks were observed during the process of heating amorphous Ge2Sb2Te5 from room up to 400 \textdegree C. With a heating rate of 5.5 \textdegree C/min, the first peak (P1) is located at 180 \textdegree C and the second peak is (P2) located at 330 \textdegree C. Only the peak P1 appears when sample has been beforehand annealed at 230 \textdegree C for 1 hour. Both the peaks P1 and P2 don't appear when sample has been beforehand annealed at 380 \textdegree C for 1 hour. The positions of P1 and P2 are both affected by the heating rate, and they follow Kissinger relation. The activation energy corresponding to the peak P1 is 1.94 {$\pm$} 0.18 eV, and the activation energy corresponding to the peak P2 is 3.49 {$\pm$} 0.24 eV. The isothermal annealing experiment revealed that Q-1 monotonously decreases with time t in the process of phase change, and the internal friction Q-1 versus time t can be represented as Q-1(t) = Q-1({$\infty$}) - [Q-1({$\infty$}) - Q-1(0)] exp (-t/{$\tau$}). The characteristic time {$\tau$} is affected by temperature, and ln{$\tau$} increase linearly with T-1.},
  file = {/Users/x0r/Zotero/storage/BH8CGEY7/Hu et al_2013_Investigation of the phase transition of Ge2Sb2Te5 films using internal.pdf},
  journal = {Journal of Non-Crystalline Solids},
  keywords = {PCM},
  language = {en}
}

@inproceedings{hu2016,
  title = {Dot-Product Engine for Neuromorphic Computing: Programming {{1T1M}} Crossbar to Accelerate Matrix-Vector Multiplication},
  shorttitle = {Dot-Product Engine for Neuromorphic Computing},
  author = {Hu, Miao and Williams, R. Stanley and Strachan, John Paul and Li, Zhiyong and Grafals, Emmanuelle M. and Davila, Noraica and Graves, Catherine and Lam, Sity and Ge, Ning and Yang, Jianhua Joshua},
  year = {2016},
  pages = {1--6},
  publisher = {{ACM Press}},
  doi = {10.1145/2897937.2898010},
  abstract = {Vector-matrix multiplication dominates the computation time and energy for many workloads, particularly neural network algorithms and linear transforms (e.g, the Discrete Fourier Transform). Utilizing the natural current accumulation feature of memristor crossbar, we developed the Dot-Product Engine (DPE) as a high density, high power efficiency accelerator for approximate matrix-vector multiplication. We firstly invented a conversion algorithm to map arbitrary matrix values appropriately to memristor conductances in a realistic crossbar array, accounting for device physics and circuit issues to reduce computational errors. The accurate device resistance programming in large arrays is enabled by close-loop pulse tuning and access transistors. To validate our approach, we simulated and benchmarked one of the state-of-the-art neural networks for pattern recognition on the DPEs. The result shows no accuracy degradation compared to software approach ( 99 \% pattern recognition accuracy for MNIST data set) with only 4 Bit DAC/ADC requirement, while the DPE can achieve a speed-efficiency product of 1,000\texttimes{} to 10,000\texttimes{} compared to a custom digital ASIC.},
  file = {/Users/x0r/Zotero/storage/CHVWNHMC/Hu et al_2016_Dot-product engine for neuromorphic computing.pdf},
  isbn = {978-1-4503-4236-0},
  keywords = {neuromorphic,To read},
  language = {en}
}

@inproceedings{huang2011,
  title = {One Selector-One Resistor ({{1S1R}}) Crossbar Array for High-Density Flexible Memory Applications},
  author = {Huang, Jiun-Jia and {Yi-Ming Tseng} and {Wun-Cheng Luo} and {Chung-Wei Hsu} and Hou, Tuo-Hung},
  year = {2011},
  month = dec,
  pages = {31.7.1-31.7.4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2011.6131653},
  abstract = {Lack of a suitable selection device to suppress sneak current has impeded the development of 4F2 crossbar memory array utilizing stable and scalable bipolar resistive-switching. We report a high-performance nonlinear bipolar selector realized by a simple Ni/TiO2/Ni MIM structure with a high current density of 105 A/cm2, and a Ni/TiO2/Ni/HfO2/Pt vertically stacked 1S1R cell capable of gigabit memory implementation. Furthermore, the demonstration of 1S1R array fabricated completely at room temperature on a plastic substrate highlights the promise of future extremely low-cost flexible nonvolatile memory.},
  file = {/Users/x0r/Zotero/storage/GB94I82A/Huang et al_2011_One selector-one resistor (1S1R) crossbar array for high-density flexible.pdf},
  isbn = {978-1-4577-0505-2 978-1-4577-0506-9 978-1-4577-0504-5},
  keywords = {neuromorphic},
  language = {en}
}

@article{huang2018,
  title = {Neural {{Task Graphs}}: {{Generalizing}} to {{Unseen Tasks}} from a {{Single Video Demonstration}}},
  shorttitle = {Neural {{Task Graphs}}},
  author = {Huang, De-An and Nair, Suraj and Xu, Danfei and Zhu, Yuke and Garg, Animesh and {Fei-Fei}, Li and Savarese, Silvio and Niebles, Juan Carlos},
  year = {2018},
  month = jul,
  abstract = {Our goal is to generate a policy to complete an unseen task given just a single video demonstration of the task in a given domain. We hypothesize that to successfully generalize to unseen complex tasks from a single video demonstration, it is necessary to explicitly incorporate the compositional structure of the tasks into the model. To this end, we propose Neural Task Graph (NTG) Networks, which use conjugate task graph as the intermediate representation to modularize both the video demonstration and the derived policy. We empirically show NTG achieves inter-task generalization on two complex tasks: Block Stacking in BulletPhysics and Object Collection in AI2-THOR. NTG improves data efficiency with visual input as well as achieve strong generalization without the need for dense hierarchical supervision. We further show that similar performance trends hold when applied to real-world data. We show that NTG can effectively predict task structure on the JIGSAWS surgical dataset and generalize to unseen tasks.},
  archivePrefix = {arXiv},
  eprint = {1807.03480},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NHKJ3RFP/Huang et al_2018_Neural Task Graphs.pdf},
  journal = {arXiv:1807.03480 [cs]},
  primaryClass = {cs}
}

@article{huh2017,
  title = {Gradient {{Descent}} for {{Spiking Neural Networks}}},
  author = {Huh, Dongsung and Sejnowski, Terrence J.},
  year = {2017},
  month = jun,
  abstract = {Much of studies on neural computation are based on network models of static neurons that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking networks and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast ({$\approx$} millisecond) spike-based interactions for efficient encoding of information, and a delayed memory XOR task over extended duration ({$\approx$} second). The results show that our method indeed optimizes the spiking network dynamics on the time scale of individual spikes as well as behavioral time scales. In conclusion, our result offers a general purpose supervised learning algorithm for spiking neural networks, thus advancing further investigations on spike-based computation.},
  archivePrefix = {arXiv},
  eprint = {1706.04698},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AZ3KFG65/Huh_Sejnowski_2017_Gradient Descent for Spiking Neural Networks.pdf;/Users/x0r/Zotero/storage/FIFV3VTZ/Huh_Sejnowski_2017_Gradient Descent for Spiking Neural Networks.pdf},
  journal = {arXiv:1706.04698 [cs, q-bio, stat]},
  keywords = {backprop,SNN},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{hupkes2019,
  title = {The Compositionality of Neural Networks: Integrating Symbolism and Connectionism},
  shorttitle = {The Compositionality of Neural Networks},
  author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  year = {2019},
  month = aug,
  abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution based and a transformer model. We provide an in depth analysis of the results, that uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  archivePrefix = {arXiv},
  eprint = {1908.08351},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2PRW6YFJ/Hupkes et al_2019_The compositionality of neural networks.pdf;/Users/x0r/Zotero/storage/NRSZ59XR/1908.html},
  journal = {arXiv:1908.08351 [cs, stat]},
  keywords = {dl},
  primaryClass = {cs, stat}
}

@article{huttunen2017,
  title = {Can Self-Awareness Be Taught? {{Monkeys}} Pass the Mirror Test\textemdash Again},
  shorttitle = {Can Self-Awareness Be Taught?},
  author = {Huttunen, Annamarie W. and Adams, Geoffrey K. and Platt, Michael L.},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {3281--3283},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1701676114},
  abstract = {{$>$} ``Mirrors,'' she said, ``are never to be trusted.''

{$>$} Neil Gaiman, Coraline 

The ability to recognize oneself in the mirror is often held as evidence of self-awareness. However, children under the age of 2 y and most animals do not behave as if their mirrored reflection represents their own face or body, calling their capacity for self-awareness into doubt. In PNAS, Chang et al. (1) build on prior work (2) to provide further evidence that\textemdash with extensive training\textemdash rhesus macaques pass the mirror self-recognition test, suggesting this training either uncovers latent self-awareness or teaches the monkeys a new cognitive skill. Have monkeys finally earned a hard-won spot among the few exalted species to display self-awareness? If so, what does mirror self-recognition imply for our understanding of the brain mechanisms that support it?

Upon initial exposure to a mirror, most animals react to their reflection as if seeing another animal. Gallup (3) first observed that, after repeated exposure to a mirror, chimpanzees began to interact with it in a more self-directed and less social way. To test whether the chimps actually recognized themselves, Gallup marked the animals' foreheads with an odorless dye while the chimps were anesthetized. Upon awakening, chimpanzees spontaneously began touching the otherwise imperceptible mark using their reflection in the mirror. Gallup, and many others thereafter, argued that such behavior is a decisive measure of self-awareness. Since Gallup's pioneering study, the mark procedure has become the litmus test for self-awareness. Human toddlers begin to pass a version of the mark test, not requiring anesthesia, at around 16 mo to 24 mo (4), whereas individuals with certain neuropsychiatric disorders, notably schizophrenia, show impairments in mirror self-recognition (5). In addition to chimpanzees, a menagerie of distantly related species, from elephants to magpies, have passed the mark test (6). Other primates, including gorillas and (previously) \ldots{} 

[{$\carriagereturn$}][1]1To whom correspondence should be addressed. Email: mplatt\{at\}mail.med.upenn.edu.

 [1]: \#xref-corresp-1-1},
  file = {/Users/x0r/Zotero/storage/ZYD3QB6M/Huttunen et al_2017_Can self-awareness be taught.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {13},
  pmid = {28302672}
}

@article{ieda1971,
  title = {A {{Consideration}} of {{Poole}}-{{Frenkel Effect}} on {{Electric Conduction}} in {{Insulators}}},
  author = {Ieda, Masayuki and Sawa, Goro and Kato, Sousuke},
  year = {1971},
  month = sep,
  volume = {42},
  pages = {3737--3740},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.1659678},
  file = {/Users/x0r/Zotero/storage/FAN3YXUD/Ieda et al_1971_A Consideration of Poole‐Frenkel Effect on Electric Conduction in Insulators.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {10}
}

@article{ielmini,
  title = {Kinetic of Resistance Drift in {{PCM}} by Structural Relaxation of the Amorphous Chalcogenide Phase},
  author = {Ielmini, D and Fugazza, D and Boniardi, M and Montemurro, G and Lacaita, A L},
  pages = {3},
  abstract = {Amorphous materials are known to undergo temperature-accelerated structural relaxation (SR), affecting the electrical and optical properties. Due to SR, phase change memory (PCM) displays a drift of resistance after programming. Our study of SR in PCMs demonstrates that drift is controlled by the activation energy for conduction, allowing for the optimization of programming algorithms and active material for minimum drift. Statistical analysis reveals the defectrelated nature of resistance drift. The possible physical processes responsible for resistance drift are finally discussed.},
  file = {/Users/x0r/Zotero/storage/TUEHCEJ8/Ielmini et al_Kinetic of resistance drift in PCM by structural relaxation of the amorphous.pdf},
  keywords = {PCM},
  language = {en}
}

@article{ielmini2007,
  title = {Recovery and {{Drift Dynamics}} of {{Resistance}} and {{Threshold Voltages}} in {{Phase}}-{{Change Memories}}},
  author = {Ielmini, Daniele and Lacaita, Andrea L. and Mantegazza, Davide},
  year = {2007},
  month = feb,
  volume = {54},
  pages = {308--315},
  issn = {0018-9383},
  doi = {10.1109/TED.2006.888752},
  abstract = {The electronic behavior of the chalcogenide material used in phase-change memory (PCM) plays a key role in defining the operation voltages and times of the memory cell. In particular, the threshold voltage for electronic switching of the amorphous chalcogenide determines the boundary between programming and readout operation, while its resistance allows the recognition of the bit status. This paper present a time-resolved analysis of threshold voltage and resistance in a PCM. Both dynamics of threshold voltage and resistance display a fast transient, named recovery behavior, in the first 30 ns after programming. A slower, nonsaturating drift transient is found for longer times. The two transients are discussed referring to electronic and structural rearrangements in the amorphous chalcogenide. Finally, the impact on the device level is considered.},
  file = {/Users/x0r/Zotero/storage/I6NB9TPM/Ielmini et al_2007_Recovery and Drift Dynamics of Resistance and Threshold Voltages in.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {drift,modeling,PCM},
  language = {en},
  number = {2}
}

@article{ielmini2007a,
  title = {Analytical Model for Subthreshold Conduction and Threshold Switching in Chalcogenide-Based Memory Devices},
  author = {Ielmini, Daniele and Zhang, Yuegang},
  year = {2007},
  month = sep,
  volume = {102},
  pages = {054517},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.2773688},
  file = {/Users/x0r/Zotero/storage/TN5ZN23X/Ielmini_Zhang_2007_Analytical model for subthreshold conduction and threshold switching in.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM,threshold-switching},
  language = {en},
  number = {5}
}

@article{ielmini2008,
  title = {Threshold Switching Mechanism by High-Field Energy Gain in the Hopping Transport of Chalcogenide Glasses},
  author = {Ielmini, Daniele},
  year = {2008},
  month = jul,
  volume = {78},
  issn = {1098-0121, 1550-235X},
  doi = {10.1103/PhysRevB.78.035308},
  file = {/Users/x0r/Zotero/storage/FPTJSWRZ/Ielmini_2008_Threshold switching mechanism by high-field energy gain in the hopping.pdf},
  journal = {Physical Review B},
  keywords = {modeling,PCM},
  language = {en},
  number = {3}
}

@article{ielmini2011,
  title = {Phase Change Materials in Non-Volatile Storage},
  author = {Ielmini, Daniele and Lacaita, Andrea L.},
  year = {2011},
  month = dec,
  volume = {14},
  pages = {600--607},
  issn = {13697021},
  doi = {10.1016/S1369-7021(11)70301-7},
  file = {/Users/x0r/Zotero/storage/H4BQWKCC/Ielmini_Lacaita_2011_Phase change materials in non-volatile storage.pdf},
  journal = {Materials Today},
  keywords = {PCM},
  language = {en},
  number = {12}
}

@article{ielmini2016,
  title = {Resistive Switching Memories Based on Metal Oxides: Mechanisms, Reliability and Scaling},
  shorttitle = {Resistive Switching Memories Based on Metal Oxides},
  author = {Ielmini, Daniele},
  year = {2016},
  month = jun,
  volume = {31},
  pages = {063002},
  issn = {0268-1242, 1361-6641},
  doi = {10.1088/0268-1242/31/6/063002},
  abstract = {With the explosive growth of digital data in the era of the Internet of Things (IoT), fast and scalable memory technologies are being researched for data storage and data-driven computation. Among the emerging memories, resistive switching memory (RRAM) raises strong interest due to its high speed, high density as a result of its simple two-terminal structure, and low cost of fabrication. The scaling projection of RRAM, however, requires a detailed understanding of switching mechanisms and there are potential reliability concerns regarding small device sizes. This work provides an overview of the current understanding of bipolarswitching RRAM operation, reliability and scaling. After reviewing the phenomenological and microscopic descriptions of the switching processes, the stability of the low- and high-resistance states will be discussed in terms of conductance fluctuations and evolution in 1D filaments containing only a few atoms. The scaling potential of RRAM will finally be addressed by reviewing the recent breakthroughs in multilevel operation and 3D architecture, making RRAM a strong competitor among future high-density memory solutions.},
  file = {/Users/x0r/Zotero/storage/49T3VSEF/Ielmini_2016_Resistive switching memories based on metal oxides.pdf},
  journal = {Semiconductor Science and Technology},
  keywords = {ReRAM,Sungjung},
  language = {en},
  number = {6}
}

@article{ielmini2017,
  title = {Physics-Based Modeling Approaches of Resistive Switching Devices for Memory and in-Memory Computing Applications},
  author = {Ielmini, D. and Milo, V.},
  year = {2017},
  month = dec,
  volume = {16},
  pages = {1121--1143},
  issn = {1569-8025, 1572-8137},
  doi = {10.1007/s10825-017-1101-9},
  abstract = {The semiconductor industry is currently challenged by the emergence of Internet of Things, Big data, and deep-learning techniques to enable object recognition and inference in portable computers. These revolutions demand new technologies for memory and computation going beyond the standard CMOS-based platform. In this scenario, resistive switching memory (RRAM) is extremely promising in the frame of storage technology, memory devices, and inmemory computing circuits, such as memristive logic or neuromorphic machines. To serve as enabling technology for these new fields, however, there is still a lack of industrial tools to predict the device behavior under certain operation schemes and to allow for optimization of the device properties based on materials and stack engineering. This work provides an overview of modeling approaches for RRAM simulation, at the level of technology computer aided design and high-level compact models for circuit simulations. Finite element method modeling, kinetic Monte Carlo models, and physics-based analytical models will be reviewed. The adaptation of modeling schemes to various RRAM concepts, such as filamentary switching and interface switching, will be discussed. Finally, application cases of compact modeling to simulate simple RRAM circuits for computing will be shown.},
  file = {/Users/x0r/Zotero/storage/97XFRIMR/Ielmini_Milo_2017_Physics-based modeling approaches of resistive switching devices for memory and.pdf},
  journal = {Journal of Computational Electronics},
  keywords = {ReRAM,Sungjung},
  language = {en},
  number = {4}
}

@article{ievlev2014,
  title = {Intermittency, Quasiperiodicity and Chaos in Probe-Induced Ferroelectric Domain Switching},
  author = {Ievlev, A. V. and Jesse, S. and Morozovska, A. N. and Strelcov, E. and Eliseev, E. A. and Pershin, Y. V. and Kumar, A. and Shur, V. Ya. and Kalinin, S. V.},
  year = {2014},
  month = jan,
  volume = {10},
  pages = {59--66},
  issn = {1745-2473, 1745-2481},
  doi = {10.1038/nphys2796},
  file = {/Users/x0r/Zotero/storage/A5DK27ET/Ievlev et al_2014_Intermittency, quasiperiodicity and chaos in probe-induced ferroelectric domain.pdf},
  journal = {Nature Physics},
  language = {en},
  number = {1}
}

@article{indiveri,
  title = {The Importance of Space and Time in Neuromorphic Cognitive Agents},
  author = {Indiveri, Giacomo},
  pages = {21},
  abstract = {Artificial neural networks and computational neuroscience models have made tremendous progress, allowing computers to achieve impressive results in artificial intelligence (AI) applications, such as image recognition, natural language processing, or autonomous driving. Despite this remarkable progress, biological neural systems consume orders of magnitude less energy than today's artificial neural networks and are much more agile and adaptive. This efficiency and adaptivity gap is partially explained by the computing substrate of biological neural processing systems that is fundamentally different from the way today's computers are built. Biological systems use in-memory computing elements operating in a massively parallel way rather than time-multiplexed computing units that are reused in a sequential fashion. Moreover, activity of biological neurons follows continuous-time dynamics in real, physical time, instead of operating on discrete temporal cycles abstracted away from real-time. Here, we present neuromorphic processing devices that emulate the biological style of processing by using parallel instances of mixed-signal analog/digital circuits that operate in real time. We argue that this approach brings significant advantages in efficiency of computation. We show examples of embodied neuromorphic agents that use such devices to interact with the environment and exhibit autonomous learning.},
  file = {/Users/x0r/Zotero/storage/D9ZFJETW/Indiveri_The importance of space and time in neuromorphic cognitive agents.pdf},
  journal = {IEEE SIGNAL PROCESSING MAGAZINE},
  keywords = {AGI,neuromorphic},
  language = {en}
}

@article{ingraham2018,
  title = {Learning {{Protein Structure}} with a {{Differentiable Simulator}}},
  author = {Ingraham, John and Riesselman, Adam and Sander, Chris and Marks, Debora},
  year = {2018},
  month = sep,
  abstract = {The Boltzmann distribution is a natural model for many systems, from brains to materials and biomolecules, but is often of limited utility for fitting data because Monte Carlo algorithms are unable...},
  file = {/Users/x0r/Zotero/storage/DL8ASQDF/Ingraham et al_2018_Learning Protein Structure with a Differentiable Simulator.pdf},
  keywords = {dl,nanotech}
}

@article{iverson,
  title = {Notation as a {{Tool}} of {{Thought}}},
  author = {Iverson, Kenneth E},
  pages = {52},
  file = {/Users/x0r/Zotero/storage/NXHJ3HW4/Iverson_Notation as a Tool of Thought.pdf},
  keywords = {math,To read},
  language = {en}
}

@article{jaderberg2015,
  title = {Spatial {{Transformer Networks}}},
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  year = {2015},
  month = jun,
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  archivePrefix = {arXiv},
  eprint = {1506.02025},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/3H5GCCMP/Jaderberg et al_2015_Spatial Transformer Networks.pdf},
  journal = {arXiv:1506.02025 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{jaderberg2016,
  title = {Decoupled {{Neural Interfaces}} Using {{Synthetic Gradients}}},
  author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = aug,
  abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass \textendash{} amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
  archivePrefix = {arXiv},
  eprint = {1608.05343},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LZSFCGZW/Jaderberg et al_2016_Decoupled Neural Interfaces using Synthetic Gradients.pdf},
  journal = {arXiv:1608.05343 [cs]},
  keywords = {backprop},
  language = {en},
  primaryClass = {cs}
}

@article{jaderberg2018,
  title = {Human-Level Performance in First-Person Multiplayer Games with Population-Based Deep Reinforcement Learning},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Castaneda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  year = {2018},
  month = jul,
  abstract = {Recent progress in artificial intelligence through reinforcement learning (RL) has shown great success on increasingly complex single-agent environments and two-player turn-based games. However, the real-world contains multiple agents, each learning and acting independently to cooperate and compete with other agents, and environments reflecting this degree of complexity remain an open challenge. In this work, we demonstrate for the first time that an agent can achieve human-level in a popular 3D multiplayer first-person video game, Quake III Arena Capture the Flag, using only pixels and game points as input. These results were achieved by a novel two-tier optimisation process in which a population of independent RL agents are trained concurrently from thousands of parallel matches with agents playing in teams together and against each other on randomly generated environments. Each agent in the population learns its own internal reward signal to complement the sparse delayed reward from winning, and selects actions using a novel temporally hierarchical representation that enables the agent to reason at multiple timescales. During game-play, these agents display human-like behaviours such as navigating, following, and defending based on a rich learned representation that is shown to encode high-level game knowledge. In an extensive tournament-style evaluation the trained agents exceeded the win-rate of strong human players both as teammates and opponents, and proved far stronger than existing state-of-the-art agents. These results demonstrate a significant jump in the capabilities of artificial agents, bringing us closer to the goal of human-level intelligence.},
  archivePrefix = {arXiv},
  eprint = {1807.01281},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/U37WQ8PL/Jaderberg et al_2018_Human-level performance in first-person multiplayer games with population-based.pdf},
  journal = {arXiv:1807.01281 [cs, stat]},
  keywords = {rl},
  primaryClass = {cs, stat}
}

@article{jaderberg2019,
  title = {Human-Level Performance in {{3D}} Multiplayer Games with Population-Based Reinforcement Learning},
  author = {Jaderberg, Max and Czarnecki, Wojciech M. and Dunning, Iain and Marris, Luke and Lever, Guy and Casta{\~n}eda, Antonio Garcia and Beattie, Charles and Rabinowitz, Neil C. and Morcos, Ari S. and Ruderman, Avraham and Sonnerat, Nicolas and Green, Tim and Deason, Louise and Leibo, Joel Z. and Silver, David and Hassabis, Demis and Kavukcuoglu, Koray and Graepel, Thore},
  year = {2019},
  month = may,
  volume = {364},
  pages = {859--865},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aau6249},
  abstract = {Artificial teamwork
Artificially intelligent agents are getting better and better at two-player games, but most real-world endeavors require teamwork. Jaderberg et al. designed a computer program that excels at playing the video game Quake III Arena in Capture the Flag mode, where two multiplayer teams compete in capturing the flags of the opposing team. The agents were trained by playing thousands of games, gradually learning successful strategies not unlike those favored by their human counterparts. Computer agents competed successfully against humans even when their reaction times were slowed to match those of humans.
Science, this issue p. 859
Reinforcement learning (RL) has shown great success in increasingly complex single-agent environments and two-player turn-based games. However, the real world contains multiple agents, each learning and acting independently to cooperate and compete with other agents. We used a tournament-style evaluation to demonstrate that an agent can achieve human-level performance in a three-dimensional multiplayer first-person video game, Quake III Arena in Capture the Flag mode, using only pixels and game points scored as input. We used a two-tier optimization process in which a population of independent RL agents are trained concurrently from thousands of parallel matches on randomly generated environments. Each agent learns its own internal reward signal and rich representation of the world. These results indicate the great potential of multiagent reinforcement learning for artificial intelligence research.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.
Teams of artificial agents compete successfully against humans in the video game Quake III Arena in Capture the Flag mode.},
  copyright = {Copyright \textcopyright{} 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/x0r/Zotero/storage/DCDDPZYU/Jaderberg et al_2019_Human-level performance in 3D multiplayer games with population-based.pdf;/Users/x0r/Zotero/storage/ESV9PPQX/aau6249-Jaderberg-SM.pdf},
  journal = {Science},
  keywords = {AGI,dl,rl},
  language = {en},
  number = {6443}
}

@article{james2018,
  title = {Guest {{Editorial}}: {{Special Issue}} on {{Large}}-{{Scale Memristive Systems}} and {{Neurochips}} for {{Computational Intelligence}}},
  shorttitle = {Guest {{Editorial}}},
  author = {James, A. Pappachen and Salama, K. Nabil and Li, H. and Biolek, D. and Indiveri, G. and Chua, L. O.},
  year = {2018},
  month = oct,
  volume = {2},
  pages = {320--323},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2018.2867375},
  file = {/Users/x0r/Zotero/storage/JAGSICTS/James et al_2018_Guest Editorial.pdf},
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  keywords = {neuromorphic},
  language = {en},
  number = {5}
}

@inproceedings{jeyasingh2012,
  title = {Phase {{Change Memory}}: {{Scaling}} and Applications},
  shorttitle = {Phase {{Change Memory}}},
  author = {Jeyasingh, Rakesh and Liang, Jiale and Caldwell, Marissa A. and Kuzum, Duygu and Wong, H.-S. Philip},
  year = {2012},
  month = sep,
  pages = {1--7},
  publisher = {{IEEE}},
  doi = {10.1109/CICC.2012.6330621},
  abstract = {Phase Change Memory (PCM) technology is a promising candidate for the future non-volatile memory applications. Scaling of PCM into the sub-10 nm regime has been demonstrated using novel applications of nanofabrication techniques. PCM devices using solution-processed GeTe nanoparticles of diameter range 1.8 \textendash{} 3.4nm has been demonstrated. Highly scaled ({$<$}2nm) PCM cross-point device using carbon nanotube as the electrode is fabricated proving the scalability of PCM to ultra small dimensions. The use of PCM as a nanoelectronic synapse for neuromorphic computation is also demonstrated as an illustration of PCM application beyond digital memory.},
  file = {/Users/x0r/Zotero/storage/75FIQ74T/Jeyasingh et al_2012_Phase Change Memory.pdf;/Users/x0r/Zotero/storage/BJYGED4A/Jeyasingh et al_2012_Phase Change Memory.pdf},
  isbn = {978-1-4673-1556-2 978-1-4673-1555-5 978-1-4673-1554-8},
  keywords = {PCM},
  language = {en}
}

@article{jiang2019,
  title = {On {{Value Functions}} and the {{Agent}}-{{Environment Boundary}}},
  author = {Jiang, Nan},
  year = {2019},
  month = may,
  abstract = {When function approximation is deployed in reinforcement learning (RL), the same problem may be formulated in different ways, often by treating a pre-processing step as a part of the environment or as part of the agent. As a consequence, fundamental concepts in RL, such as (optimal) value functions, are not uniquely defined as they depend on where we draw this agent-environment boundary, causing problems in theoretical analyses that provide optimality guarantees. We address this issue via a simple and novel boundary-invariant analysis of Fitted Q-Iteration, a representative RL algorithm, where the assumptions and the guarantees are invariant to the choice of boundary. We also discuss closely related issues on state resetting and Monte-Carlo Tree Search, deterministic vs stochastic systems, imitation learning, and the verifiability of theoretical assumptions from data.},
  archivePrefix = {arXiv},
  eprint = {1905.13341},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/F67CBYUY/Jiang_2019_On Value Functions and the Agent-Environment Boundary.pdf;/Users/x0r/Zotero/storage/6P6ZZKGU/1905.html},
  journal = {arXiv:1905.13341 [cs, stat]},
  keywords = {AGI,rl},
  primaryClass = {cs, stat}
}

@article{Jo_etal10,
  ids = {jo2010},
  title = {Nanoscale Memristor Device as Synapse in Neuromorphic Systems},
  author = {Jo, S. H. and Chang, T. and Ebong, I. and Bhadviya, B. B. and Mazumder, P. and Lu, W.},
  year = {2010},
  volume = {10},
  pages = {1297--1301},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/967Z7D7Z/Jo et al_2010_Nanoscale Memristor Device as Synapse in Neuromorphic Systems.pdf},
  journal = {Nano letters},
  keywords = {neuromorphic},
  number = {4}
}

@article{jonas2017,
  title = {Could a {{Neuroscientist Understand}} a {{Microprocessor}}?},
  author = {Jonas, Eric and Kording, Konrad Paul},
  editor = {Diedrichsen, J{\"o}rn},
  year = {2017},
  month = jan,
  volume = {13},
  pages = {e1005268},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005268},
  abstract = {There is a popular belief in neuroscience that we are primarily data limited, and that producing large, multimodal, and complex datasets will, with the help of advanced data analysis algorithms, lead to fundamental insights into the way the brain processes information. These datasets do not yet exist, and if they did we would have no way of evaluating whether or not the algorithmically-generated insights were sufficient or even correct. To address this, here we take a classical microprocessor as a model organism, and use our ability to perform arbitrary experiments on it to see if popular data analysis methods from neuroscience can elucidate the way it processes information. Microprocessors are among those artificial information processing systems that are both complex and that we understand at all levels, from the overall logical flow, via logical gates, to the dynamics of transistors. We show that the approaches reveal interesting structure in the data but do not meaningfully describe the hierarchy of information processing in the microprocessor. This suggests current analytic approaches in neuroscience may fall short of producing meaningful understanding of neural systems, regardless of the amount of data. Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.},
  file = {/Users/x0r/Zotero/storage/ET997QBJ/Jonas_Kording_2017_Could a Neuroscientist Understand a Microprocessor.pdf},
  journal = {PLOS Computational Biology},
  keywords = {neuroscience},
  language = {en},
  number = {1}
}

@book{jones1985,
  title = {Theoretical Solid State Physics},
  author = {Jones, William and March, Norman H.},
  year = {1985},
  publisher = {{Dover Publications}},
  address = {{New York}},
  file = {/Users/x0r/Zotero/storage/VZCL775V/Jones_March_1985_Theoretical solid state physics.pdf},
  isbn = {978-0-486-65015-9 978-0-486-65016-6},
  lccn = {QC176.A1 J572 1985}
}

@article{jouppi2017,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  month = apr,
  file = {/Users/x0r/Zotero/storage/I94KD64B/Jouppi et al_2017_In-Datacenter Performance Analysis of a Tensor Processing Unit.pdf;/Users/x0r/Zotero/storage/E9W6VRBE/1704.html},
  keywords = {perf,TPU},
  language = {en}
}

@article{jung2007,
  title = {The {{Parieto}}-{{Frontal Integration Theory}} ({{P}}-{{FIT}}) of Intelligence: {{Converging}} Neuroimaging Evidence},
  shorttitle = {The {{Parieto}}-{{Frontal Integration Theory}} ({{P}}-{{FIT}}) of Intelligence},
  author = {Jung, Rex E. and Haier, Richard J.},
  year = {2007},
  month = apr,
  volume = {30},
  pages = {135},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X07001185},
  abstract = {Is there a biology of intelligence which is characteristic of the normal human nervous system?'' Here we review 37 modern neuroimaging studies in an attempt to address this question posed by Halstead (1947) as he and other icons of the last century endeavored to understand how brain and behavior are linked through the expression of intelligence and reason. Reviewing studies from functional (i.e., functional magnetic resonance imaging, positron emission tomography) and structural (i.e., magnetic resonance spectroscopy, diffusion tensor imaging, voxel-based morphometry) neuroimaging paradigms, we report a striking consensus suggesting that variations in a distributed network predict individual differences found on intelligence and reasoning tasks. We describe this network as the Parieto-Frontal Integration Theory (P-FIT). The P-FIT model includes, by Brodmann areas (BAs): the dorsolateral prefrontal cortex (BAs 6, 9, 10, 45, 46, 47), the inferior (BAs 39, 40) and superior (BA 7) parietal lobule, the anterior cingulate (BA 32), and regions within the temporal (BAs 21, 37) and occipital (BAs 18, 19) lobes. White matter regions (i.e., arcuate fasciculus) are also implicated. The P-FIT is examined in light of findings from human lesion studies, including missile wounds, frontal lobotomy/leukotomy, temporal lobectomy, and lesions resulting in damage to the language network (e.g., aphasia), as well as findings from imaging research identifying brain regions under significant genetic control. Overall, we conclude that modern neuroimaging techniques are beginning to articulate a biology of intelligence. We propose that the P-FIT provides a parsimonious account for many of the empirical observations, to date, which relate individual differences in intelligence test scores to variations in brain structure and function. Moreover, the model provides a framework for testing new hypotheses in future experimental designs.},
  file = {/Users/x0r/Zotero/storage/FBCLSXJN/Jung_Haier_2007_The Parieto-Frontal Integration Theory (P-FIT) of intelligence.pdf},
  journal = {Behavioral and Brain Sciences},
  keywords = {evolution,neuroscience},
  language = {en},
  number = {02}
}

@article{kaes2015,
  title = {High-Field Electrical Transport in Amorphous Phase-Change Materials},
  author = {Kaes, Matthias and Le Gallo, Manuel and Sebastian, Abu and Salinga, Martin and Krebs, Daniel},
  year = {2015},
  month = oct,
  volume = {118},
  pages = {135707},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.4932204},
  file = {/Users/x0r/Zotero/storage/6DNI6K8U/Kaes et al_2015_High-field electrical transport in amorphous phase-change materials.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {13}
}

@article{kaes2016,
  title = {Impact of Defect Occupation on Conduction in Amorphous {{Ge2Sb2Te5}}},
  author = {Kaes, Matthias and Salinga, Martin},
  year = {2016},
  month = oct,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep31699},
  file = {/Users/x0r/Zotero/storage/B72IGAUH/Kaes_Salinga_2016_Impact of defect occupation on conduction in amorphous Ge2Sb2Te5.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@book{kaeslin2015,
  title = {Top-down Digital {{VLSI}} Design: From Architectures to Gate-Level Circuits and {{FPGAS}}},
  shorttitle = {Top-down Digital {{VLSI}} Design},
  author = {Kaeslin, Hubert},
  year = {2015},
  publisher = {{Morgan Kaufmann is an imprint of Elsevier}},
  address = {{Waltham, MA}},
  file = {/Users/x0r/Zotero/storage/THL4QIKR/Kaeslin - 2015 - Top-down digital VLSI design from architectures t.pdf},
  isbn = {978-0-12-800730-3},
  keywords = {VLSI},
  language = {en},
  lccn = {TK7874.65 .K336 2015}
}

@article{kaiser2015,
  title = {Neural {{GPUs Learn Algorithms}}},
  author = {Kaiser, {\L}ukasz and Sutskever, Ilya},
  year = {2015},
  month = nov,
  abstract = {Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they cannot be parallelized and are are hard to train due to their large depth when unfolded.},
  archivePrefix = {arXiv},
  eprint = {1511.08228},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/M3KPQJQZ/Kaiser_Sutskever_2015_Neural GPUs Learn Algorithms.pdf},
  journal = {arXiv:1511.08228 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{kaiser2019a,
  ids = {kaiser2019},
  title = {Model-{{Based Reinforcement Learning}} for {{Atari}}},
  author = {Kaiser, Lukasz and Babaeizadeh, Mohammad and Milos, Piotr and Osinski, Blazej and Campbell, Roy H. and Czechowski, Konrad and Erhan, Dumitru and Finn, Chelsea and Kozakowski, Piotr and Levine, Sergey and Mohiuddin, Afroz and Sepassi, Ryan and Tucker, George and Michalewski, Henryk},
  year = {2019},
  month = mar,
  abstract = {Model-free reinforcement learning (RL) can be used to learn effective policies for complex tasks, such as Atari games, even from image observations. However, this typically requires very large amounts of interaction -- substantially more, in fact, than a human would need to learn the same games. How can people learn so quickly? Part of the answer may be that people can learn how the game works and predict which actions will lead to desirable outcomes. In this paper, we explore how video prediction models can similarly enable agents to solve Atari games with fewer interactions than model-free methods. We describe Simulated Policy Learning (SimPLe), a complete model-based deep RL algorithm based on video prediction models and present a comparison of several model architectures, including a novel architecture that yields the best results in our setting. Our experiments evaluate SimPLe on a range of Atari games in low data regime of \$100\$K interactions between the agent and the environment, which corresponds to two hours of real-time play. In most games SimPLe outperforms state-of-the-art model-free algorithms, in some games by over an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1903.00374},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/HNWTY985/Kaiser et al_2019_Model-Based Reinforcement Learning for Atari.pdf;/Users/x0r/Zotero/storage/Q5LIHALF/Kaiser et al_2019_Model-Based Reinforcement Learning for Atari.pdf},
  journal = {arXiv:1903.00374 [cs, stat]},
  keywords = {rl,To read},
  primaryClass = {cs, stat}
}

@article{kaiser2019b,
  title = {Embodied {{Synaptic Plasticity With Online Reinforcement Learning}}},
  author = {Kaiser, Jacques and Hoff, Michael and Konle, Andreas and Vasquez Tieck, J. Camilo and Kappel, David and Reichard, Daniel and Subramoney, Anand and Legenstein, Robert and Roennau, Arne and Maass, Wolfgang and Dillmann, R{\"u}diger},
  year = {2019},
  month = oct,
  volume = {13},
  pages = {81},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00081},
  abstract = {The endeavor to understand the brain involves multiple collaborating research fields. Classically, synaptic plasticity rules derived by theoretical neuroscientists are evaluated in isolation on pattern classification tasks. This contrasts with the biological brain which purpose is to control a body in closed-loop. This paper contributes to bringing the fields of computational neuroscience and robotics closer together by integrating open-source software components from these two fields. The resulting framework allows to evaluate the validity of biologically-plausibe plasticity models in closed-loop robotics environments. We demonstrate this framework to evaluate Synaptic Plasticity with Online REinforcement learning (SPORE), a reward-learning rule based on synaptic sampling, on two visuomotor tasks: reaching and lane following. We show that SPORE is capable of learning to perform policies within the course of simulated hours for both tasks. Provisional parameter explorations indicate that the learning rate and the temperature driving the stochastic processes that govern synaptic learning dynamics need to be regulated for performance improvements to be retained. We conclude by discussing the recent deep reinforcement learning techniques which would be beneficial to increase the functionality of SPORE on visuomotor tasks.},
  file = {/Users/x0r/Zotero/storage/2WEXEH86/Kaiser et al. - 2019 - Embodied Synaptic Plasticity With Online Reinforce.pdf},
  journal = {Frontiers in Neurorobotics},
  keywords = {rl},
  language = {en}
}

@article{kaiser2019c,
  title = {Embodied {{Synaptic Plasticity With Online Reinforcement Learning}}},
  author = {Kaiser, Jacques and Hoff, Michael and Konle, Andreas and Vasquez Tieck, J. Camilo and Kappel, David and Reichard, Daniel and Subramoney, Anand and Legenstein, Robert and Roennau, Arne and Maass, Wolfgang and Dillmann, R{\"u}diger},
  year = {2019},
  month = oct,
  volume = {13},
  pages = {81},
  issn = {1662-5218},
  doi = {10.3389/fnbot.2019.00081},
  abstract = {The endeavor to understand the brain involves multiple collaborating research fields. Classically, synaptic plasticity rules derived by theoretical neuroscientists are evaluated in isolation on pattern classification tasks. This contrasts with the biological brain which purpose is to control a body in closed-loop. This paper contributes to bringing the fields of computational neuroscience and robotics closer together by integrating open-source software components from these two fields. The resulting framework allows to evaluate the validity of biologically-plausibe plasticity models in closed-loop robotics environments. We demonstrate this framework to evaluate Synaptic Plasticity with Online REinforcement learning (SPORE), a reward-learning rule based on synaptic sampling, on two visuomotor tasks: reaching and lane following. We show that SPORE is capable of learning to perform policies within the course of simulated hours for both tasks. Provisional parameter explorations indicate that the learning rate and the temperature driving the stochastic processes that govern synaptic learning dynamics need to be regulated for performance improvements to be retained. We conclude by discussing the recent deep reinforcement learning techniques which would be beneficial to increase the functionality of SPORE on visuomotor tasks.},
  file = {/Users/x0r/Zotero/storage/W58YJRX4/Kaiser et al. - 2019 - Embodied Synaptic Plasticity With Online Reinforce.pdf},
  journal = {Frontiers in Neurorobotics},
  keywords = {To read},
  language = {en}
}

@article{kaiser2020,
  title = {Synaptic {{Plasticity Dynamics}} for {{Deep Continuous Local Learning}} ({{DECOLLE}})},
  author = {Kaiser, Jacques and Mostafa, Hesham and Neftci, Emre},
  year = {2020},
  volume = {14},
  publisher = {{Frontiers}},
  issn = {1662-453X},
  doi = {10.3389/fnins.2020.00424},
  abstract = {A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, discusses similarities between learning dynamics employed in deep Artificial Neural Network and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using locally synthesized gradients can overcome this challenge. Here, we show that synthetic gradients enable the derivation of Deep Continuous Local Learning (DECOLLE) in spiking neural networks. DECOLLE is capable of learning deep spatio-temporal representations from spikes relying solely on local information. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.},
  file = {/Users/x0r/Zotero/storage/62CXJBJT/Kaiser et al. - 2020 - Synaptic Plasticity Dynamics for Deep Continuous L.pdf},
  journal = {Frontiers in Neuroscience},
  language = {English}
}

@article{kaplan2020,
  title = {Nested {{Neuronal Dynamics Orchestrate}} a {{Behavioral Hierarchy}} across {{Timescales}}},
  author = {Kaplan, Harris S. and Salazar Thula, Oriana and Khoss, Niklas and Zimmer, Manuel},
  year = {2020},
  month = feb,
  volume = {105},
  pages = {562-576.e9},
  issn = {08966273},
  doi = {10.1016/j.neuron.2019.10.037},
  abstract = {Classical and modern ethological studies suggest that animal behavior is organized hierarchically across timescales, such that longer-timescale behaviors are composed of specific shorter-timescale actions. Despite progress relating neuronal dynamics to single-timescale behavior, it remains unclear how different timescale dynamics interact to give rise to such higher-order behavioral organization. Here, we show, in the nematode Caenorhabditis elegans, that a behavioral hierarchy spanning three timescales is implemented by nested neuronal dynamics. At the uppermost hierarchical level, slow neuronal population dynamics spanning brain and motor periphery control two faster motor neuron oscillations, toggling them between different activity states and functional roles. At lower hierarchical levels, these faster oscillations are further nested in a manner that enables flexible behavioral control in an otherwise rigid hierarchical framework. Our findings establish nested neuronal activity patterns as a repeated dynamical motif of the C. elegans nervous system, which together implement a controllable hierarchical organization of behavior.},
  file = {/Users/x0r/Zotero/storage/2RZ443MK/Kaplan et al. - 2020 - Nested Neuronal Dynamics Orchestrate a Behavioral .pdf},
  journal = {Neuron},
  language = {en},
  number = {3}
}

@article{kapturowski2018,
  title = {Recurrent {{Experience Replay}} in {{Distributed Reinforcement Learning}}},
  author = {Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  year = {2018},
  month = sep,
  abstract = {Building on the recent successes of distributed training of RL agents, in this paper we investigate the training of RNN-based RL agents from distributed prioritized experience replay. We study the...},
  file = {/Users/x0r/Zotero/storage/7E94QNGX/Kapturowski et al_2018_Recurrent Experience Replay in Distributed Reinforcement Learning.pdf},
  keywords = {rl}
}

@article{kar2019,
  title = {Evidence That Recurrent Circuits Are Critical to the Ventral Stream's Execution of Core Object Recognition Behavior},
  author = {Kar, Kohitij and Kubilius, Jonas and Schmidt, Kailyn and Issa, Elias B. and DiCarlo, James J.},
  year = {2019},
  month = apr,
  pages = {1},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0392-5},
  abstract = {Using model- and primate behavior-driven image selection with large-scale electrophysiology in monkeys performing core recognition tasks, Kar et al. provide evidence that automatically engaged recurrent circuits are critical for rapid object identification.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  file = {/Users/x0r/Zotero/storage/EAX9HHLH/Kar et al_2019_Evidence that recurrent circuits are critical to the ventral stream’s execution.pdf},
  journal = {Nature Neuroscience},
  keywords = {dl,neuroscience},
  language = {En}
}

@article{karpov2007,
  title = {Fundamental Drift of Parameters in Chalcogenide Phase Change Memory},
  author = {Karpov, I. V. and Mitra, M. and Kau, D. and Spadini, G. and Kryukov, Y. A. and Karpov, V. G.},
  year = {2007},
  month = dec,
  volume = {102},
  pages = {124503},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.2825650},
  file = {/Users/x0r/Zotero/storage/DRRQPH27/Karpov et al_2007_Fundamental drift of parameters in chalcogenide phase change memory.pdf},
  journal = {Journal of Applied Physics},
  keywords = {drift,modeling,PCM},
  language = {en},
  number = {12}
}

@article{karpov2008,
  title = {Field-Induced Nucleation in Phase Change Memory},
  author = {Karpov, V. G. and Kryukov, Y. A. and Karpov, I. V. and Mitra, M.},
  year = {2008},
  month = aug,
  volume = {78},
  issn = {1098-0121, 1550-235X},
  doi = {10.1103/PhysRevB.78.052201},
  file = {/Users/x0r/Zotero/storage/476NBAYS/Karpov et al_2008_Field-induced nucleation in phase change memory.pdf},
  journal = {Physical Review B},
  keywords = {modeling,PCM},
  language = {en},
  number = {5}
}

@article{karpov2010,
  title = {Phase {{Change Memory}} with {{Chalcogenide Selector}} ({{PCMS}}): {{Characteristic Behaviors}}, {{Physical Models}} and {{Key Material Properties}}},
  shorttitle = {Phase {{Change Memory}} with {{Chalcogenide Selector}} ({{PCMS}})},
  author = {Karpov, Ilya V. and Kencke, David and Kau, Derchang and Tang, Stephen and Spadini, Gianpalo},
  year = {2010},
  month = jan,
  volume = {1250},
  issn = {1946-4274},
  doi = {10.1557/PROC-1250-G14-01-H07-01},
  abstract = {We present a novel scalable and stackable nonvolatile solid state memory. Each cell consists of a storage element, based on phase change memory (PCM) element, and an integrated selector, using an Ovonic threshold switch (OTS). The cell is implemented to enable a true cross-point array. The main device characteristics and behaviors, corresponding physical processes in different operation modes, and key material properties are discussed.},
  file = {/Users/x0r/Zotero/storage/INNY82RA/Karpov et al_2010_Phase Change Memory with Chalcogenide Selector (PCMS).pdf},
  journal = {MRS Proceedings},
  keywords = {modeling,neuromorphic,OTS,PCM},
  language = {en}
}

@article{karras2018,
  title = {A {{Style}}-{{Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  year = {2018},
  month = dec,
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  archivePrefix = {arXiv},
  eprint = {1812.04948},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ADPYM2AQ/Karras et al_2018_A Style-Based Generator Architecture for Generative Adversarial Networks.pdf},
  journal = {arXiv:1812.04948 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{karunaratne2020,
  title = {In-Memory Hyperdimensional Computing},
  author = {Karunaratne, Geethan and Le Gallo, Manuel and Cherubini, Giovanni and Benini, Luca and Rahimi, Abbas and Sebastian, Abu},
  year = {2020},
  month = jun,
  issn = {2520-1131},
  doi = {10.1038/s41928-020-0410-3},
  file = {/Users/x0r/Zotero/storage/RXCJRLIC/Karunaratne et al. - 2020 - In-memory hyperdimensional computing.pdf},
  journal = {Nature Electronics},
  language = {en}
}

@book{kashchiev2000,
  title = {Nucleation: Basic Theory with Applications},
  shorttitle = {Nucleation},
  author = {Kashchiev, Dimo},
  year = {2000},
  publisher = {{Butterworth Heinemann}},
  address = {{Oxford ; Boston}},
  file = {/Users/x0r/Zotero/storage/CAUTNMNX/Kashchiev_2000_Nucleation.pdf},
  isbn = {978-0-7506-4682-6},
  keywords = {PCM},
  language = {en},
  lccn = {QD548 .K37 2000}
}

@article{kelton1986,
  title = {Transient Nucleation Effects in Glass Formation},
  author = {Kelton, K.F. and Greer, A.L.},
  year = {1986},
  month = feb,
  volume = {79},
  pages = {295--309},
  issn = {00223093},
  doi = {10.1016/0022-3093(86)90229-2},
  file = {/Users/x0r/Zotero/storage/CEGIE8I7/Kelton_Greer_1986_Transient nucleation effects in glass formation.pdf},
  journal = {Journal of Non-Crystalline Solids},
  keywords = {PCM},
  language = {en},
  number = {3}
}

@article{kendall2017,
  title = {Multi-{{Task Learning Using Uncertainty}} to {{Weigh Losses}} for {{Scene Geometry}} and {{Semantics}}},
  author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  year = {2017},
  month = may,
  abstract = {Numerous deep learning applications benefit from multitask learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
  archivePrefix = {arXiv},
  eprint = {1705.07115},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/C499379E/Kendall et al_2017_Multi-Task Learning Using Uncertainty to Weigh Losses for Scene Geometry and.pdf},
  journal = {arXiv:1705.07115 [cs]},
  keywords = {meta-learning},
  language = {en},
  primaryClass = {cs}
}

@article{kim2007,
  title = {Three-Dimensional Simulation Model of Switching Dynamics in Phase Change Random Access Memory Cells},
  author = {Kim, Dae-Hwang and Merget, Florian and F{\"o}rst, Michael and Kurz, Heinrich},
  year = {2007},
  month = mar,
  volume = {101},
  pages = {064512},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.2710440},
  file = {/Users/x0r/Zotero/storage/N267J9FE/Kim et al_2007_Three-dimensional simulation model of switching dynamics in phase change random.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM,threshold-switching},
  language = {en},
  number = {6}
}

@article{kim2011,
  title = {Resistance and {{Threshold Switching Voltage Drift Behavior}} in {{Phase}}-{{Change Memory}} and {{Their Temperature Dependence}} at {{Microsecond Time Scales Studied Using}} a {{Micro}}-{{Thermal Stage}}},
  author = {Kim, SangBum and Lee, Byoungil and Asheghi, Mehdi and Hurkx, Fred and Reifenberg, John P. and Goodson, Kenneth E. and Wong, H.-S. Philip},
  year = {2011},
  month = mar,
  volume = {58},
  pages = {584--592},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2010.2095502},
  abstract = {We study the drift behavior of RESET resistance RRESET and threshold switching voltage Vth in phase-change memory (PCM) and their temperature dependence. To extend the temperature-dependent measurement to microsecond time scales, we integrate an innovative micro-thermal stage (MTS) on the PCM cell. The MTS changes the temperature of the programmed region of the PCM cell within a few microseconds by placing the Pt heater in close proximity of the programmed region. First, we experimentally verify the existing phenomenological RRESET and Vth drift model for constant annealing temperature at various temperatures between 25 {$\smwhtcircle$}C and 185 {$\smwhtcircle$}C down to 100 {$\mu$}s and show that the measured temperature dependence of the drift coefficient agrees well with what is expected from the existing drift models. Based on the existing drift model for a constant annealing temperature, we derive the analytical expression for the RRESET drift for time-varying annealing temperature and experimentally verify the analytical expression. The derived analytical expression is important to understand the impact of thermal disturbance on PCM reliability such as variations in RRESET and Vth.},
  file = {/Users/x0r/Zotero/storage/WB5AVXS2/Kim et al_2011_Resistance and Threshold Switching Voltage Drift Behavior in Phase-Change.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {drift,GST,modeling},
  language = {en},
  number = {3}
}

@article{kim2013,
  title = {Physical Electro-Thermal Model of Resistive Switching in Bi-Layered Resistance-Change Memory},
  author = {Kim, Sungho and Kim, Sae-Jin and Kim, Kyung Min and Lee, Seung Ryul and Chang, Man and Cho, Eunju and Kim, Young-Bae and Kim, Chang Jung and {-In Chung}, U. and Yoo, In-Kyeong},
  year = {2013},
  month = dec,
  volume = {3},
  issn = {2045-2322},
  doi = {10.1038/srep01680},
  file = {/Users/x0r/Zotero/storage/RJ3Z8VRR/Kim et al_2013_Physical electro-thermal model of resistive switching in bi-layered.pdf},
  journal = {Scientific Reports},
  keywords = {PCM},
  language = {en},
  number = {1}
}

@article{kim2014,
  title = {A {{Finite Element Model}} for {{Bipolar Resistive Random Access Memory}}},
  author = {Kim, Kwanyong and Lee, Kwangseok and Lee, Keun-Ho and Park, Young-Kwan and Choi, Woo Young},
  year = {2014},
  month = jun,
  volume = {14},
  pages = {268--273},
  issn = {1598-1657},
  doi = {10.5573/JSTS.2014.14.3.268},
  abstract = {The forming, reset and set operation of bipolar resistive random access memory (RRAM) have been predicted by using a finite element (FE) model which includes interface effects. To the best of our knowledge, our bipolar RRAM model is applicable to realistic cell structure optimization because our model is based on the FE method (FEM) unlike precedent models.},
  file = {/Users/x0r/Zotero/storage/B7FT9Q29/Kim et al_2014_A Finite Element Model for Bipolar Resistive Random Access Memory.pdf},
  journal = {JSTS:Journal of Semiconductor Technology and Science},
  keywords = {modeling,ReRAM},
  language = {en},
  number = {3}
}

@inproceedings{kim2015,
  title = {{{NVM}} Neuromorphic Core with 64k-Cell (256-by-256) Phase Change Memory Synaptic Array with on-Chip Neuron Circuits for Continuous in-Situ Learning},
  author = {Kim, S. and Ishii, M. and Lewis, S. and Perri, T. and BrightSky, M. and Kim, W. and Jordan, R. and Burr, G. W. and Sosa, N. and Ray, A. and Han, J.-P. and Miller, C. and Hosokawa, K. and Lam, C.},
  year = {2015},
  month = dec,
  pages = {17.1.1-17.1.4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2015.7409716},
  abstract = {We demonstrate a neuromorphic core with 64k-cell phase change memory (PCM) synaptic array (256 axons by 256 dendrites) with in-situ learning capability. 256 configurable on-chip neuron circuits perform leaky integrate and fire (LIF) and synaptic weight update based on spike-timing dependent plasticity (STDP). 2T-1R PCM unit cell design separates LIF and STDP learning paths, minimizing neuron circuit size. The circuit implementation of STDP learning algorithm along with 2T-1R structure enables both LIF and STDP learning to operate asynchronously and simultaneously within the array, avoiding additional complication and power consumption associated with timing schemes. We show hardware demonstration of in-situ learning with large representational capacity, enabled by large array size and analog synaptic weights of PCM cells.},
  file = {/Users/x0r/Zotero/storage/ZRWVBSHZ/Kim et al_2015_NVM neuromorphic core with 64k-cell (256-by-256) phase change memory synaptic.pdf},
  isbn = {978-1-4673-9894-7},
  keywords = {neuromorphic,PCM},
  language = {en}
}

@article{kim2015a,
  title = {Numerical Study of Read Scheme in One-Selector One-Resistor Crossbar Array},
  author = {Kim, Sungho and Kim, Hee-Dong and Choi, Sung-Jin},
  year = {2015},
  month = dec,
  volume = {114},
  pages = {80--86},
  issn = {00381101},
  doi = {10.1016/j.sse.2015.08.001},
  abstract = {A comprehensive numerical circuit analysis of read schemes of a one selector\textendash one resistance change memory (1S1R) crossbar array is carried out. Three schemes\textemdash the ground, V/2, and V/3 schemes\textemdash are compared with each other in terms of sensing margin and power consumption. Without the aid of a complex analytical approach or SPICE-based simulation, a simple numerical iteration method is developed to simulate entire current flows and node voltages within a crossbar array. Understanding such phenomena is essential in successfully evaluating the electrical specifications of selectors for suppressing intrinsic drawbacks of crossbar arrays, such as sneaky current paths and series line resistance problems. This method provides a quantitative tool for the accurate analysis of crossbar arrays and provides guidelines for developing an optimal read scheme, array configuration, and selector device specifications.},
  file = {/Users/x0r/Zotero/storage/Y2HKXMJX/Kim et al_2015_Numerical study of read scheme in one-selector one-resistor crossbar array.pdf},
  journal = {Solid-State Electronics},
  keywords = {neuromorphic},
  language = {en}
}

@article{kim2017,
  title = {Deep {{Neural Network Optimized}} to {{Resistive Memory}} with {{Nonlinear Current}}-{{Voltage Characteristics}}},
  author = {Kim, Hyungjun and Kim, Taesu and Kim, Jinseok and Kim, Jae-Joon},
  year = {2017},
  month = mar,
  abstract = {Artificial Neural Network computation relies on intensive vector-matrix multiplications. Recently, the emerging nonvolatile memory (NVM) crossbar array showed a feasibility of implementing such operations with high energy efficiency, thus there are many works on efficiently utilizing emerging NVM crossbar array as analog vector-matrix multiplier. However, its nonlinear I-V characteristics restrain critical design parameters, such as the read voltage and weight range, resulting in substantial accuracy loss. In this paper, instead of optimizing hardware parameters to a given neural network, we propose a methodology of reconstructing a neural network itself optimized to resistive memory crossbar arrays. To verify the validity of the proposed method, we simulated various neural network with MNIST and CIFAR-10 dataset using two different specific Resistive Random Access Memory (RRAM) model. Simulation results show that our proposed neural network produces significantly higher inference accuracies than conventional neural network when the synapse devices have nonlinear I-V characteristics.},
  archivePrefix = {arXiv},
  eprint = {1703.10642},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/FYMPVSYD/Kim et al_2017_Deep Neural Network Optimized to Resistive Memory with Nonlinear.pdf},
  journal = {arXiv:1703.10642 [cs]},
  keywords = {neuromorphic},
  language = {en},
  primaryClass = {cs}
}

@article{kim2019,
  title = {Variational {{Temporal Abstraction}}},
  author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  year = {2019},
  month = oct,
  abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpyimagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
  archivePrefix = {arXiv},
  eprint = {1910.00775},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/IWFL6T7U/Kim et al. - 2019 - Variational Temporal Abstraction.pdf},
  journal = {arXiv:1910.00775 [cs, stat]},
  keywords = {rl,vi},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kim2019a,
  title = {Simple {{Framework}} for {{Constructing Functional Spiking Recurrent Neural Networks}}},
  author = {Kim, Robert and Li, Yinghao and Sejnowski, Terrence J.},
  year = {2019},
  month = mar,
  pages = {579706},
  doi = {10.1101/579706},
  abstract = {{$<$}h3{$>$}Abstract{$<$}/h3{$>$} {$<$}p{$>$}Cortical microcircuits exhibit complex recurrent architectures that possess dynamically rich properties. The neurons that make up these microcircuits communicate mainly via discrete spikes, and it is not clear how spikes give rise to dynamics that can be used to perform computationally challenging tasks. In contrast, continuous models of rate-coding neurons can be trained to perform complex tasks. Here, we present a simple framework to construct biologically realistic spiking recurrent neural networks (RNNs) capable of learning a wide range of tasks. Our framework involves training a continuous-variable rate RNN with important biophysical constraints and transferring the learned dynamics and constraints to a spiking RNN in a one-to-one manner. We validate our framework on several cognitive task paradigms to replicate previously observed experimental results. We also demonstrate different ways to exploit the biological features of our models to elucidate neural mechanisms underlying cognitive functions.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  file = {/Users/x0r/Zotero/storage/TY8LPQJY/Kim et al_2019_Simple Framework for Constructing Functional Spiking Recurrent Neural Networks.pdf},
  journal = {bioRxiv},
  keywords = {RNN,SNN},
  language = {en}
}

@article{kim2019b,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  year = {2019},
  month = jan,
  abstract = {Neural Processes (NPs) (Garnelo et al., 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  archivePrefix = {arXiv},
  eprint = {1901.05761},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/QUUR4KAF/Kim et al_2019_Attentive Neural Processes.pdf},
  journal = {arXiv:1901.05761 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kirkpatrick2017,
  title = {Overcoming Catastrophic Forgetting in Neural Networks},
  author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and {Grabska-Barwinska}, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
  year = {2017},
  month = mar,
  volume = {114},
  pages = {3521--3526},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1611835114},
  file = {/Users/x0r/Zotero/storage/47V33SD8/Kirkpatrick et al_2017_Overcoming catastrophic forgetting in neural networks.pdf;/Users/x0r/Zotero/storage/A97YBWZE/Kirkpatrick et al_2017_Overcoming catastrophic forgetting in neural networks.pdf;/Users/x0r/Zotero/storage/KNE5PLNW/1612.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {dl},
  language = {en},
  number = {13}
}

@article{kirsch2019,
  title = {Improving {{Generalization}} in {{Meta Reinforcement Learning}} Using {{Learned Objectives}}},
  author = {Kirsch, Louis and {van Steenkiste}, Sjoerd and Schmidhuber, J{\"u}rgen},
  year = {2019},
  month = oct,
  abstract = {Biological evolution has distilled the experiences of many learners into the general learning algorithms of humans. Our novel meta-reinforcement learning algorithm MetaGenRL is inspired by this process. MetaGenRL distills the experiences of many complex agents to meta-learn a low-complexity neural objective function that affects how future individuals will learn. Unlike recent meta-RL algorithms, MetaGenRL can generalize to new environments that are entirely different from those used for meta-training. In some cases, it even outperforms humanengineered RL algorithms. MetaGenRL uses off-policy second-order gradients during meta-training that greatly increase its sample efficiency.},
  archivePrefix = {arXiv},
  eprint = {1910.04098},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NSQ6784M/Kirsch et al. - 2019 - Improving Generalization in Meta Reinforcement Lea.pdf},
  journal = {arXiv:1910.04098 [cs, stat]},
  keywords = {meta-rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{klos2019,
  title = {Dynamical Learning of Dynamics},
  author = {Klos, Christian and Kossio, Yaroslav Felipe Kalle and Goedeke, Sven and Gilra, Aditya and Memmesheimer, Raoul-Martin},
  year = {2019},
  month = nov,
  abstract = {The ability of humans and animals to quickly adapt to novel tasks is difficult to reconcile with the standard paradigm of learning by slow synaptic weight modification. Here we show that already static neural networks can learn to generate required dynamics by imitation. After appropriate weight pretraining, the networks quickly and dynamically adapt to learn new tasks and thereafter continue to achieve them without further teacher feedback. We explain this ability and illustrate it with a variety of target dynamics, ranging from oscillatory trajectories to driven and chaotic dynamical systems.},
  archivePrefix = {arXiv},
  eprint = {1902.02875},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VR466KDX/Klos et al. - 2019 - Dynamical learning of dynamics.pdf},
  journal = {arXiv:1902.02875 [q-bio]},
  language = {en},
  primaryClass = {q-bio}
}

@article{krakovna2018,
  title = {Penalizing Side Effects Using Stepwise Relative Reachability},
  author = {Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
  year = {2018},
  month = jun,
  abstract = {How can we design safe reinforcement learning agents that avoid unnecessary disruptions to their environment? We show that current approaches to penalizing side effects can introduce bad incentives, e.g. to prevent any irreversible changes in the environment, including the actions of other agents. To isolate the source of such undesirable incentives, we break down side effects penalties into two components: a baseline state and a measure of deviation from this baseline state. We argue that some of these incentives arise from the choice of baseline, and others arise from the choice of deviation measure. We introduce a new variant of the stepwise inaction baseline and a new deviation measure based on relative reachability of states. The combination of these design choices avoids the given undesirable incentives, while simpler baselines and the unreachability measure fail. We demonstrate this empirically by comparing different combinations of baseline and deviation measure choices on a set of gridworld experiments designed to illustrate possible bad incentives.},
  archivePrefix = {arXiv},
  eprint = {1806.01186},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AUJKZB3P/Krakovna et al_2018_Penalizing side effects using stepwise relative reachability.pdf},
  journal = {arXiv:1806.01186 [cs, stat]},
  keywords = {rl},
  primaryClass = {cs, stat}
}

@article{krebs2014,
  title = {Changes in Electrical Transport and Density of States of Phase Change Materials upon Resistance Drift},
  author = {Krebs, Daniel and Bachmann, Tobias and Jonnalagadda, Prasad and Dellmann, Laurent and Raoux, Simone},
  year = {2014},
  month = apr,
  volume = {16},
  pages = {043015},
  issn = {1367-2630},
  doi = {10.1088/1367-2630/16/4/043015},
  file = {/Users/x0r/Zotero/storage/F83R8MK5/Krebs et al_2014_Changes in electrical transport and density of states of phase change materials.pdf},
  journal = {New Journal of Physics},
  keywords = {drift,modeling},
  language = {en},
  number = {4}
}

@article{kriegman2020,
  title = {A Scalable Pipeline for Designing Reconfigurable Organisms},
  author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
  year = {2020},
  month = jan,
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1910837117},
  abstract = {Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.},
  copyright = {Copyright \textcopyright{} 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
  file = {/Users/x0r/Zotero/storage/76V675XM/Kriegman et al. - 2020 - A scalable pipeline for designing reconfigurable o.pdf;/Users/x0r/Zotero/storage/7DIUQSY3/1910837117.html},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en}
}

@article{kriegman2020a,
  title = {A Scalable Pipeline for Designing Reconfigurable Organisms},
  author = {Kriegman, Sam and Blackiston, Douglas and Levin, Michael and Bongard, Josh},
  year = {2020},
  month = jan,
  volume = {117},
  pages = {1853--1859},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1910837117},
  abstract = {Living systems are more robust, diverse, complex, and supportive of human life than any technology yet created. However, our ability to create novel lifeforms is currently limited to varying existing organisms or bioengineering organoids in vitro. Here we show a scalable pipeline for creating functional novel lifeforms: AI methods automatically design diverse candidate lifeforms in silico to perform some desired function, and transferable designs are then created using a cell-based construction toolkit to realize living systems with the predicted behaviors. Although some steps in this pipeline still require manual intervention, complete automation in future would pave the way to designing and deploying unique, bespoke living systems for a wide range of functions.},
  file = {/Users/x0r/Zotero/storage/974LXTAV/Kriegman et al. - 2020 - A scalable pipeline for designing reconfigurable o.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {To read},
  language = {en},
  number = {4}
}

@article{krizhevsky2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {/Users/x0r/Zotero/storage/TWUNSKTM/Krizhevsky et al_2017_ImageNet classification with deep convolutional neural networks.pdf},
  journal = {Communications of the ACM},
  keywords = {dl},
  language = {en},
  number = {6}
}

@article{krotov2016,
  title = {Dense {{Associative Memory}} for {{Pattern Recognition}}},
  author = {Krotov, Dmitry and Hopfield, John J.},
  year = {2016},
  month = jun,
  abstract = {A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions \textendash{} the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set.},
  archivePrefix = {arXiv},
  eprint = {1606.01164},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/6863KLNA/Krotov_Hopfield_2016_Dense Associative Memory for Pattern Recognition.pdf},
  journal = {arXiv:1606.01164 [cond-mat, q-bio, stat]},
  language = {en},
  primaryClass = {cond-mat, q-bio, stat}
}

@article{kulkarni2016,
  title = {Hierarchical {{Deep Reinforcement Learning}}: {{Integrating Temporal Abstraction}} and {{Intrinsic Motivation}}},
  shorttitle = {Hierarchical {{Deep Reinforcement Learning}}},
  author = {Kulkarni, Tejas D. and Narasimhan, Karthik R. and Saeedi, Ardavan and Tenenbaum, Joshua B.},
  year = {2016},
  month = apr,
  abstract = {Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. The primary difficulty arises due to insufficient exploration, resulting in an agent being unable to learn robust value functions. Intrinsically motivated agents can explore new behavior for its own sake rather than to directly solve problems. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical value functions, operating at different temporal scales, with intrinsically motivated deep reinforcement learning. A top-level value function learns a policy over intrinsic goals, and a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse, delayed feedback: (1) a complex discrete stochastic decision process, and (2) the classic ATARI game `Montezuma's Revenge'.},
  archivePrefix = {arXiv},
  eprint = {1604.06057},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/X75C8WCG/Kulkarni et al_2016_Hierarchical Deep Reinforcement Learning.pdf},
  journal = {arXiv:1604.06057 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kumaran2016,
  title = {What {{Learning Systems}} Do {{Intelligent Agents Need}}? {{Complementary Learning Systems Theory Updated}}},
  shorttitle = {What {{Learning Systems}} Do {{Intelligent Agents Need}}?},
  author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
  year = {2016},
  month = jul,
  volume = {20},
  pages = {512--534},
  issn = {13646613},
  doi = {10.1016/j.tics.2016.05.004},
  file = {/Users/x0r/Zotero/storage/QT66A28P/Kumaran et al_2016_What Learning Systems do Intelligent Agents Need.pdf},
  journal = {Trends in Cognitive Sciences},
  keywords = {AGI},
  language = {en},
  number = {7}
}

@article{kunin2020,
  title = {Two {{Routes}} to {{Scalable Credit Assignment}} without {{Weight Symmetry}}},
  author = {Kunin, Daniel and Nayebi, Aran and {Sagastuy-Brena}, Javier and Ganguli, Surya and Bloom, Jon and Yamins, Daniel L. K.},
  year = {2020},
  month = feb,
  abstract = {The neural plausibility of backpropagation has long been disputed, primarily for its use of nonlocal weight transport \textemdash{} the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible ``weight estimation'' process, showing that these rules match state-ofthe-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.},
  archivePrefix = {arXiv},
  eprint = {2003.01513},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LT4R7YDQ/Kunin et al. - 2020 - Two Routes to Scalable Credit Assignment without W.pdf},
  journal = {arXiv:2003.01513 [cs, q-bio, stat]},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{kunin2020a,
  title = {Two {{Routes}} to {{Scalable Credit Assignment}} without {{Weight Symmetry}}},
  author = {Kunin, Daniel and Nayebi, Aran and {Sagastuy-Brena}, Javier and Ganguli, Surya and Bloom, Jon and Yamins, Daniel L. K.},
  year = {2020},
  month = feb,
  abstract = {The neural plausibility of backpropagation has long been disputed, primarily for its use of nonlocal weight transport \textemdash{} the biologically dubious requirement that one neuron instantaneously measure the synaptic weights of another. Until recently, attempts to create local learning rules that avoid weight transport have typically failed in the large-scale learning scenarios where backpropagation shines, e.g. ImageNet categorization with deep convolutional networks. Here, we investigate a recently proposed local learning rule that yields competitive performance with backpropagation and find that it is highly sensitive to metaparameter choices, requiring laborious tuning that does not transfer across network architecture. Our analysis indicates the underlying mathematical reason for this instability, allowing us to identify a more robust local learning rule that better transfers without metaparameter tuning. Nonetheless, we find a performance and stability gap between this local rule and backpropagation that widens with increasing model depth. We then investigate several non-local learning rules that relax the need for instantaneous weight transport into a more biologically-plausible ``weight estimation'' process, showing that these rules match state-ofthe-art performance on deep networks and operate effectively in the presence of noisy updates. Taken together, our results suggest two routes towards the discovery of neural implementations for credit assignment without weight symmetry: further improvement of local rules so that they perform consistently across architectures and the identification of biological implementations for non-local learning mechanisms.},
  archivePrefix = {arXiv},
  eprint = {2003.01513},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/SR4P94BS/Kunin et al. - 2020 - Two Routes to Scalable Credit Assignment without W.pdf},
  journal = {arXiv:2003.01513 [cs, q-bio, stat]},
  keywords = {To read},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{Kuzum_etal12,
  ids = {kuzum2012},
  title = {Nanoelectronic Programmable Synapses Based on Phase Change Materials for Brain-Inspired Computing},
  author = {Kuzum, D. and Jeyasingh, R.-G.-D. and Lee, B. and Wong, H.-S. Philip},
  year = {2012},
  volume = {12},
  pages = {2179--2186},
  doi = {10.1021/nl201040y},
  file = {/Users/x0r/Zotero/storage/7WIHYL7L/Kuzum et al_2012_Nanoelectronic Programmable Synapses Based on Phase Change Materials for.pdf;/Users/x0r/Zotero/storage/ZLYGB988/Kuzum et al_2012_Nanoelectronic Programmable Synapses Based on Phase Change Materials for.pdf},
  journal = {Nano Letters},
  keywords = {neuromorphic,PCM},
  number = {5}
}

@article{Kuzum_etal13,
  ids = {kuzum2013},
  title = {Synaptic Electronics: Materials, Devices and Applications},
  author = {Kuzum, D. and Yu, S. and Wong, H.-S. Philip},
  year = {2013},
  volume = {24},
  pages = {382001},
  doi = {10.1088/0957-4484/24/38/382001},
  abstract = {In this paper, the recent progress of synaptic electronics is reviewed. The basics of biological synaptic plasticity and learning are described. The material properties and electrical switching characteristics of a variety of synaptic devices are discussed, with a focus on the use of synaptic devices for neuromorphic or brain-inspired computing. Performance metrics desirable for large-scale implementations of synaptic devices are illustrated. A review of recent work on targeted computing applications with synaptic devices is presented.},
  file = {/Users/x0r/Zotero/storage/4RBBMYIH/Kuzum et al_2013_Synaptic electronics.pdf},
  journal = {Nanotechnology},
  keywords = {neuromorphic},
  number = {38}
}

@inproceedings{kuzum2011,
  title = {Energy Efficient Programming of Nanoelectronic Synaptic Devices for Large-Scale Implementation of Associative and Temporal Sequence Learning},
  author = {Kuzum, Duygu and Jeyasingh, Rakesh G. D. and Wong, H.-S. Philip},
  year = {2011},
  month = dec,
  pages = {30.3.1-30.3.4},
  publisher = {{IEEE}},
  doi = {10.1109/IEDM.2011.6131643},
  abstract = {A nanoscale, two-terminal device emulating plasticity and energy efficiency of biological synapses is a critical element for realizing brain-inspired computational systems and real-time brain simulators. In this work, we explore the use of phase change materials (PCM), widely used for memory applications, to build electronic synapses which implement synaptic plasticity with picojoule level energy consumption. Gradual switching characteristics and different spike schemes are discussed from implementation of synaptic plasticity and energy consumption perspectives. Our simulations demonstrate that a recurrent network of PCM synapses in a crossbar array can achieve brain\- like associative and temporal sequence learning. Asymmetric plasticity is shown to transform temporal information into spatial information for sequence learning. Symmetric plasticity enables the storage and recall of certain patterns associatively by acting as a coincidence detector for neuronal activity.},
  file = {/Users/x0r/Zotero/storage/PLC5J3BP/Kuzum et al_2011_Energy efficient programming of nanoelectronic synaptic devices for large-scale.pdf},
  isbn = {978-1-4577-0505-2 978-1-4577-0506-9 978-1-4577-0504-5},
  keywords = {neuromorphic},
  language = {en}
}

@article{kuzum2012a,
  title = {Low-{{Energy Robust Neuromorphic Computation Using Synaptic Devices}}},
  author = {Kuzum, Duygu and Jeyasingh, Rakesh Gnana David and Yu, Shimeng and Wong, H.-S. Philip},
  year = {2012},
  month = dec,
  volume = {59},
  pages = {3489--3494},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2012.2217146},
  abstract = {Brain-inspired computing is an emerging field, which aims to reach brainlike performance in real-time processing of sensory data. The challenges that need to be addressed toward reaching such a computational system include building a compact massively parallel architecture with scalable interconnection devices, ultralow-power consumption, and robust neuromorphic computational schemes for implementation of learning in hardware. In this paper, we discuss programming strategies, material characteristics, and spike schemes, which enable implementation of symmetric and asymmetric synaptic plasticity with devices using phase-change materials. We demonstrate that energy consumption can be optimized by tuning the device operation regime and the spike scheme. Our simulations illustrate that a crossbar array consisting of synaptic devices and neurons can achieve hippocampus-like associative learning with symmetric synapses and sequence learning with asymmetric synapses. Pattern completion for patterns with 50\% missing elements is achieved via associative learning with symmetric plasticity. Robustness of learning against input noise, variation in sensory data, and device resistance variation are investigated through simulations.},
  file = {/Users/x0r/Zotero/storage/33GPQX2S/Kuzum et al_2012_Low-Energy Robust Neuromorphic Computation Using Synaptic Devices.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {neuromorphic},
  language = {en},
  number = {12}
}

@inproceedings{kwong2008,
  title = {Verilog-{{A}} Model for Phase Change Memory Simulation},
  author = {Kwong, K. C. and {Lin Li} and {Jin He} and {Mansun Chan}},
  year = {2008},
  month = oct,
  pages = {492--495},
  publisher = {{IEEE}},
  doi = {10.1109/ICSICT.2008.4734588},
  abstract = {A fully-customized phase change memory (PCM) model for circuit simulation has been developed and implemented in Verilog-A platform. A temperature sensing circuit is used to track the set and reset conditions of the PCM element. The current-voltage of PCM cell at the two different states and the change of states during set and reset can be correctly simulated by the model. The model has also been calibrated by comparing with experimental data reported in the literature.},
  file = {/Users/x0r/Zotero/storage/ZC3XNV9Q/Kwong et al_2008_Verilog-A model for phase change memory simulation.pdf},
  isbn = {978-1-4244-2185-5},
  keywords = {modeling,PCM},
  language = {en}
}

@article{lai2015,
  title = {Giraffe: {{Using Deep Reinforcement Learning}} to {{Play Chess}}},
  shorttitle = {Giraffe},
  author = {Lai, Matthew},
  year = {2015},
  month = sep,
  abstract = {This report presents Giraffe, a chess engine that uses self-play to discover all its domain-specific knowledge, with minimal hand-crafted knowledge given by the programmer. Unlike previous attempts using machine learning only to perform parameter-tuning on hand-crafted evaluation functions, Giraffe's learning system also performs automatic feature extraction and pattern recognition. The trained evaluation function performs comparably to the evaluation functions of state-of-the-art chess engines - all of which containing thousands of lines of carefully hand-crafted pattern recognizers, tuned over many years by both computer chess experts and human chess masters. Giraffe is the most successful attempt thus far at using end-to-end machine learning to play chess.},
  archivePrefix = {arXiv},
  eprint = {1509.01549},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/K9F43LZ7/Lai_2015_Giraffe.pdf},
  journal = {arXiv:1509.01549 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{Lake_etal15,
  ids = {lake2015},
  title = {Human-Level Concept Learning through Probabilistic Program Induction},
  author = {Lake, Brenden M and Salakhutdinov, Ruslan and Tenenbaum, Joshua B},
  year = {2015},
  volume = {350},
  pages = {1332--1338},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/F8YPSCGQ/Lake et al_2015_Human-level concept learning through probabilistic program induction.pdf},
  journal = {Science},
  keywords = {dl},
  number = {6266}
}

@article{lake2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017},
  volume = {40},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X16001837},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  file = {/Users/x0r/Zotero/storage/YIW73MI8/Lake et al_2017_Building machines that learn and think like people.pdf},
  journal = {Behavioral and Brain Sciences},
  keywords = {AGI},
  language = {en}
}

@article{lample2018,
  title = {Phrase-{{Based}} \& {{Neural Unsupervised Machine Translation}}},
  author = {Lample, Guillaume and Ott, Myle and Conneau, Alexis and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2018},
  month = apr,
  abstract = {Machine translation systems achieve near human-level performance on some languages, yet their effectiveness strongly relies on the availability of large amounts of parallel sentences, which hinders their applicability to the majority of language pairs. This work investigates how to learn to translate when having access to only large monolingual corpora in each language. We propose two model variants, a neural and a phrase-based model. Both versions leverage a careful initialization of the parameters, the denoising effect of language models and automatic generation of parallel data by iterative back-translation. These models are significantly better than methods from the literature, while being simpler and having fewer hyper-parameters. On the widely used WMT'14 English-French and WMT'16 German-English benchmarks, our models respectively obtain 28.1 and 25.2 BLEU points without using a single parallel sentence, outperforming the state of the art by more than 11 BLEU points. On low-resource languages like English-Urdu and English-Romanian, our methods achieve even better results than semi-supervised and supervised approaches leveraging the paucity of available bitexts. Our code for NMT and PBSMT is publicly available.},
  archivePrefix = {arXiv},
  eprint = {1804.07755},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Y6SHH3L4/Lample et al_2018_Phrase-Based & Neural Unsupervised Machine Translation.pdf},
  journal = {arXiv:1804.07755 [cs]},
  keywords = {NMT},
  language = {en},
  primaryClass = {cs}
}

@article{lansdell2020,
  title = {{{LEARNING TO SOLVE THE CREDIT ASSIGNMENT PROBLEM}}},
  author = {Lansdell, Benjamin James and Kording, Konrad Paul and Prakash, Prashanth Ravi},
  year = {2020},
  pages = {19},
  abstract = {Backpropagation is driving today's artificial neural networks (ANNs). However, despite extensive research, it remains unclear if the brain implements this algorithm. Among neuroscientists, reinforcement learning (RL) algorithms are often seen as a realistic alternative: neurons can randomly introduce change, and use unspecific feedback signals to observe their effect on the cost and thus approximate their gradient. However, the convergence rate of such learning scales poorly with the number of involved neurons. Here we propose a hybrid learning approach. Each neuron uses an RL-type strategy to learn how to approximate the gradients that backpropagation would provide. We provide proof that our approach converges to the true gradient for certain classes of networks. In both feedforward and convolutional networks, we empirically show that our approach learns to approximate the gradient, and can match or the performance of exact gradient-based learning. Learning feedback weights provides a biologically plausible mechanism of achieving good performance, without the need for precise, pre-specified learning rules.},
  file = {/Users/x0r/Zotero/storage/7N5VZBC2/Lansdell et al. - 2020 - LEARNING TO SOLVE THE CREDIT ASSIGNMENT PROBLEM.pdf},
  keywords = {SNN},
  language = {en}
}

@article{larcher2014,
  title = {Progresses in {{Modeling HfOx RRAM Operations}} and {{Variability}}},
  author = {Larcher, L. and Pirrotta, O. and Puglisi, F. M. and Padovani, A. and Pavan, P. and Vandelli, L.},
  year = {2014},
  month = aug,
  volume = {64},
  pages = {49--60},
  issn = {1938-6737, 1938-5862},
  doi = {10.1149/06414.0049ecst},
  file = {/Users/x0r/Zotero/storage/IFPQDSXD/Larcher et al_2014_Progresses in Modeling HfOx RRAM Operations and Variability.pdf},
  journal = {ECS Transactions},
  keywords = {modeling,ReRAM},
  language = {en},
  number = {14}
}

@article{larentis2012,
  title = {Resistive {{Switching}} by {{Voltage}}-{{Driven Ion Migration}} in {{Bipolar RRAM}}\textemdash{{Part II}}: {{Modeling}}},
  shorttitle = {Resistive {{Switching}} by {{Voltage}}-{{Driven Ion Migration}} in {{Bipolar RRAM}}\textemdash{{Part II}}},
  author = {Larentis, Stefano and Nardi, Federico and Balatti, Simone and Gilmer, David C. and Ielmini, Daniele},
  year = {2012},
  month = sep,
  volume = {59},
  pages = {2468--2475},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2012.2202320},
  abstract = {Resistive-switching memory (RRAM) based on transition metal oxides is a potential candidate for replacing Flash and dynamic random access memory in future generation nodes. Although very promising from the standpoints of scalability and technology, RRAM still has severe drawbacks in terms of understanding and modeling of the resistive-switching mechanism. This paper addresses the modeling of resistive switching in bipolar metal-oxide RRAMs. Reset and set processes are described in terms of voltage-driven ion migration within a conductive filament generated by electroforming. Ion migration is modeled by drift\textendash diffusion equations with Arrhenius-activated diffusivity and mobility. The local temperature and field are derived from the self-consistent solution of carrier and heat conduction equations in a 3-D axis-symmetric geometry. The model accounts for set\textendash reset characteristics, correctly describing the abrupt set and gradual reset transitions and allowing scaling projections for metal-oxide RRAM.},
  file = {/Users/x0r/Zotero/storage/K8XFQQFF/Larentis et al_2012_Resistive Switching by Voltage-Driven Ion Migration in Bipolar RRAM—Part II.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {ReRAM},
  language = {en},
  number = {9}
}

@article{larkum1999,
  title = {A New Cellular Mechanism for Coupling Inputs Arriving at Different Cortical Layers},
  author = {Larkum, Matthew E. and Zhu, J. Julius and Sakmann, Bert},
  year = {1999},
  month = mar,
  volume = {398},
  pages = {338--341},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/18686},
  file = {/Users/x0r/Zotero/storage/VYS224NT/Larkum et al. - 1999 - A new cellular mechanism for coupling inputs arriv.pdf},
  journal = {Nature},
  language = {en},
  number = {6725}
}

@article{lavin2015,
  title = {Fast {{Algorithms}} for {{Convolutional Neural Networks}}},
  author = {Lavin, Andrew and Gray, Scott},
  year = {2015},
  month = nov,
  abstract = {Deep convolutional neural networks take GPU days of compute time to train on large data sets. Pedestrian detection for self driving cars requires very low latency. Image recognition for mobile phones is constrained by limited processing resources. The success of convolutional neural networks in these situations is limited by how fast we can compute them. Conventional FFT based convolution is fast for large filters, but state of the art convolutional neural networks use small, 3x3 filters. We introduce a new class of fast algorithms for convolutional neural networks using Winograd's minimal filtering algorithms. The algorithms compute minimal complexity convolution over small tiles, which makes them fast with small filters and small batch sizes. We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64.},
  archivePrefix = {arXiv},
  eprint = {1509.09308},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/HNUJCWJY/Lavin and Gray - 2015 - Fast Algorithms for Convolutional Neural Networks.pdf;/Users/x0r/Zotero/storage/8Y88JDTY/1509.html},
  journal = {arXiv:1509.09308 [cs]},
  primaryClass = {cs}
}

@article{LeCun_etal15,
  ids = {lecun2015},
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  volume = {521},
  pages = {436--444},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/8BMZ259R/LeCun et al_2015_Deep learning.pdf},
  journal = {Nature},
  keywords = {dl},
  number = {7553}
}

@article{lee2010,
  title = {{{RETHINKING DIGITAL DESIGN}}: {{WHY DESIGN MUST CHANGE}}},
  author = {Lee, Benjamin and Solomatnikov, Alex and Firoozshahian, Amin},
  year = {2010},
  pages = {16},
  file = {/Users/x0r/Zotero/storage/8I5CK92U/Lee et al_2010_RETHINKING DIGITAL DESIGN.pdf},
  journal = {IEEE MICRO},
  keywords = {VLSI},
  language = {en}
}

@inproceedings{lee2013,
  title = {A Two-Step Set Operation for Highly Uniform Resistive Swtiching {{ReRAM}} by Controllable Filament},
  author = {Lee, Sangheon and Lee, Daeseok and Woo, Jiyong and Cha, Euijun and Hwang, Hyunsang},
  year = {2013},
  month = sep,
  pages = {178--181},
  publisher = {{IEEE}},
  doi = {10.1109/ESSDERC.2013.6818848},
  abstract = {For the first time, we demonstrate filament controllability by a two-step set operation based on triple-layer ReRAM. Hence, we report a highly reliable memory switching of a triple-layer structure based on resistive switching memory devices by inserting an additional binary metal oxide layer. The inserted oxide layer in the triple-layer can act as a filament controlling factor with the two-step set operation for reliable resistive switching. Compared with the bi-layer structure, the three layers of the two-step set operation showed excellent uniform Vset, Vreset, high-resistance state (RHRS), and lowresistance state (RLRS). Furthermore, the devices exhibited excellent memory performance such as endurance, resistance on/off ratio, and reliable data retention of up to 104 s at 180 \textdegree C.},
  file = {/Users/x0r/Zotero/storage/5DBAHMD5/Lee et al_2013_A two-step set operation for highly uniform resistive swtiching ReRAM by.pdf},
  isbn = {978-1-4799-0649-9},
  keywords = {ReRAM},
  language = {en}
}

@article{lee2014,
  title = {Neural {{Computations Underlying Arbitration}} between {{Model}}-{{Based}} and {{Model}}-Free {{Learning}}},
  author = {Lee, Sang Wan and Shimojo, Shinsuke and O'Doherty, John P.},
  year = {2014},
  month = feb,
  volume = {81},
  pages = {687--699},
  issn = {08966273},
  doi = {10.1016/j.neuron.2013.11.028},
  abstract = {There is accumulating neural evidence to support the existence of two distinct systems for guiding action selection, a deliberative ``model-based'' and a reflexive ``model-free'' system. However, little is known about how the brain determines which of these systems controls behavior at one moment in time. We provide evidence for an arbitration mechanism that allocates the degree of control over behavior by model-based and model-free systems as a function of the reliability of their respective predictions. We show that the inferior lateral prefrontal and frontopolar cortex encode both reliability signals and the output of a comparison between those signals, implicating these regions in the arbitration process. Moreover, connectivity between these regions and model-free valuation areas is negatively modulated by the degree of model-based control in the arbitrator, suggesting that arbitration may work through modulation of the model-free valuation system when the arbitrator deems that the model-based system should drive behavior.},
  file = {/Users/x0r/Zotero/storage/QX2BW3GV/Lee et al_2014_Neural Computations Underlying Arbitration between Model-Based and Model-free.pdf},
  journal = {Neuron},
  keywords = {rl},
  language = {en},
  number = {3}
}

@article{lee2014a,
  title = {Distribution of Nanoscale Nuclei in the Amorphous Dome of a Phase Change Random Access Memory},
  author = {Lee, Bong-Sub and Darmawikarta, Kristof and Raoux, Simone and Shih, Yen-Hao and Zhu, Yu and Bishop, Stephen G. and Abelson, John R.},
  year = {2014},
  month = feb,
  volume = {104},
  pages = {071907},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.4865586},
  file = {/Users/x0r/Zotero/storage/773WNRFT/Lee et al_2014_Distribution of nanoscale nuclei in the amorphous dome of a phase change random.pdf},
  journal = {Applied Physics Letters},
  keywords = {PCM},
  language = {en},
  number = {7}
}

@article{lee2014b,
  title = {Difference {{Target Propagation}}},
  author = {Lee, Dong-Hyun and Zhang, Saizheng and Fischer, Asja and Bengio, Yoshua},
  year = {2014},
  month = dec,
  abstract = {Back-propagation has been the workhorse of recent successes of deep learning
but it relies on infinitesimal effects (partial derivatives) in order to
perform credit assignment. This could become a serious issue as one considers
deeper and more non-linear functions, e.g., consider the extreme case of
nonlinearity where the relation between parameters and cost is actually
discrete. Inspired by the biological implausibility of back-propagation, a few
approaches have been proposed in the past that could play a similar credit
assignment role. In this spirit, we explore a novel approach to credit
assignment in deep networks that we call target propagation. The main idea is
to compute targets rather than gradients, at each layer. Like gradients, they
are propagated backwards. In a way that is related but different from
previously proposed proxies for back-propagation which rely on a backwards
network with symmetric weights, target propagation relies on auto-encoders at
each layer. Unlike back-propagation, it can be applied even when units exchange
stochastic bits rather than real numbers. We show that a linear correction for
the imperfectness of the auto-encoders, called difference target propagation,
is very effective to make target propagation actually work, leading to results
comparable to back-propagation for deep networks with discrete and continuous
units and denoising auto-encoders and achieving state of the art for stochastic
networks.},
  file = {/Users/x0r/switchdrive/zotero/Lee et al_2014_Difference Target Propagation.pdf;/Users/x0r/Zotero/storage/T8I44NS6/1412.html},
  language = {en}
}

@article{lee2016,
  title = {Training {{Deep Spiking Neural Networks Using Backpropagation}}},
  author = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
  year = {2016},
  volume = {10},
  issn = {1662-453X},
  doi = {10.3389/fnins.2016.00508},
  abstract = {Deep spiking neural networks (SNNs) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous SNN, and also achieves a higher accuracy than a conventional convolutional neural network (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their event-driven operation, deep SNNs (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.},
  file = {/Users/x0r/Zotero/storage/85229DGH/Lee et al_2016_Training Deep Spiking Neural Networks Using Backpropagation.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {backprop,neuromorphic,SNN},
  language = {English}
}

@article{legallo2016,
  title = {Evidence for Thermally Assisted Threshold Switching Behavior in Nanoscale Phase-Change Memory Cells},
  author = {Le Gallo, Manuel and Athmanathan, Aravinthan and Krebs, Daniel and Sebastian, Abu},
  year = {2016},
  month = jan,
  volume = {119},
  pages = {025704},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.4938532},
  file = {/Users/x0r/Zotero/storage/J4BXAKY6/Le Gallo et al_2016_Evidence for thermally assisted threshold switching behavior in nanoscale.pdf},
  journal = {Journal of Applied Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {2}
}

@inproceedings{legallo2016a,
  title = {The Complete Time/Temperature Dependence of {{I}}-{{V}} Drift in {{PCM}} Devices},
  author = {Le Gallo, Manuel and Sebastian, Abu and Krebs, Daniel and Stanisavljevic, Milos and Eleftheriou, Evangelos},
  year = {2016},
  month = apr,
  pages = {MY-1-1-MY-1-6},
  publisher = {{IEEE}},
  doi = {10.1109/IRPS.2016.7574617},
  abstract = {Phase-change memory (PCM) devices are expected to play a key role in future computing systems as both memory and computing elements. Hence, a comprehensive understanding of the change in the current/voltage (I\textendash V) characteristics of these devices with time and temperature is of considerable importance. Here, we present a unified drift model able to predict the I\textendash V characteristics at any instance in time and at any temperature. The model was validated on large sets of experimental data for an extensive range of time (10 orders of magnitude) and temperatures (180 - 400 K), different phase-change materials and a collection of 4k cells from a PCM chip.},
  file = {/Users/x0r/Zotero/storage/LDD6FWZK/Le Gallo et al_2016_The complete time-temperature dependence of I-V drift in PCM devices.pdf},
  isbn = {978-1-4673-9137-5},
  keywords = {drift,PCM},
  language = {en}
}

@article{legallo2018,
  title = {Mixed-Precision in-Memory Computing},
  author = {Le Gallo, Manuel and Sebastian, Abu and Mathis, Roland and Manica, Matteo and Giefers, Heiner and Tuma, Tomas and Bekas, Costas and Curioni, Alessandro and Eleftheriou, Evangelos},
  year = {2018},
  month = apr,
  volume = {1},
  pages = {246--253},
  issn = {2520-1131},
  doi = {10.1038/s41928-018-0054-8},
  file = {/Users/x0r/Zotero/storage/HYVMG8SR/Le Gallo et al_2018_Mixed-precision in-memory computing.pdf;/Users/x0r/Zotero/storage/ICM4JCDV/Le Gallo et al_2018_Mixed-precision in-memory computing.pdf;/Users/x0r/Zotero/storage/PG73XG9E/Le Gallo et al_2018_Mixed-precision in-memory computing.pdf},
  journal = {Nature Electronics},
  keywords = {PCM},
  language = {en},
  number = {4}
}

@article{legallo2018a,
  title = {Collective {{Structural Relaxation}} in {{Phase}}-{{Change Memory Devices}}},
  author = {Le Gallo, Manuel and Krebs, Daniel and Zipoli, Federico and Salinga, Martin and Sebastian, Abu},
  year = {2018},
  month = jul,
  pages = {1700627},
  issn = {2199160X},
  doi = {10.1002/aelm.201700627},
  file = {/Users/x0r/Zotero/storage/ISVYAKQ8/Le Gallo et al_2018_Collective Structural Relaxation in Phase-Change Memory Devices.pdf},
  journal = {Advanced Electronic Materials},
  language = {en}
}

@article{legg2007,
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  shorttitle = {Universal {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  year = {2007},
  month = dec,
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  archivePrefix = {arXiv},
  eprint = {0712.3329},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LVWL626G/Legg_Hutter_2007_Universal Intelligence.pdf},
  journal = {arXiv:0712.3329 [cs]},
  keywords = {AGI,To read},
  language = {en},
  primaryClass = {cs}
}

@incollection{lehman2011,
  title = {Novelty {{Search}} and the {{Problem}} with {{Objectives}}},
  booktitle = {Genetic {{Programming Theory}} and {{Practice IX}}},
  author = {Lehman, Joel and Stanley, Kenneth O.},
  editor = {Riolo, Rick and Vladislavleva, Ekaterina and Moore, Jason H.},
  year = {2011},
  pages = {37--56},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4614-1770-5_3},
  abstract = {By synthesizing a growing body of work in search processes that are not driven by explicit objectives, this paper advances the hypothesis that there is a fundamental problem with the dominant paradigm of objective-based search in evolutionary computation and genetic programming: Most ambitious objectives do not illuminate a path to themselves. That is, the gradient of improvement induced by ambitious objectives tends to lead not to the objective itself but instead to dead-end local optima. Indirectly supporting this hypothesis, great discoveries often are not the result of objective-driven search. For example, the major inspiration for both evolutionary computation and genetic programming, natural evolution, innovates through an open-ended process that lacks a final objective. Similarly, large-scale cultural evolutionary processes, such as the evolution of technology, mathematics, and art, lack a unified fixed goal. In addition, direct evidence for this hypothesis is presented from a recently-introduced search algorithm called novelty search. Though ignorant of the ultimate objective of search, in many instances novelty search has counter-intuitively outperformed searching directly for the objective, including a wide variety of randomly-generated problems introduced in an experiment in this chapter. Thus a new understanding is beginning to emerge that suggests that searching for a fixed objective, which is the reigning paradigm in evolutionary computation and even machine learning as a whole, may ultimately limit what can be achieved. Yet the liberating implication of this hypothesis argued in this paper is that by embracing search processes that are not driven by explicit objectives, the breadth and depth of what is reachable through evolutionary methods such as genetic programming may be greatly expanded.},
  file = {/Users/x0r/Zotero/storage/4KJAG55F/Lehman and Stanley - 2011 - Novelty Search and the Problem with Objectives.pdf},
  isbn = {978-1-4614-1769-9 978-1-4614-1770-5},
  language = {en}
}

@article{lehman2018,
  title = {The {{Surprising Creativity}} of {{Digital Evolution}}: {{A Collection}} of {{Anecdotes}} from the {{Evolutionary Computation}} and {{Artificial Life Research Communities}}},
  shorttitle = {The {{Surprising Creativity}} of {{Digital Evolution}}},
  author = {Lehman, Joel and Clune, Jeff and Misevic, Dusan and Adami, Christoph and Beaulieu, Julie and Bentley, Peter J. and Bernard, Samuel and Beslon, Guillaume and Bryson, David M. and Chrabaszcz, Patryk and Cheney, Nick and Cully, Antoine and Doncieux, Stephane and Dyer, Fred C. and Ellefsen, Kai Olav and Feldt, Robert and Fischer, Stephan and Forrest, Stephanie and Fr{\'e}noy, Antoine and Gagn{\'e}, Christian and Goff, Leni Le and Grabowski, Laura M. and Hodjat, Babak and Hutter, Frank and Keller, Laurent and Knibbe, Carole and Krcah, Peter and Lenski, Richard E. and Lipson, Hod and MacCurdy, Robert and Maestre, Carlos and Miikkulainen, Risto and Mitri, Sara and Moriarty, David E. and Mouret, Jean-Baptiste and Nguyen, Anh and Ofria, Charles and Parizeau, Marc and Parsons, David and Pennock, Robert T. and Punch, William F. and Ray, Thomas S. and Schoenauer, Marc and Shulte, Eric and Sims, Karl and Stanley, Kenneth O. and Taddei, Fran{\c c}ois and Tarapore, Danesh and Thibault, Simon and Weimer, Westley and Watson, Richard and Yosinksi, Jason},
  year = {2018},
  month = mar,
  abstract = {Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This paper is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.},
  archivePrefix = {arXiv},
  eprint = {1803.03453},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/MGABN2XI/Lehman et al_2018_The Surprising Creativity of Digital Evolution.pdf},
  journal = {arXiv:1803.03453 [cs]},
  keywords = {AGI,evolution},
  language = {en},
  primaryClass = {cs}
}

@article{lehmann2019,
  title = {One-Shot Learning and Behavioral Eligibility Traces in Sequential Decision Making},
  author = {Lehmann, Marco and Xu, He and Liakoni, Vasiliki and Herzog, Michael and Gerstner, Wulfram and Preuschoff, Kerstin},
  year = {2019},
  month = nov,
  volume = {8},
  pages = {e47463},
  issn = {2050-084X},
  doi = {10.7554/eLife.47463},
  abstract = {In many daily tasks we make multiple decisions before reaching a goal. In order to learn such sequences of decisions, a mechanism to link earlier actions to later reward is necessary. Reinforcement learning theory suggests two classes of algorithms solving this credit assignment problem: In classic temporal-difference learning, earlier actions receive reward information only after multiple repetitions of the task, whereas models with eligibility traces reinforce entire sequences of actions from a single experience (one-shot). Here we asked whether humans use eligibility traces. We developed a novel paradigm to directly observe which actions and states along a multi-step sequence are reinforced after a single reward. By focusing our analysis on those states for which RL with and without eligibility trace make qualitatively distinct predictions, we find direct behavioral (choice probability) and physiological (pupil dilation) signatures of reinforcement learning with eligibility trace across multiple sensory modalities.},
  archivePrefix = {arXiv},
  eprint = {1707.04192},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2K2IBEHS/Lehmann et al. - 2019 - One-shot learning and behavioral eligibility trace.pdf},
  journal = {eLife},
  keywords = {rl,SNN},
  language = {en}
}

@article{leibo2015,
  title = {The {{Invariance Hypothesis Implies Domain}}-{{Specific Regions}} in {{Visual Cortex}}},
  author = {Leibo, Joel Z and Liao, Qianli and Anselmi, Fabio and Poggio, Tomaso},
  year = {2015},
  pages = {29},
  file = {/Users/x0r/Zotero/storage/HZ8FK329/Leibo et al_2015_The Invariance Hypothesis Implies Domain-Specific Regions in Visual Cortex.pdf},
  journal = {PLOS Computational Biology},
  keywords = {neuromorphic},
  language = {en}
}

@article{leibo2019,
  title = {Autocurricula and the {{Emergence}} of {{Innovation}} from {{Social Interaction}}: {{A Manifesto}} for {{Multi}}-{{Agent Intelligence Research}}},
  shorttitle = {Autocurricula and the {{Emergence}} of {{Innovation}} from {{Social Interaction}}},
  author = {Leibo, Joel Z. and Hughes, Edward and Lanctot, Marc and Graepel, Thore},
  year = {2019},
  month = mar,
  abstract = {Evolution has produced a multi-scale mosaic of interacting adaptive units. Innovations arise when perturbations push parts of the system away from stable equilibria into new regimes where previously well-adapted solutions no longer work. Here we explore the hypothesis that multi-agent systems sometimes display intrinsic dynamics arising from competition and cooperation that provide a naturally emergent curriculum, which we term an autocurriculum. The solution of one social task often begets new social tasks, continually generating novel challenges, and thereby promoting innovation. Under certain conditions these challenges may become increasingly complex over time, demanding that agents accumulate ever more innovations.},
  archivePrefix = {arXiv},
  eprint = {1903.00742},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LVAPTJ4C/Leibo et al_2019_Autocurricula and the Emergence of Innovation from Social Interaction.pdf},
  journal = {arXiv:1903.00742 [cs, q-bio]},
  keywords = {AGI,rl,To read},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{leng2018,
  title = {Spiking Neurons with Short-Term Synaptic Plasticity Form Superior Generative Networks},
  author = {Leng, Luziwei and Martel, Roman and Breitwieser, Oliver and Bytschok, Ilja and Senn, Walter and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
  year = {2018},
  month = dec,
  volume = {8},
  pages = {10651},
  issn = {2045-2322},
  doi = {10.1038/s41598-018-28999-2},
  file = {/Users/x0r/Zotero/storage/HYKZ67WP/Leng et al. - 2018 - Spiking neurons with short-term synaptic plasticit.pdf},
  journal = {Scientific Reports},
  keywords = {GAN,SNN},
  language = {en},
  number = {1}
}

@article{Lennie03,
  ids = {lennie2003},
  title = {The Cost of Cortical Computation},
  author = {Lennie, P.},
  year = {2003},
  month = mar,
  volume = {13},
  pages = {493--497},
  abstract = {Electrophysiological recordings show that individual neurons in cortex are strongly activated when engaged in appropriate tasks, but they tell us little about how many neurons might be engaged by a task, which is important to know if we are to understand how cortex encodes information. For human cortex, I estimate the cost of individual spikes, then, from the known energy consumption of cortex, I establish how many neurons can be active concurrently. The cost of a single spike is high, and this severely limits, possibly to fewer than 1\%, the number of neurons that can be substantially active concurrently. The high cost of spikes requires the brain not only to use representational codes that rely on very few active neurons, but also to allocate its energy resources flexibly among cortical regions according to task demand. The latter constraint explains the investment in local control of hemodynamics, exploited by functional magnetic resonance imaging, and the need for mechanisms of selective attention.},
  file = {/Users/x0r/Zotero/storage/QJ358DTJ/Lennie_2003_The Cost of Cortical Computation.pdf},
  journal = {Current Biology},
  keywords = {neuromorphic,neuroscience},
  number = {6},
  pmid = {12646132}
}

@article{leong2016,
  title = {Direction {{Selectivity}} in {{Drosophila Emerges}} from {{Preferred}}-{{Direction Enhancement}} and {{Null}}-{{Direction Suppression}}},
  author = {Leong, J. C. S. and Esch, J. J. and Poole, B. and Ganguli, S. and Clandinin, T. R.},
  year = {2016},
  month = aug,
  volume = {36},
  pages = {8078--8092},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1272-16.2016},
  file = {/Users/x0r/Zotero/storage/NMWMA7A5/Leong et al_2016_Direction Selectivity in Drosophila Emerges from Preferred-Direction.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {31}
}

@article{leong2017,
  title = {Dynamic {{Interaction}} between {{Reinforcement Learning}} and {{Attention}} in {{Multidimensional Environments}}},
  author = {Leong, Yuan Chang and Radulescu, Angela and Daniel, Reka and DeWoskin, Vivian and Niv, Yael},
  year = {2017},
  month = jan,
  volume = {93},
  pages = {451--463},
  issn = {08966273},
  doi = {10.1016/j.neuron.2016.12.040},
  abstract = {Little is known about the relationship between attention and learning during decision making. Using eye tracking and multivariate pattern analysis of fMRI data, we measured participants' dimensional attention as they performed a trial-and-error learning task in which only one of three stimulus dimensions was relevant for reward at any given time. Analysis of participants' choices revealed that attention biased both value computation during choice and value update during learning. Value signals in the ventromedial prefrontal cortex and prediction errors in the striatum were similarly biased by attention. In turn, participants' focus of attention was dynamically modulated by ongoing learning. Attentional switches across dimensions correlated with activity in a frontoparietal attention network, which showed enhanced connectivity with the ventromedial prefrontal cortex between switches. Our results suggest a bidirectional interaction between attention and learning: attention constrains learning to relevant dimensions of the environment, while we learn what to attend to via trial and error.},
  file = {/Users/x0r/Zotero/storage/KLJ5G7VK/Leong et al_2017_Dynamic Interaction between Reinforcement Learning and Attention in.pdf},
  journal = {Neuron},
  keywords = {attention,rl},
  language = {en},
  number = {2}
}

@article{letzkus2006,
  title = {Learning {{Rules}} for {{Spike Timing}}-{{Dependent Plasticity Depend}} on {{Dendritic Synapse Location}}},
  author = {Letzkus, J. J. and Kampa, B. M. and Stuart, G. J.},
  year = {2006},
  month = oct,
  volume = {26},
  pages = {10420--10429},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.2650-06.2006},
  file = {/Users/x0r/Zotero/storage/HIPGE8IT/aac7341.full.pdf;/Users/x0r/Zotero/storage/JQDIXAI2/Letzkus et al. - 2006 - Learning Rules for Spike Timing-Dependent Plastici.pdf},
  journal = {Journal of Neuroscience},
  language = {en},
  number = {41}
}

@article{li2011,
  title = {An {{SPICE}} Model for Phase-Change Memory Simulations},
  author = {Li, Xi and Song, Zhitang and Cai, Daolin and Chen, Xiaogang and Chen, Houpeng},
  year = {2011},
  month = sep,
  volume = {32},
  pages = {094011},
  issn = {1674-4926},
  doi = {10.1088/1674-4926/32/9/094011},
  abstract = {Along with a series of research works on the physical prototype and properties of the memory cell, an SPICE model for phase-change memory (PCM) simulations based on Verilog-A language is presented. By handling it with the heat distribution algorithm, threshold switching theory and the crystallization kinetic model, the proposed SPICE model can effectively reproduce the physical behaviors of the phase-change memory cell. In particular, it can emulate the cell's temperature curve and crystallinity profile during the programming process, which can enable us to clearly understand the PCM's working principle and program process.},
  file = {/Users/x0r/Zotero/storage/W8KL6WZL/Li et al_2011_An SPICE model for phase-change memory simulations.pdf},
  journal = {Journal of Semiconductors},
  keywords = {modeling,PCM},
  language = {en},
  number = {9}
}

@inproceedings{li2012,
  title = {Resistance Drift in Phase Change Memory},
  author = {Li, Jing and Luan, Binquan and Lam, Chung},
  year = {2012},
  month = apr,
  pages = {6C.1.1-6C.1.6},
  publisher = {{IEEE}},
  doi = {10.1109/IRPS.2012.6241871},
  abstract = {This paper discusses the major reliability issue in MLC PCM, namely time dependent resistance drift in amorphous chalcogenide materials. Starting from experimental observations, this paper presents a complete physical picture for structural relaxation (SR), which is considered to be the underlying mechanism for resistance drift. In particularly, various physics models and quantum molecular dynamics simulation are presented to reveal the interrelationship between atomic structure and electrical properties of amorphous chalcogenide materials. The paper provides insights to develop mitigation techniques including material engineering and various design techniques, etc. to ensure reliable MLC operations.},
  file = {/Users/x0r/Zotero/storage/V6SXGXS5/Li et al_2012_Resistance drift in phase change memory.pdf},
  isbn = {978-1-4577-1680-5 978-1-4577-1678-2 978-1-4577-1679-9},
  keywords = {drift,modeling,PCM},
  language = {en}
}

@article{li2018,
  title = {Analogue Signal and Image Processing with Large Memristor Crossbars},
  author = {Li, Can and Hu, Miao and Li, Yunning and Jiang, Hao and Ge, Ning and Montgomery, Eric and Zhang, Jiaming and Song, Wenhao and D{\'a}vila, Noraica and Graves, Catherine E. and Li, Zhiyong and Strachan, John Paul and Lin, Peng and Wang, Zhongrui and Barnell, Mark and Wu, Qing and Williams, R. Stanley and Yang, J. Joshua and Xia, Qiangfei},
  year = {2018},
  month = jan,
  volume = {1},
  pages = {52--59},
  issn = {2520-1131},
  doi = {10.1038/s41928-017-0002-z},
  file = {/Users/x0r/Zotero/storage/89V7X38L/Li et al_2018_Analogue signal and image processing with large memristor crossbars.pdf},
  journal = {Nature Electronics},
  keywords = {neuromorphic},
  language = {en},
  number = {1}
}

@article{li2019,
  title = {Long Short-Term Memory Networks in Memristor Crossbar Arrays},
  author = {Li, Can and Wang, Zhongrui and Rao, Mingyi and Belkin, Daniel and Song, Wenhao and Jiang, Hao and Yan, Peng and Li, Yunning and Lin, Peng and Hu, Miao and Ge, Ning and Strachan, John Paul and Barnell, Mark and Wu, Qing and Williams, R. Stanley and Yang, J. Joshua and Xia, Qiangfei},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {49},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0001-4},
  abstract = {Deep neural networks are increasingly popular in data-intensive applications, but are power-hungry. New types of computer chips that are suited to the task of deep learning, such as memristor arrays where data handling and computing take place within the same unit, are required. A well-used deep learning model called long short-term memory, which can handle temporal sequential data analysis, is now implemented in a memristor crossbar array, promising an energy-efficient and low-footprint deep learning platform.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {/Users/x0r/Zotero/storage/P9N9J2Z6/Li et al_2019_Long short-term memory networks in memristor crossbar arrays.pdf},
  journal = {Nature Machine Intelligence},
  language = {En},
  number = {1}
}

@article{li2019a,
  title = {Long Short-Term Memory Networks in Memristor Crossbar Arrays},
  author = {Li, Can and Wang, Zhongrui and Rao, Mingyi and Belkin, Daniel and Song, Wenhao and Jiang, Hao and Yan, Peng and Li, Yunning and Lin, Peng and Hu, Miao and Ge, Ning and Strachan, John Paul and Barnell, Mark and Wu, Qing and Williams, R. Stanley and Yang, J. Joshua and Xia, Qiangfei},
  year = {2019},
  month = jan,
  volume = {1},
  pages = {49--57},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0001-4},
  abstract = {Deep neural networks are increasingly popular in data-intensive applications, but are power-hungry. New types of computer chips that are suited to the task of deep learning, such as memristor arrays where data handling and computing take place within the same unit, are required. A well-used deep learning model called long short-term memory, which can handle temporal sequential data analysis, is now implemented in a memristor crossbar array, promising an energy-efficient and low-footprint deep learning platform.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {/Users/x0r/Zotero/storage/LSTQLWD5/Li et al. - 2019 - Long short-term memory networks in memristor cross.pdf;/Users/x0r/Zotero/storage/BYRJXI2Z/s42256-018-0001-4.html},
  journal = {Nature Machine Intelligence},
  language = {en},
  number = {1}
}

@article{Liang_Indiveri19,
  ids = {liang2019},
  title = {A Neuromorphic Computational Primitive for Robust Context-Dependent Decision Making and Context-Dependent Stochastic Computation},
  author = {Liang, D. and Indiveri, G.},
  year = {2019},
  file = {/Users/x0r/Zotero/storage/K7HSTLYK/Liang_Indiveri_2019_A Neuromorphic Computational Primitive for Robust Context-Dependent Decision.pdf},
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  keywords = {neuromorphic}
}

@article{liao2015,
  title = {How {{Important}} Is {{Weight Symmetry}} in {{Backpropagation}}?},
  author = {Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
  year = {2015},
  month = oct,
  abstract = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections\textemdash the same weights must be used for forward and backward passes. This ``weight transport problem'' [1] is thought to be one of the main reasons of BP's biological implausibility. Using 15 different classification datasets, we systematically study to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.'s demonstration [2] but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter\textemdash the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) [3] and/or a ``Batch Manhattan'' (BM) update rule.},
  archivePrefix = {arXiv},
  eprint = {1510.05067},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/A287VTWT/Liao et al_2015_How Important is Weight Symmetry in Backpropagation.pdf},
  journal = {arXiv:1510.05067 [cs]},
  keywords = {backprop,neuromorphic},
  language = {en},
  primaryClass = {cs}
}

@article{lichtsteiner2008,
  title = {A 128x128 120 {{dB}} 15 Us {{Latency Asynchronous Temporal Contrast Vision Sensor}}},
  author = {Lichtsteiner, Patrick and Posch, Christoph and Delbruck, Tobi},
  year = {2008},
  volume = {43},
  pages = {566--576},
  issn = {0018-9200},
  doi = {10.1109/JSSC.2007.914337},
  abstract = {This paper describes a 128 times 128 pixel CMOS vision sensor. Each pixel independently and in continuous time quantizes local relative intensity changes to generate spike events. These events appear at the output of the sensor as an asynchronous stream of digital pixel addresses. These address-events signify scene reflectance change and have sub-millisecond timing precision. The output data rate depends on the dynamic content of the scene and is typically orders of magnitude lower than those of conventional frame-based imagers. By combining an active continuous-time front-end logarithmic photoreceptor with a self-timed switched-capacitor differencing circuit, the sensor achieves an array mismatch of 2.1\% in relative intensity event threshold and a pixel bandwidth of 3 kHz under 1 klux scene illumination. Dynamic range is {$>$} 120 dB and chip power consumption is 23 mW. Event latency shows weak light dependency with a minimum of 15 mus at {$>$} 1 klux pixel illumination. The sensor is built in a 0.35 mum 4M2P process. It has 40times40 mum2 pixels with 9.4\% fill factor. By providing high pixel bandwidth, wide dynamic range, and precisely timed sparse digital output, this silicon retina provides an attractive combination of characteristics for low-latency dynamic vision under uncontrolled illumination with low post-processing requirements.},
  file = {/Users/x0r/Zotero/storage/H4UMQME7/Lichtsteiner et al. - 2008 - A 128$times$128 120 dB 15 $mu$s Latency Asynchro.pdf},
  journal = {IEEE Journal of Solid-State Circuits},
  language = {en},
  number = {2}
}

@article{lillicrap2015,
  title = {Continuous Control with Deep Reinforcement Learning},
  author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  year = {2015},
  month = sep,
  abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
  archivePrefix = {arXiv},
  eprint = {1509.02971},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2VAF4HWM/Lillicrap et al_2015_Continuous control with deep reinforcement learning.pdf},
  journal = {arXiv:1509.02971 [cs, stat]},
  keywords = {rl,spinning-up},
  primaryClass = {cs, stat}
}

@article{lillicrap2016,
  title = {Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning},
  author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
  year = {2016},
  month = dec,
  volume = {7},
  issn = {2041-1723},
  doi = {10.1038/ncomms13276},
  file = {/Users/x0r/Zotero/storage/9T9FMBU3/Lillicrap et al_2016_Random synaptic feedback weights support error backpropagation for deep learning.pdf},
  journal = {Nature Communications},
  keywords = {backprop,neuromorphic},
  language = {en},
  number = {1}
}

@article{lillicrap2019,
  title = {Backpropagation through Time and the Brain},
  author = {Lillicrap, Timothy P and Santoro, Adam},
  year = {2019},
  month = apr,
  volume = {55},
  pages = {82--89},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2019.01.011},
  abstract = {It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike.},
  file = {/Users/x0r/Zotero/storage/JZVFNQZE/Lillicrap_Santoro_2019_Backpropagation through time and the brain.pdf},
  journal = {Current Opinion in Neurobiology},
  keywords = {dl,neuroscience},
  series = {Machine {{Learning}}, {{Big Data}}, and {{Neuroscience}}}
}

@article{lillicrap2020,
  title = {Backpropagation and the Brain},
  author = {Lillicrap, Timothy P. and Santoro, Adam and Marris, Luke and Akerman, Colin J. and Hinton, Geoffrey},
  year = {2020},
  month = apr,
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0277-3},
  abstract = {During learning, the brain modifies synapses to improve behaviour. In the cortex, synapses are embedded within multilayered networks, making it difficult to determine the effect of an individual synaptic modification on the behaviour of the system. The backpropagation algorithm solves this problem in deep artificial neural networks, but historically it has been viewed as biologically problematic. Nonetheless, recent developments in neuroscience and the successes of artificial neural networks have reinvigorated interest in whether backpropagation offers insights for understanding learning in the cortex. The backpropagation algorithm learns quickly by computing synaptic updates using feedback connections to deliver error signals. Although feedback connections are ubiquitous in the cortex, it is difficult to see how they could deliver the error signals required by strict formulations of backpropagation. Here we build on past and recent developments to argue that feedback connections may instead induce neural activities whose differences can be used to locally approximate these signals and hence drive effective learning in deep networks in the brain.},
  file = {/Users/x0r/Zotero/storage/CXSN3SXY/Lillicrap et al. - 2020 - Backpropagation and the brain.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en}
}

@article{lim2016,
  title = {{{ReRAM Crossbar Array}}: {{Reduction}} of {{Access Time}} by {{Reducing}} the {{Parasitic Capacitance}} of the {{Selector Device}}},
  shorttitle = {{{ReRAM Crossbar Array}}},
  author = {Lim, Hyein and Sun, Wookyung and Shin, Hyungsoon},
  year = {2016},
  month = feb,
  volume = {63},
  pages = {873--876},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2015.2506598},
  abstract = {The transient response of a crossbar array is investigated with HSPICE circuit simulation. A crossbar array contains many parasitic elements. This brief focuses on the parasitic capacitances of the selector devices and resistors. The read access time is determined for various array sizes and parasitic capacitances. The results show that the read access time increases with the array size and parasitic capacitance, particularly the selector capacitance. The effect of selector capacitance is analyzed with a time-dependent, unit-cell voltage model and simulation. The simulation results reveal that the selector capacitance has a greater effect than the resistor capacitance on the read access time because of the large time constant. The effect of the selector capacitance on the read access time remains significant for larger values of the other parasitic capacitances.},
  file = {/Users/x0r/Zotero/storage/7A93HWKP/Lim et al_2016_ReRAM Crossbar Array.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {neuromorphic,ReRAM},
  language = {en},
  number = {2}
}

@book{Liu_etal02a,
  ids = {liu2002},
  title = {Analog {{VLSI}}:Circuits and Principles},
  author = {Liu, S.-C. and Kramer, J. and Indiveri, G. and Delbruck, T. and Douglas, R.J.},
  year = {2002},
  publisher = {{MIT Press}},
  file = {/Users/x0r/Zotero/storage/AFP8LLRP/Liu_2002_Analog VLSI.pdf},
  keywords = {neuromorphic,VLSI}
}

@article{liu2017,
  title = {Hierarchical {{Representations}} for {{Efficient Architecture Search}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Vinyals, Oriol and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  abstract = {We explore efficient neural architecture search methods and present a simple yet powerful evolutionary algorithm that can discover new architectures achieving state of the art results. Our approach combines a novel hierarchical genetic representation scheme that imitates the modularized design pattern commonly adopted by human experts, and an expressive search space that supports complex topologies. Our algorithm efficiently discovers architectures that outperform a large number of manually designed models for image classification, obtaining top-1 error of 3.6\% on CIFAR-10 and 20.3\% when transferred to ImageNet, which is competitive with the best existing neural architecture search approaches and represents the new state of the art for evolutionary strategies on this task. We also present results using random search, achieving 0.3\% less top-1 accuracy on CIFAR-10 and 0.1\% less on ImageNet whilst reducing the architecture search time from 36 hours down to 1 hour.},
  archivePrefix = {arXiv},
  eprint = {1711.00436},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/3IWBDYGW/Liu et al_2017_Hierarchical Representations for Efficient Architecture Search.pdf},
  journal = {arXiv:1711.00436 [cs, stat]},
  keywords = {dl,meta-learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{livesey,
  title = {The {{Existence}} of {{Local Minima}} in {{Local}}-{{Minimum}}-{{Free Potential Surfaces}}},
  author = {Livesey, Graeme Bell Mike},
  pages = {6},
  abstract = {Existing approaches to potential field based navigation, such as (Khatib, 1986) and (Rimon and Koditschek, 1992), have traditionally seen the local minimum problem as the only significant obstacle. This is because they have concentrated on the problem of `classical' local minima, characterised by a positive definite Hessian.},
  file = {/Users/x0r/Zotero/storage/FMGL76LD/Livesey_The Existence of Local Minima in Local-Minimum-Free Potential Surfaces.pdf},
  keywords = {dl,rl},
  language = {en}
}

@article{lohani2019,
  title = {Dopamine {{Modulation}} of {{Prefrontal Cortex Activity Is Manifold}} and {{Operates}} at {{Multiple Temporal}} and {{Spatial Scales}}},
  author = {Lohani, Sweyta and Martig, Adria K. and Deisseroth, Karl and Witten, Ilana B. and Moghaddam, Bita},
  year = {2019},
  month = apr,
  volume = {27},
  pages = {99-114.e6},
  issn = {22111247},
  doi = {10.1016/j.celrep.2019.03.012},
  abstract = {Although the function of dopamine in subcortical structures is largely limited to reward and movement, dopamine neurotransmission in the prefrontal cortex (PFC) is critical to a multitude of temporally and functionally diverse processes, such as attention, working memory, behavioral flexibility, action planning, and sustained motivational and affective states. How does dopamine influence computation of these temporally complex functions? We find causative links between sustained and burst patterns of phasic dopamine neuron activation and modulation of medial PFC neuronal activity at multiple spatiotemporal scales. These include a multidirectional and weak impact on individual neuron rate activity but a robust influence on coordinated ensemble activity, gamma oscillations, and gamma-theta coupling that persisted for minutes. In addition, PFC network responses to burst pattern of dopamine firing were selectively strengthened in behaviorally active states. This multiplex mode of modulation by dopamine input may enable PFC to compute and generate spatiotemporally diverse and specialized outputs.},
  file = {/Users/x0r/Zotero/storage/5YU5JYPN/Lohani et al. - 2019 - Dopamine Modulation of Prefrontal Cortex Activity .pdf},
  journal = {Cell Reports},
  keywords = {neuroscience},
  language = {en},
  number = {1}
}

@article{long2012,
  title = {Analysis and Modeling of Resistive Switching Statistics},
  author = {Long, Shibing and Cagli, Carlo and Ielmini, Daniele and Liu, Ming and Su{\~n}{\'e}, Jordi},
  year = {2012},
  month = apr,
  volume = {111},
  pages = {074508},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.3699369},
  file = {/Users/x0r/Zotero/storage/2WDLH7ME/Long et al_2012_Analysis and modeling of resistive switching statistics.pdf},
  journal = {Journal of Applied Physics},
  keywords = {ReRAM,Sungjung},
  language = {en},
  number = {7}
}

@inproceedings{long2016,
  title = {{{ReRAM Crossbar}} Based {{Recurrent Neural Network}} for Human Activity Detection},
  author = {Long, Yun and Jung, Eui Min and Kung, Jaeha and Mukhopadhyay, Saibal},
  year = {2016},
  month = jul,
  pages = {939--946},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2016.7727299},
  abstract = {We present a programmable high-efficient Recurrent Neural Network (RNN) with Synapses design using Resistive Random Access Memory (ReRAM). The presented ReRAM-RNN employs crossbar ReRAM arrays as synapses. A fast synapses programming methodology is realized by CMOSbased neuron with in-built programming circuitry. The simulations are performed using experimentally verified physical resistive switching model, instead of only functional models, providing better estimate of system speed and power efficiency. Simulation results show that ReRAM-RNN can provide higher computation efficiency and/or more compact design than software realization of RNN, and dedicated CMOS based digitaland analog-RNN. We show that the efficiency improvement of ReRAM-based neural network design is more significant in feedback networks than in feedforward networks.},
  file = {/Users/x0r/Zotero/storage/SX7RZ4TI/Long et al_2016_ReRAM Crossbar based Recurrent Neural Network for human activity detection.pdf},
  isbn = {978-1-5090-0620-5},
  keywords = {application,neuromorphic,ReRAM},
  language = {en}
}

@article{lowe2017,
  title = {Multi-{{Agent Actor}}-{{Critic}} for {{Mixed Cooperative}}-{{Competitive Environments}}},
  author = {Lowe, Ryan and Wu, Yi and Tamar, Aviv and Harb, Jean and Abbeel, Pieter and Mordatch, Igor},
  year = {2017},
  month = jun,
  abstract = {We explore deep reinforcement learning methods for multi-agent domains. We begin by analyzing the difficulty of traditional algorithms in the multi-agent case: Q-learning is challenged by an inherent non-stationarity of the environment, while policy gradient suffers from a variance that increases as the number of agents grows. We then present an adaptation of actor-critic methods that considers action policies of other agents and is able to successfully learn policies that require complex multiagent coordination. Additionally, we introduce a training regimen utilizing an ensemble of policies for each agent that leads to more robust multi-agent policies. We show the strength of our approach compared to existing methods in cooperative as well as competitive scenarios, where agent populations are able to discover various physical and informational coordination strategies.},
  archivePrefix = {arXiv},
  eprint = {1706.02275},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JSUNFP7T/Lowe et al_2017_Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments.pdf},
  journal = {arXiv:1706.02275 [cs]},
  keywords = {AGI,rl,To read},
  language = {en},
  primaryClass = {cs}
}

@article{luckas2010,
  title = {Investigation of Defect States in the Amorphous Phase of Phase Change Alloys {{GeTe}} and {{Ge}} {\textsubscript{2}} {{Sb}} {\textsubscript{2}} {{Te}} {\textsubscript{5}}},
  author = {Luckas, Jennifer and Krebs, Daniel and Salinga, Martin and Wuttig, Matthias and Longeaud, Christophe},
  year = {2010},
  month = jan,
  pages = {NA-NA},
  issn = {18626351, 16101642},
  doi = {10.1002/pssc.200982694},
  file = {/Users/x0r/Zotero/storage/W87NH6GS/Luckas et al_2010_Investigation of defect states in the amorphous phase of phase change alloys.pdf},
  journal = {physica status solidi (c)},
  keywords = {drift,PCM},
  language = {en}
}

@article{luckas2011,
  title = {The Influence of a Temperature Dependent Bandgap on the Energy Scale of Modulated Photocurrent Experiments},
  author = {Luckas, Jennifer and Kremers, Stephan and Krebs, Daniel and Salinga, Martin and Wuttig, Matthias and Longeaud, Christophe},
  year = {2011},
  month = jul,
  volume = {110},
  pages = {013719},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.3605517},
  file = {/Users/x0r/Zotero/storage/6W44TW4J/Luckas et al_2011_The influence of a temperature dependent bandgap on the energy scale of.pdf},
  journal = {Journal of Applied Physics},
  language = {en},
  number = {1}
}

@article{luckas2013,
  title = {Defects in Amorphous Phase-Change Materials},
  author = {Luckas, Jennifer and Krebs, Daniel and Grothe, Stephanie and Klomfa{\ss}, Josef and Carius, Reinhard and Longeaud, Christophe and Wuttig, Matthias},
  year = {2013},
  month = may,
  volume = {28},
  pages = {1139--1147},
  issn = {0884-2914, 2044-5326},
  doi = {10.1557/jmr.2013.72},
  file = {/Users/x0r/Zotero/storage/KDELMUFV/Luckas et al_2013_Defects in amorphous phase-change materials.pdf},
  journal = {Journal of Materials Research},
  keywords = {PCM},
  language = {en},
  number = {09}
}

@article{luketina2019,
  title = {A {{Survey}} of {{Reinforcement Learning Informed}} by {{Natural Language}}},
  author = {Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  year = {2019},
  month = jun,
  abstract = {To be successful in real-world tasks, Reinforcement Learning (RL) needs to exploit the compositional, relational, and hierarchical structure of the world, and learn to transfer it to the task at hand. Recent advances in representation learning for language make it possible to build models that acquire world knowledge from text corpora and integrate this knowledge into downstream decision making problems. We thus argue that the time is right to investigate a tight integration of natural language understanding into RL in particular. We survey the state of the field, including work on instruction following, text games, and learning from textual domain knowledge. Finally, we call for the development of new environments as well as further investigation into the potential uses of recent Natural Language Processing (NLP) techniques for such tasks.},
  archivePrefix = {arXiv},
  eprint = {1906.03926},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NACXYYN8/Luketina et al_2019_A Survey of Reinforcement Learning Informed by Natural Language.pdf},
  journal = {arXiv:1906.03926 [cs, stat]},
  keywords = {agi,comm},
  primaryClass = {cs, stat}
}

@book{macmillan2005,
  title = {Detection Theory: A User's Guide},
  shorttitle = {Detection Theory},
  author = {Macmillan, Neil A. and Creelman, C. Douglas},
  year = {2005},
  edition = {2nd ed},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Mahwah, N.J}},
  file = {/Users/x0r/Zotero/storage/YUWJABSD/Macmillan_Creelman_2005_Detection theory.pdf},
  isbn = {978-0-8058-4230-2 978-0-8058-4231-9},
  language = {en},
  lccn = {BF237 .M25 2005}
}

@book{mahajan2010,
  title = {Street-Fighting Mathematics: The Art of Educated Guessing and Opportunistic Problem Solving},
  shorttitle = {Street-Fighting Mathematics},
  author = {Mahajan, Sanjoy},
  year = {2010},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/Users/x0r/Zotero/storage/TFYJ85BS/Mahajan - 2010 - Street-fighting mathematics the art of educated g.pdf},
  isbn = {978-0-262-51429-3},
  keywords = {Estimation theory,Hypothesis,Problem solving},
  language = {en},
  lccn = {QA63 .M34 2010}
}

@article{maheswaranathan2018,
  title = {Guided Evolutionary Strategies: {{Augmenting}} Random Search with Surrogate Gradients},
  shorttitle = {Guided Evolutionary Strategies},
  author = {Maheswaranathan, Niru and Metz, Luke and Tucker, George and Choi, Dami and {Sohl-Dickstein}, Jascha},
  year = {2018},
  month = jun,
  abstract = {Many applications in machine learning require optimizing a function whose true gradient is inaccessible, but where surrogate gradient information (directions that may be correlated with, but not necessarily identical to, the true gradient) is available instead. This arises when an approximate gradient is easier to compute than the full gradient (e.g. in meta-learning or unrolled optimization), or when a true gradient is intractable and is replaced with a surrogate (e.g. in certain reinforcement learning applications or training networks with discrete variables). We propose Guided Evolutionary Strategies, a method for optimally using surrogate gradient directions along with random search. We define a search distribution for evolutionary strategies that is elongated along a subspace spanned by the surrogate gradients. This allows us to estimate a descent direction which can then be passed to a first-order optimizer. We analytically and numerically characterize the trade-offs that result from tuning how strongly the search distribution is stretched along the guiding subspace, and use this to derive a setting of the hyperparameters that works well across problems. Finally, we apply our method to example problems, demonstrating an improvement over both standard evolutionary strategies and first-order methods that directly follow the surrogate gradient.},
  archivePrefix = {arXiv},
  eprint = {1806.10230},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/69F7HLQV/Maheswaranathan et al. - 2018 - Guided evolutionary strategies Augmenting random .pdf},
  journal = {arXiv:1806.10230 [cs, stat]},
  keywords = {evolutionary-strategies},
  language = {en},
  primaryClass = {cs, stat}
}

@article{maheswaranathan2019,
  title = {Universality and Individuality in Neural Dynamics across Large Populations of Recurrent Networks},
  author = {Maheswaranathan, Niru and Williams, Alex H. and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  year = {2019},
  month = jul,
  abstract = {Task-based modeling with recurrent neural networks (RNNs) has emerged as a popular way to infer the computational function of different brain regions. These models are quantitatively assessed by comparing the low-dimensional neural representations of the model with the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve common tasks uniquely determine the network dynamics, independent of modeling architectural choices? Or alternatively, are the learned dynamics highly sensitive to different model choices? Knowing the answer to these questions has strong implications for whether and how we should use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks, with commonly used RNN architectures, trained to solve neuroscientifically motivated tasks and characterize their nonlinear dynamics. We find the geometry of the RNN representations can be highly sensitive to different network architectures, yielding a cautionary tale for measures of similarity that rely on representational geometry, such as CCA. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold\textemdash the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics\textemdash often appears universal across all architectures.},
  archivePrefix = {arXiv},
  eprint = {1907.08549},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AMRB6NK8/Maheswaranathan et al_2019_Universality and individuality in neural dynamics across large populations of.pdf},
  journal = {arXiv:1907.08549 [cs, q-bio]},
  keywords = {RNN},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{maheswaranathan2019a,
  title = {Reverse Engineering Recurrent Networks for Sentiment Classification Reveals Line Attractor Dynamics},
  author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew D. and Ganguli, Surya and Sussillo, David},
  year = {2019},
  month = jun,
  abstract = {Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it--to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.},
  archivePrefix = {arXiv},
  eprint = {1906.10720},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JZFLHFZK/Maheswaranathan et al_2019_Reverse engineering recurrent networks for sentiment classification reveals.pdf;/Users/x0r/Zotero/storage/LFL2DM4A/1906.html},
  journal = {arXiv:1906.10720 [cs, stat]},
  keywords = {RNN},
  primaryClass = {cs, stat}
}

@article{maier2017,
  title = {Precision {{Learning}}: {{Towards Use}} of {{Known Operators}} in {{Neural Networks}}},
  shorttitle = {Precision {{Learning}}},
  author = {Maier, Andreas and Schebesch, Frank and Syben, Christopher and W{\"u}rfl, Tobias and Steidl, Stefan and Choi, Jang-Hwan and Fahrig, Rebecca},
  year = {2017},
  month = dec,
  abstract = {In this paper, we consider the use of prior knowledge within neural networks. In particular, we investigate the effect of a known transform within the mapping from input data space to the output domain. We demonstrate that use of known transforms is able to change maximal error bounds.},
  archivePrefix = {arXiv},
  eprint = {1712.00374},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VSNMJMV4/Maier et al_2017_Precision Learning.pdf},
  journal = {arXiv:1712.00374 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{malkiel,
  title = {Deep {{Learning}} for {{Design}} and {{Retrieval}} of {{Nano}}-Photonic {{Structures}}},
  author = {Malkiel, Itzik and Nagler, Achiya and Arieli, Uri and Mrejen, Michael},
  pages = {13},
  file = {/Users/x0r/Zotero/storage/NEDDZU8X/Malkiel et al_Deep Learning for Design and Retrieval of Nano-photonic Structures.pdf},
  keywords = {neuromorphic,photonics},
  language = {en}
}

@article{marblestone2016,
  title = {Toward an {{Integration}} of {{Deep Learning}} and {{Neuroscience}}},
  author = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
  year = {2016},
  month = sep,
  volume = {10},
  issn = {1662-5188},
  doi = {10.3389/fncom.2016.00094},
  abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
  file = {/Users/x0r/Zotero/storage/FH43TASC/Marblestone et al. - 2016 - Toward an Integration of Deep Learning and Neurosc.pdf},
  journal = {Frontiers in Computational Neuroscience},
  keywords = {neuroscience,SNN,To read},
  language = {en}
}

@article{marinescu2018,
  title = {Quasi-Experimental Causality in Neuroscience and Behavioural Research},
  author = {Marinescu, Ioana E. and Lawlor, Patrick N. and Kording, Konrad P.},
  year = {2018},
  month = nov,
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0466-5},
  file = {/Users/x0r/Zotero/storage/QZJECYHV/Marinescu et al_2018_Quasi-experimental causality in neuroscience and behavioural research.pdf},
  journal = {Nature Human Behaviour},
  keywords = {neuroscience},
  language = {en}
}

@article{marra2019,
  title = {{{LYRICS}}: A {{General Interface Layer}} to {{Integrate AI}} and {{Deep Learning}}},
  shorttitle = {{{LYRICS}}},
  author = {Marra, Giuseppe and Giannini, Francesco and Diligenti, Michelangelo and Gori, Marco},
  year = {2019},
  month = mar,
  abstract = {In spite of the amazing results obtained by deep learning in many applications, a real intelligent behavior of an agent acting in a complex environment is likely to require some kind of higher-level symbolic inference. Therefore, there is a clear need for the definition of a general and tight integration between low-level tasks, processing sensorial data that can be effectively elaborated using deep learning techniques, and the logic reasoning that allows humans to take decisions in complex environments. This paper presents LYRICS, a generic interface layer for AI, which is implemented in TersorFlow (TF). LYRICS provides an input language that allows to define arbitrary First Order Logic (FOL) background knowledge. The predicates and functions of the FOL knowledge can be bound to any TF computational graph, and the formulas are converted into a set of real-valued constraints, which participate to the overall optimization problem. This allows to learn the weights of the learners, under the constraints imposed by the prior knowledge. The framework is extremely general as it imposes no restrictions in terms of which models or knowledge can be integrated. In this paper, we show the generality of the approach showing some use cases of the presented language, including generative models, logic reasoning, model checking and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1903.07534},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/IZQY53EY/Marra et al_2019_LYRICS.pdf},
  journal = {arXiv:1903.07534 [cs, stat]},
  keywords = {AGI},
  primaryClass = {cs, stat}
}

@article{marschall2019,
  title = {Using Local Plasticity Rules to Train Recurrent Neural Networks},
  author = {Marschall, Owen and Cho, Kyunghyun and Savin, Cristina},
  year = {2019},
  month = may,
  abstract = {To learn useful dynamics on long time scales, neurons must use plasticity rules that account for longterm, circuit-wide effects of synaptic changes. In other words, neural circuits must solve a credit assignment problem to appropriately assign responsibility for global network behavior to individual circuit components. Furthermore, biological constraints demand that plasticity rules are spatially and temporally local; that is, synaptic changes can depend only on variables accessible to the pre- and postsynaptic neurons. While artificial intelligence offers a computational solution for credit assignment, namely backpropagation through time (BPTT), this solution is wildly biologically implausible. It requires both nonlocal computations and unlimited memory capacity, as any synaptic change is a complicated function of the entire history of network activity. Similar nonlocality issues plague other approaches such as FORCE [1]. Overall, we are still missing a model for learning in recurrent circuits that both works computationally and uses only local updates. Leveraging recent advances in machine learning on approximating gradients for BPTT, we derive biologically plausible plasticity rules that enable recurrent networks to accurately learn long-term dependencies in sequential data. The solution takes the form of neurons with segregated voltage compartments, with several synaptic sub-populations that have different functional properties. The network operates in distinct phases during which each synaptic sub-population is updated by its own local plasticity rule. Our results provide new insights into the potential roles of segregated dendritic compartments, branch-specific inhibition, and global circuit phases in learning.},
  archivePrefix = {arXiv},
  eprint = {1905.12100},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/IFEBBMUX/Marschall et al_2019_Using local plasticity rules to train recurrent neural networks.pdf},
  journal = {arXiv:1905.12100 [cs, q-bio]},
  keywords = {neuromorphic,neuroscience},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{martin,
  title = {Three {{Generations}} of {{Asynchronous Microprocessors}}},
  author = {Martin, Alain J and Nystrom, Mika and Wong, Catherine G},
  pages = {14},
  abstract = {Asynchronous VLSI offers low power, modularity, and robustness to physical variations. We describe three generations of asynchronous microprocessors designed at Caltech between 1988 and today, and the evolving circuits and design techniques associated with each of them.},
  file = {/Users/x0r/Zotero/storage/VT4NARM7/Martin et al_Three Generations of Asynchronous Microprocessors.pdf},
  keywords = {VLSI},
  language = {en}
}

@article{merel2018,
  title = {Hierarchical Visuomotor Control of Humanoids},
  author = {Merel, Josh and Ahuja, Arun and Pham, Vu and Tunyasuvunakool, Saran and Liu, Siqi and Tirumala, Dhruva and Heess, Nicolas and Wayne, Greg},
  year = {2018},
  month = nov,
  abstract = {We aim to build complex humanoid agents that integrate perception, motor control, and memory. In this work, we partly factor this problem into low-level motor control from proprioception and high-level coordination of the low-level skills informed by vision. We develop an architecture capable of surprisingly flexible, task-directed motor control of a relatively high-DoF humanoid body by combining pre-training of low-level motor controllers with a high-level, task-focused controller that switches among low-level sub-policies. The resulting system is able to control a physically-simulated humanoid body to solve tasks that require coupling visual perception from an unstabilized egocentric RGB camera during locomotion in the environment. For a supplementary video link, see https://youtu.be/7GISvfbykLE .},
  archivePrefix = {arXiv},
  eprint = {1811.09656},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GTTSBRI5/Merel et al_2018_Hierarchical visuomotor control of humanoids.pdf},
  journal = {arXiv:1811.09656 [cs]},
  keywords = {neuroscience},
  primaryClass = {cs}
}

@article{merel2019,
  title = {Hierarchical Motor Control in Mammals and Machines},
  author = {Merel, Josh and Botvinick, Matthew and Wayne, Greg},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5489},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13239-6},
  file = {/Users/x0r/Zotero/storage/IQ8EN8RN/Merel et al. - 2019 - Hierarchical motor control in mammals and machines.pdf},
  journal = {Nature Communications},
  keywords = {To read},
  language = {en},
  number = {1}
}

@article{merel2019a,
  title = {Hierarchical Motor Control in Mammals and Machines},
  author = {Merel, Josh and Botvinick, Matthew and Wayne, Greg},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {5489},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13239-6},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{Merolla_etal14a,
  ids = {merolla2014},
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  author = {Merolla, Paul A. and Arthur, John V. and {Alvarez-Icaza}, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  year = {2014},
  month = aug,
  volume = {345},
  pages = {668--673},
  issn = {0036-8075, 1095-9203},
  file = {/Users/x0r/Zotero/storage/LDIZ5LLE/Merolla et al_2014_A million spiking-neuron integrated circuit with a scalable communication.pdf},
  journal = {Science},
  keywords = {neuromorphic,SNN},
  number = {6197}
}

@article{Metropolis_etal53,
  ids = {metropolis1953},
  title = {Equation of State Calculations by Fast Computing Machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W and Rosenbluth, Marshall N and Teller, Augusta H and Teller, Edward},
  year = {1953},
  volume = {21},
  pages = {1087},
  file = {/Users/x0r/Zotero/storage/DQ3TMZU7/Metropolis et al_1953_Equation of State Calculations by Fast Computing Machines.pdf},
  journal = {The journal of chemical physics}
}

@article{metz2018,
  title = {Learning {{Unsupervised Learning Rules}}},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and {Sohl-Dickstein}, Jascha},
  year = {2018},
  month = sep,
  abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this goal is...},
  file = {/Users/x0r/Zotero/storage/M866BJT5/Metz et al_2018_Learning Unsupervised Learning Rules.pdf}
}

@article{miconi2017,
  title = {Biologically Plausible Learning in Recurrent Neural Networks Reproduces Neural Dynamics Observed during Cognitive Tasks},
  author = {Miconi, Thomas},
  editor = {Frank, Michael J},
  year = {2017},
  month = feb,
  volume = {6},
  pages = {e20899},
  publisher = {{eLife Sciences Publications, Ltd}},
  issn = {2050-084X},
  doi = {10.7554/eLife.20899},
  abstract = {Neural activity during cognitive tasks exhibits complex dynamics that flexibly encode task-relevant variables. Chaotic recurrent networks, which spontaneously generate rich dynamics, have been proposed as a model of cortical computation during cognitive tasks. However, existing methods for training these networks are either biologically implausible, and/or require a continuous, real-time error signal to guide learning. Here we show that a biologically plausible learning rule can train such recurrent networks, guided solely by delayed, phasic rewards at the end of each trial. Networks endowed with this learning rule can successfully learn nontrivial tasks requiring flexible (context-dependent) associations, memory maintenance, nonlinear mixed selectivities, and coordination among multiple outputs. The resulting networks replicate complex dynamics previously observed in animal cortex, such as dynamic encoding of task features and selective integration of sensory inputs. We conclude that recurrent neural networks offer a plausible model of cortical dynamics during both learning and performance of flexible behavior.},
  file = {/Users/x0r/Zotero/storage/KTF9TM24/Miconi - 2017 - Biologically plausible learning in recurrent neura.pdf},
  journal = {eLife}
}

@article{miconi2019,
  title = {Backpropamine: {{Training}} Self-Modifying Neural Networks with Differentiable Neuromodulated Plasticity},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O},
  year = {2019},
  pages = {15},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  file = {/Users/x0r/Zotero/storage/FY9EDXSW/Miconi et al. - 2019 - BACKPROPAMINE TRAINING SELF-MODIFYING NEU- RAL NE.pdf},
  keywords = {backprop},
  language = {en}
}

@article{miikkulainen2017,
  title = {Evolving {{Deep Neural Networks}}},
  author = {Miikkulainen, Risto and Liang, Jason and Meyerson, Elliot and Rawal, Aditya and Fink, Dan and Francon, Olivier and Raju, Bala and Shahrzad, Hormoz and Navruzyan, Arshak and Duffy, Nigel and Hodjat, Babak},
  year = {2017},
  month = mar,
  abstract = {The success of deep learning depends on finding an architecture to fit the task. As deep learning has scaled up to more challenging tasks, the architectures have become difficult to design by hand. This paper proposes an automated method, CoDeepNEAT, for optimizing deep learning architectures through evolution. By extending existing neuroevolution methods to topology, components, and hyperparameters, this method achieves results comparable to best human designs in standard benchmarks in object recognition and language modeling. It also supports building a real-world application of automated image captioning on a magazine website. Given the anticipated increases in available computing power, evolution of deep networks is promising approach to constructing deep learning applications in the future.},
  archivePrefix = {arXiv},
  eprint = {1703.00548},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/EZLDFR2A/Miikkulainen et al_2017_Evolving Deep Neural Networks.pdf},
  journal = {arXiv:1703.00548 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{miranda2015,
  title = {{{DARPA}}-Funded Efforts in the Development of Novel Brain\textendash Computer Interface Technologies},
  author = {Miranda, Robbin A. and Casebeer, William D. and Hein, Amy M. and Judy, Jack W. and Krotkov, Eric P. and Laabs, Tracy L. and Manzo, Justin E. and Pankratz, Kent G. and Pratt, Gill A. and Sanchez, Justin C. and Weber, Douglas J. and Wheeler, Tracey L. and Ling, Geoffrey S.F.},
  year = {2015},
  month = apr,
  volume = {244},
  pages = {52--67},
  issn = {01650270},
  doi = {10.1016/j.jneumeth.2014.07.019},
  abstract = {The Defense Advanced Research Projects Agency (DARPA) has funded innovative scientific research and technology developments in the field of brain\textendash computer interfaces (BCI) since the 1970s. This review highlights some of DARPA's major advances in the field of BCI, particularly those made in recent years. Two broad categories of DARPA programs are presented with respect to the ultimate goals of supporting the nation's warfighters: (1) BCI efforts aimed at restoring neural and/or behavioral function, and (2) BCI efforts aimed at improving human training and performance. The programs discussed are synergistic and complementary to one another, and, moreover, promote interdisciplinary collaborations among researchers, engineers, and clinicians. Finally, this review includes a summary of some of the remaining challenges for the field of BCI, as well as the goals of new DARPA efforts in this domain.},
  file = {/Users/x0r/Zotero/storage/RK26UJB8/Miranda et al_2015_DARPA-funded efforts in the development of novel brain–computer interface.pdf},
  journal = {Journal of Neuroscience Methods},
  keywords = {BMI},
  language = {en}
}

@article{mirowski2016,
  title = {Learning to {{Navigate}} in {{Complex Environments}}},
  author = {Mirowski, Piotr and Pascanu, Razvan and Viola, Fabio and Soyer, Hubert and Ballard, Andrew J. and Banino, Andrea and Denil, Misha and Goroshin, Ross and Sifre, Laurent and Kavukcuoglu, Koray and Kumaran, Dharshan and Hadsell, Raia},
  year = {2016},
  month = nov,
  abstract = {Learning to navigate in complex environments with dynamic elements is an important milestone in developing AI agents. In this work we formulate the navigation question as a reinforcement learning problem and show that data efficiency and task performance can be dramatically improved by relying on additional auxiliary tasks leveraging multimodal sensory inputs. In particular we consider jointly learning the goal-driven reinforcement learning problem with auxiliary depth prediction and loop closure classification tasks. This approach can learn to navigate from raw sensory input in complicated 3D mazes, approaching human-level performance even under conditions where the goal location changes frequently. We provide detailed analysis of the agent behaviour, its ability to localise, and its network activity dynamics, showing that the agent implicitly learns key navigation abilities.},
  archivePrefix = {arXiv},
  eprint = {1611.03673},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/D7BWTEV5/Mirowski et al_2016_Learning to Navigate in Complex Environments.pdf},
  journal = {arXiv:1611.03673 [cs]},
  keywords = {rl},
  primaryClass = {cs}
}

@article{mitra2010,
  title = {Extremely Low Drift of Resistance and Threshold Voltage in Amorphous Phase Change Nanowire Devices},
  author = {Mitra, Mukut and Jung, Yeonwoong and Gianola, Daniel S. and Agarwal, Ritesh},
  year = {2010},
  month = may,
  volume = {96},
  pages = {222111},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.3447941},
  abstract = {Time-dependent drift of resistance and threshold voltage in phase change memory (PCM) devices is of concern as it leads to data loss. Electrical drift in amorphous chalcogenides has been argued to be either due to electronic or stress relaxation mechanisms. Here we show that drift in amorphized Ge2Sb2Te5 nanowires with exposed surfaces is extremely low in comparison to thin-film devices. However, drift in stressed nanowires embedded under dielectric films is comparable to thin-films. Our results shows that drift in PCM is due to stress relaxation and will help in understanding and controlling drift in PCM devices.},
  file = {/Users/x0r/Zotero/storage/HPD2BIH5/Mitra et al_2010_Extremely low drift of resistance and threshold voltage in amorphous phase.pdf;/Users/x0r/Zotero/storage/YUCVVI26/Mitra et al_2010_Extremely low drift of resistance and threshold voltage in amorphous phase.pdf},
  journal = {Applied Physics Letters},
  keywords = {drift,PCM},
  language = {en},
  number = {22}
}

@article{mnih,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  pages = {9},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  file = {/Users/x0r/Zotero/storage/LNEWKSB3/Mnih et al_Playing Atari with Deep Reinforcement Learning.pdf},
  keywords = {rl,spinning-up},
  language = {en}
}

@article{Mnih_etal15,
  ids = {mnih2015},
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  year = {2015},
  volume = {518},
  pages = {529--533},
  publisher = {{[object Object]}},
  doi = {10.1038/nature14236},
  file = {/Users/x0r/Zotero/storage/6G5W5RKE/Mnih et al_2015_Human-level control through deep reinforcement learning.pdf;/Users/x0r/Zotero/storage/96WUS3WD/Mnih et al_2015_Human-level control through deep reinforcement learning.pdf},
  journal = {Nature},
  keywords = {rl},
  number = {7540}
}

@article{mnih2016,
  title = {Asynchronous {{Methods}} for {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P. and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  year = {2016},
  month = feb,
  abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
  archivePrefix = {arXiv},
  eprint = {1602.01783},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2URP76G9/Mnih et al_2016_Asynchronous Methods for Deep Reinforcement Learning.pdf},
  journal = {arXiv:1602.01783 [cs]},
  keywords = {rl,spinning-up},
  language = {en},
  primaryClass = {cs}
}

@article{moghadam2019,
  title = {Structure-{{Mechanical Stability Relations}} of {{Metal}}-{{Organic Frameworks}} via {{Machine Learning}}},
  author = {Moghadam, Peyman Z. and Rogge, Sven M.J. and Li, Aurelia and Chow, Chun-Man and Wieme, Jelle and Moharrami, Noushin and {Aragones-Anglada}, Marta and Conduit, Gareth and {Gomez-Gualdron}, Diego A. and Van Speybroeck, Veronique and {Fairen-Jimenez}, David},
  year = {2019},
  month = may,
  issn = {25902385},
  doi = {10.1016/j.matt.2019.03.002},
  abstract = {Assessing the mechanical stability of metal-organic frameworks (MOFs) is critical to bring these materials to any application. Here, we derive the first interactive map of the structure-mechanical landscape of MOFs by performing a multi-level computational analysis. First, we used high-throughput molecular simulations for 3,385 MOFs containing 41 distinct network topologies. Second, we developed a freely available machine-learning algorithm to automatically predict the mechanical properties of MOFs. For distinct regions of the high-throughput space, in-depth analysis based on in operando molecular dynamics simulations reveals the loss-of-crystallinity pressure within a given topology. The overarching mechanical screening approach presented here reveals the sensitivity on structural parameters such as topology, coordination characteristics and the nature of the building blocks, and paves the way for computational as well as experimental researchers to assess and design MOFs with enhanced mechanical stability to accelerate the translation of MOFs to industrial applications.},
  file = {/Users/x0r/Zotero/storage/JKLGM7QP/Moghadam et al_2019_Structure-Mechanical Stability Relations of Metal-Organic Frameworks via.pdf},
  journal = {Matter},
  keywords = {dl,material,nanotech},
  language = {en}
}

@article{momennejad2017,
  title = {The Successor Representation in Human Reinforcement Learning},
  author = {Momennejad, I. and Russek, E. M. and Cheong, J. H. and Botvinick, M. M. and Daw, N. D. and Gershman, S. J.},
  year = {2017},
  month = sep,
  volume = {1},
  pages = {680--692},
  issn = {2397-3374},
  doi = {10.1038/s41562-017-0180-8},
  file = {/Users/x0r/Zotero/storage/U4QL2S4M/Momennejad et al_2017_The successor representation in human reinforcement learning.pdf},
  journal = {Nature Human Behaviour},
  keywords = {neuroscience,rl},
  language = {en},
  number = {9}
}

@article{moon2019,
  title = {Temporal Data Classification and Forecasting Using a Memristor-Based Reservoir Computing System},
  author = {Moon, John and Ma, Wen and Shin, Jong Hoon and Cai, Fuxi and Du, Chao and Lee, Seung Hwan and Lu, Wei D.},
  year = {2019},
  month = oct,
  volume = {2},
  pages = {480--487},
  issn = {2520-1131},
  doi = {10.1038/s41928-019-0313-3},
  file = {/Users/x0r/Zotero/storage/MGAC7MEJ/Moon et al. - 2019 - Temporal data classification and forecasting using.pdf},
  journal = {Nature Electronics},
  keywords = {neuromorphic,reservoir-computing,SNN},
  language = {en},
  number = {10}
}

@article{moradi2018,
  title = {A {{Scalable Multicore Architecture With Heterogeneous Memory Structures}} for {{Dynamic Neuromorphic Asynchronous Processors}} ({{DYNAPs}})},
  author = {Moradi, Saber and Qiao, Ning and Stefanini, Fabio and Indiveri, Giacomo},
  year = {2018},
  month = feb,
  volume = {12},
  pages = {106--122},
  issn = {1932-4545, 1940-9990},
  doi = {10.1109/TBCAS.2017.2759700},
  abstract = {Neuromorphic computing systems comprise networks of neurons that use asynchronous events for both computation and communication. This type of representation offers several advantages in terms of bandwidth and power consumption in neuromorphic electronic systems. However, managing the traffic of asynchronous events in large scale systems is a daunting task, both in terms of circuit complexity and memory requirements. Here, we present a novel routing methodology that employs both hierarchical and mesh routing strategies and combines heterogeneous memory structures for minimizing both memory requirements and latency, while maximizing programming flexibility to support a wide range of event-based neural network architectures, through parameter configuration. We validated the proposed scheme in a prototype multicore neuromorphic processor chip that employs hybrid analog/digital circuits for emulating synapse and neuron dynamics together with asynchronous digital circuits for managing the address-event traffic. We present a theoretical analysis of the proposed connectivity scheme, describe the methods and circuits used to implement such scheme, and characterize the prototype chip. Finally, we demonstrate the use of the neuromorphic processor with a convolutional neural network for the real-time classification of visual symbols being flashed to a dynamic vision sensor (DVS) at high speed.},
  file = {/Users/x0r/Zotero/storage/NBAED3VS/Moradi et al_2018_A Scalable Multicore Architecture With Heterogeneous Memory Structures for.pdf},
  journal = {IEEE Transactions on Biomedical Circuits and Systems},
  keywords = {neuromorphic,VLSI},
  language = {en},
  number = {1}
}

@article{moraitis2017,
  title = {Fatiguing {{STDP}}: {{Learning}} from {{Spike}}-{{Timing Codes}} in the {{Presence}} of {{Rate Codes}}},
  shorttitle = {Fatiguing {{STDP}}},
  author = {Moraitis, Timoleon and Sebastian, Abu and Boybat, Irem and Gallo, Manuel Le and Tuma, Tomas and Eleftheriou, Evangelos},
  year = {2017},
  month = jun,
  abstract = {Spiking neural networks (SNNs) could play a key role in unsupervised machine learning applications, by virtue of strengths related to learning from the fine temporal structure of event-based signals. However, some spike-timing-related strengths of SNNs are hindered by the sensitivity of spike-timingdependent plasticity (STDP) rules to input spike rates, as fine temporal correlations may be obstructed by coarser correlations between firing rates. In this article, we propose a spike-timingdependent learning rule that allows a neuron to learn from the temporally-coded information despite the presence of rate codes. Our long-term plasticity rule makes use of short-term synaptic fatigue dynamics. We show analytically that, in contrast to conventional STDP rules, our fatiguing STDP (FSTDP) helps learn the temporal code, and we derive the necessary conditions to optimize the learning process. We showcase the effectiveness of FSTDP in learning spike-timing correlations among processes of different rates in synthetic data. Finally, we use FSTDP to detect correlations in real-world weather data from the United States in an experimental realization of the algorithm that uses a neuromorphic hardware platform comprising phase-change memristive devices. Taken together, our analyses and demonstrations suggest that FSTDP paves the way for the exploitation of the spike-based strengths of SNNs in real-world applications.},
  archivePrefix = {arXiv},
  eprint = {1706.05563},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/6J9NIYXS/Moraitis et al_2017_Fatiguing STDP.pdf},
  journal = {arXiv:1706.05563 [cs, stat]},
  keywords = {STDP},
  language = {en},
  primaryClass = {cs, stat}
}

@article{moskovitz2018,
  title = {Feedback Alignment in Deep Convolutional Networks},
  author = {Moskovitz, Theodore H. and {Litwin-Kumar}, Ashok and Abbott, L. F.},
  year = {2018},
  month = dec,
  abstract = {Ongoing studies have identified similarities between neural representations in biological networks and in deep artificial neural networks. This has led to renewed interest in developing analogies between the backpropagation learning algorithm used to train artificial networks and the synaptic plasticity rules operative in the brain. These efforts are challenged by biologically implausible features of backpropagation, one of which is a reliance on symmetric forward and backward synaptic weights. A number of methods have been proposed that do not rely on weight symmetry but, thus far, these have failed to scale to deep convolutional networks and complex data. We identify principal obstacles to the scalability of such algorithms and introduce several techniques to mitigate them. We demonstrate that a modification of the feedback alignment method that enforces a weaker form of weight symmetry, one that requires agreement of weight sign but not magnitude, can achieve performance competitive with backpropagation. Our results complement those of Bartunov et al. (2018) and Xiao et al. (2018b) and suggest that mechanisms that promote alignment of feedforward and feedback weights are critical for learning in deep networks.},
  archivePrefix = {arXiv},
  eprint = {1812.06488},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AJ77AC7R/Moskovitz et al_2018_Feedback alignment in deep convolutional networks.pdf},
  journal = {arXiv:1812.06488 [cs, stat]},
  keywords = {neuroscience,SNN,To read},
  primaryClass = {cs, stat}
}

@article{mozafari2019,
  title = {{{SpykeTorch}}: {{Efficient Simulation}} of {{Convolutional Spiking Neural Networks With}} at {{Most One Spike}} per {{Neuron}}},
  shorttitle = {{{SpykeTorch}}},
  author = {Mozafari, Milad and Ganjtabesh, Mohammad and {Nowzari-Dalini}, Abbas and Masquelier, Timoth{\'e}e},
  year = {2019},
  month = jul,
  volume = {13},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00625},
  abstract = {Application of deep convolutional spiking neural networks (SNNs) to artificial intelligence (AI) tasks has recently gained a lot of interest since SNNs are hardware-friendly and energy-efficient. Unlike the non-spiking counterparts, most of the existing SNN simulation frameworks are not practically efficient enough for large-scale AI tasks. In this paper, we introduce SpykeTorch, an open-source high-speed simulation framework based on PyTorch. This framework simulates convolutional SNNs with at most one spike per neuron and the rank-order encoding scheme. In terms of learning rules, both spike-timing-dependent plasticity (STDP) and reward-modulated STDP (R-STDP) are implemented, but other rules could be implemented easily. Apart from the aforementioned properties, SpykeTorch is highly generic and capable of reproducing the results of various studies. Computations in the proposed framework are tensor-based and totally done by PyTorch functions, which in turn brings the ability of just-in-time optimization for running on CPUs, GPUs, or Multi-GPU platforms.},
  file = {/Users/x0r/Zotero/storage/AY3LBTCZ/Mozafari et al_2019_SpykeTorch.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic,SNN},
  language = {en}
}

@article{mugan2019,
  title = {The Shift to Life on Land Selected for Planning},
  author = {Mugan, Ugurcan and MacIver, Malcolm A.},
  year = {2019},
  month = mar,
  pages = {585760},
  doi = {10.1101/585760},
  abstract = {{$<$}p{$>$}It is uncontroversial that land animals have developed more elaborated cognitive abilities than aquatic animals, with the possible exception of formerly land-based mammals like whales and dolphins that have returned to an aquatic existence. Yet there is no apparent a-priori reason for aquatic and land animals to have different evolutionary capacities for the rise of cognition. One possibility is that investigators have been anthropocentric in their definition of cognition, rather than attuned to cognitive phenomena as appropriate to the broader ethological and ecological context of each species. However, this concern may not apply to the paradigmatically cognitive faculty of being able to imagine multiple complete sequences of actions to accomplish a goal and select one to enact, or planning. Although planning over space requires cognitive maps--which exist in many species including fish--behavioral and neural evidence for planning is presently restricted to birds and mammals. Here, we present evidence that a reason for the absence of planning in fish and many other aquatic animals is that in a key driver of natural selection, predator-prey interactions, there is no benefit to planning above habit-based action selection. In contrast, there is a significant benefit of planning under similar predator-prey scenarios for terrestrial conditions. This effect is dependent on a combination of increased visual range and the spatial complexity of terrestrial environments. The ability to plan in select terrestrial vertebrates may therefore be an adaptive response to the massive increase in visual sensory range (100x) occurring with the shift to life on land in combination with the complexity of terrestrial habitats and its affordance of strategic behavior during predator-prey interactions.{$<$}/p{$>$}},
  copyright = {\textcopyright{} 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  file = {/Users/x0r/Zotero/storage/H9ACW72Z/Mugan_MacIver_2019_The shift to life on land selected for planning.pdf},
  journal = {bioRxiv},
  keywords = {neuroscience},
  language = {en}
}

@article{munos2016,
  title = {Safe and {{Efficient Off}}-{{Policy Reinforcement Learning}}},
  author = {Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc G.},
  year = {2016},
  month = jun,
  abstract = {In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace({$\lambda$}), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of ``off-policyness''; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q{${_\ast}$} without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q({$\lambda$}), which was an open problem since 1989. We illustrate the benefits of Retrace({$\lambda$}) on a standard suite of Atari 2600 games.},
  archivePrefix = {arXiv},
  eprint = {1606.02647},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/JHCBBZKY/Munos et al_2016_Safe and Efficient Off-Policy Reinforcement Learning.pdf},
  journal = {arXiv:1606.02647 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@book{murphy2012,
  title = {Machine Learning: A Probabilistic Perspective},
  shorttitle = {Machine Learning},
  author = {Murphy, Kevin P.},
  year = {2012},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  file = {/Users/x0r/Zotero/storage/WLHZKKSF/Murphy_2012_Machine learning.pdf},
  isbn = {978-0-262-01802-9},
  language = {en},
  lccn = {Q325.5 .M87 2012},
  series = {Adaptive Computation and Machine Learning Series}
}

@article{murray,
  title = {Local Online Learning in Recurrent Networks with Random Feedback},
  author = {Murray, James M},
  pages = {25},
  file = {/Users/x0r/Zotero/storage/QT8F4UPB/Murray_Local online learning in recurrent networks with random feedback.pdf},
  keywords = {neuroscience,SNN},
  language = {en}
}

@article{nagabandi2017,
  title = {Neural {{Network Dynamics}} for {{Model}}-{{Based Deep Reinforcement Learning}} with {{Model}}-{{Free Fine}}-{{Tuning}}},
  author = {Nagabandi, Anusha and Kahn, Gregory and Fearing, Ronald S. and Levine, Sergey},
  year = {2017},
  month = aug,
  abstract = {Model-free deep reinforcement learning algorithms have been shown to be capable of learning a wide range of robotic skills, but typically require a very large number of samples to achieve good performance. Model-based algorithms, in principle, can provide for much more efficient learning, but have proven difficult to extend to expressive, high-capacity models such as deep neural networks. In this work, we demonstrate that medium-sized neural network models can in fact be combined with model predictive control (MPC) to achieve excellent sample complexity in a model-based reinforcement learning algorithm, producing stable and plausible gaits to accomplish various complex locomotion tasks. We also propose using deep neural network dynamics models to initialize a model-free learner, in order to combine the sample efficiency of model-based approaches with the high task-specific performance of model-free methods. We empirically demonstrate on MuJoCo locomotion tasks that our pure model-based approach trained on just random action data can follow arbitrary trajectories with excellent sample efficiency, and that our hybrid algorithm can accelerate model-free learning on high-speed benchmark tasks, achieving sample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents. Videos can be found at https://sites.google.com/view/mbmf},
  archivePrefix = {arXiv},
  eprint = {1708.02596},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/5CARUDIA/Nagabandi et al_2017_Neural Network Dynamics for Model-Based Deep Reinforcement Learning with.pdf},
  journal = {arXiv:1708.02596 [cs]},
  keywords = {rl,spinning-up},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{Nair_Dudek15,
  ids = {nair2015},
  title = {Gradient-Descent-Based Learning in Memristive Crossbar Arrays},
  booktitle = {Neural Networks ({{IJCNN}}), 2015 International Joint Conference On},
  author = {Nair, Manu V and Dudek, Piotr},
  year = {2015},
  pages = {1--7},
  file = {/Users/x0r/Zotero/storage/X5MUUZ6Y/Nair_Dudek_2015_Gradient-descent-based learning in memristive crossbar arrays.pdf},
  keywords = {backprop,neuromorphic},
  organization = {{[object Object]}}
}

@article{nair2017,
  title = {A Differential Memristive Synapse Circuit for On-Line Learning in Neuromorphic Computing Systems},
  author = {Nair, Manu V. and Muller, Lorenz K. and Indiveri, Giacomo},
  year = {2017},
  month = nov,
  volume = {1},
  pages = {035003},
  issn = {2399-1984},
  doi = {10.1088/2399-1984/aa954a},
  abstract = {Spike-based learning with memristive devices in neuromorphic computing architectures typically uses learning circuits that require overlapping pulses from preand post-synaptic nodes. This imposes severe constraints on the length of the pulses transmitted in the network, and on the network's throughput. Furthermore, most of these circuits do not decouple the currents flowing through memristive devices from the one stimulating the target neuron. This can be a problem when using devices with high conductance values, because of the resulting large currents. In this paper we propose a novel circuit that decouples the current produced by the memristive device from the one used to stimulate the post-synaptic neuron, by using a novel differential scheme based on the Gilbert normalizer circuit. We show how this circuit is useful for reducing the effect of variability in the memristive devices, and how it is ideally suited for spike-based learning mechanisms that do not require overlapping pre- and post-synaptic pulses. We demonstrate the features of the proposed synapse circuit with SPICE simulations, and validate its learning properties with high-level behavioral network simulations which use a stochastic gradient descent learning rule in two classification tasks.},
  archivePrefix = {arXiv},
  eprint = {1709.05484},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/H5E6ZLV3/Nair et al_2017_A differential memristive synapse circuit for on-line learning in neuromorphic.pdf},
  journal = {Nano Futures},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {3}
}

@article{nair2019,
  title = {An Ultra-Low-Power Sigma-Delta Neuron Circuit},
  author = {Nair, Manu V. and Indiveri, Giacomo},
  year = {2019},
  month = may,
  pages = {1--5},
  doi = {10.1109/ISCAS.2019.8702500},
  abstract = {Neural processing systems typically represent data using leaky integrate and fire (LIF) neuron models that generate spikes or pulse trains at a rate proportional to their input amplitudes. This mechanism requires high firing rates when encoding time-varying signals, leading to increased power consumption. Neuromorphic systems that use adaptive LIF neuron models overcome this problem by encoding signals in the relative timing of their output spikes rather than their rate. In this paper, we analyze recent adaptive LIF neuron circuit implementations and highlight the analogies and differences between them and a first-order sigma-delta feedback loop. We propose a new sigma-delta neuron circuit that addresses some of the limitations in existing implementations and present simulation results that quantify the improvements. We show that the new circuit, implemented in a 1.8 V, 180 nm CMOS process, offers up to 42 dB signal-to-distortion ratio and consumes orders of magnitude lower energy. Finally, we also demonstrate how the sigma-delta interpretation enables mapping of real-valued recurrent neural network to the spiking framework to emphasize the envisioned application of the proposed circuit.},
  archivePrefix = {arXiv},
  eprint = {1902.07149},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/XMRXFZMI/Nair_Indiveri_2019_An ultra-low-power sigma-delta neuron circuit.pdf},
  journal = {2019 IEEE International Symposium on Circuits and Systems (ISCAS)},
  keywords = {neuromorphic}
}

@article{nair2019a,
  title = {A Neuromorphic Boost to {{RNNs}} Using Low Pass Filters},
  author = {Nair, Manu V. and Indiveri, Giacomo},
  year = {2019},
  month = may,
  abstract = {The increasing difficulty with Moore's law scaling and the remarkable success of machine learning have triggered a renaissance in the study of low-latency, energyefficient accelerators for machine learning applications. In particular, spiking neural networks (SNNs) and their neuromorphic hardware implementations have started to receive substantial attention from academia and industry. However, SNNs perform relatively poorly compared to their rate-based counterparts, in terms of accuracy in pattern recognition tasks. In this paper, we present a low pass recurrent neural network (lpRNN) cell that can be trained using backpropagation and implemented on neuromorphic SNN devices. The ability to implement our model on neuromorphic hardware enables the construction of compact devices that can perform always-on processing in ultra-low power edge computing applications. We further argue that the low pass filter is a temporal regularizer and highlight its advantage in a Long Short-Term Memory (LSTM) cell. We show that low pass RNNs are able to learn and generalize better than their unfiltered variants on two long memory synthetic tasks, a character-level text modeling task, and a neuromorphic spoken command detection system.},
  archivePrefix = {arXiv},
  eprint = {1905.10692},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/32Q8ZV76/Nair_Indiveri_2019_A neuromorphic boost to RNNs using low pass filters.pdf},
  journal = {arXiv:1905.10692 [cs, eess]},
  keywords = {neuromorphic,RNN},
  language = {en},
  primaryClass = {cs, eess}
}

@article{nandakumar2018,
  title = {A Phase-Change Memory Model for Neuromorphic Computing},
  author = {Nandakumar, S. R. and Le Gallo, Manuel and Boybat, Irem and Rajendran, Bipin and Sebastian, Abu and Eleftheriou, Evangelos},
  year = {2018},
  month = oct,
  volume = {124},
  pages = {152135},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.5042408},
  file = {/Users/x0r/Zotero/storage/5KYGJLAM/Nandakumar et al_2018_A phase-change memory model for neuromorphic computing.pdf},
  journal = {Journal of Applied Physics},
  keywords = {PCM},
  language = {en},
  number = {15}
}

@article{nandakumar2020,
  title = {Experimental {{Demonstration}} of {{Supervised Learning}} in {{Spiking Neural Networks}} with {{Phase}}-{{Change Memory Synapses}}},
  author = {Nandakumar, S. R. and Boybat, Irem and Gallo, Manuel Le and Eleftheriou, Evangelos and Sebastian, Abu and Rajendran, Bipin},
  year = {2020},
  month = may,
  volume = {10},
  pages = {1--11},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-64878-5},
  abstract = {Spiking neural networks (SNN) are computational models inspired by the brain's ability to naturally encode and process information in the time domain. The added temporal dimension is believed to render them more computationally efficient than the conventional artificial neural networks, though their full computational capabilities are yet to be explored. Recently, in-memory computing architectures based on non-volatile memory crossbar arrays have shown great promise to implement parallel computations in artificial and spiking neural networks. In this work, we evaluate the feasibility to realize high-performance event-driven in-situ supervised learning systems using nanoscale and stochastic analog memory synapses. For the first time, the potential of analog memory synapses to generate precisely timed spikes in SNNs is experimentally demonstrated. The experiment targets applications which directly integrates spike encoded signals generated from bio-mimetic sensors with in-memory computing based learning systems to generate precisely timed control signal spikes for neuromorphic actuators. More than 170,000 phase-change memory (PCM) based synapses from our prototype chip were trained based on an event-driven learning rule, to generate spike patterns with more than 85\% of the spikes within a 25\,ms tolerance interval in a 1250\,ms long spike pattern. We observe that the accuracy is mainly limited by the imprecision related to device programming and temporal drift of conductance values. We show that an array level scaling scheme~can significantly improve the retention of the trained SNN states in the presence of conductance drift in the PCM. Combining the computational potential of supervised SNNs with the parallel compute power of in-memory computing, this work paves the way for next-generation of efficient brain-inspired systems.},
  copyright = {2020 The Author(s)},
  file = {/Users/x0r/Zotero/storage/SIZE32QD/Nandakumar et al. - 2020 - Experimental Demonstration of Supervised Learning .pdf;/Users/x0r/Zotero/storage/2ZSSD6C2/s41598-020-64878-5.html},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{narayanan2017,
  title = {Toward On-Chip Acceleration of the Backpropagation Algorithm Using Nonvolatile Memory},
  author = {Narayanan, P. and Fumarola, A. and Sanches, L. L. and Hosokawa, K. and Lewis, S. C. and Shelby, R. M. and Burr, G. W.},
  year = {2017},
  month = jul,
  volume = {61},
  pages = {11:1-11:11},
  issn = {0018-8646, 0018-8646},
  doi = {10.1147/JRD.2017.2716579},
  file = {/Users/x0r/Zotero/storage/2B3R4BVF/Narayanan et al_2017_Toward on-chip acceleration of the backpropagation algorithm using nonvolatile.pdf},
  journal = {IBM Journal of Research and Development},
  keywords = {backprop,neuromorphic},
  language = {en},
  number = {4/5}
}

@inproceedings{narayanan2017a,
  title = {Reducing Circuit Design Complexity for Neuromorphic Machine Learning Systems Based on {{Non}}-{{Volatile Memory}} Arrays},
  booktitle = {2017 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Narayanan, Pritish and Sanches, Lucas L. and Fumarola, Alessandro and Shelby, Robert M. and Ambrogio, Stefano and Jang, Junwoo and Hwang, Hyunsang and Leblebici, Yusuf and Burr, Geoffrey W.},
  year = {2017},
  month = may,
  pages = {1--4},
  publisher = {{IEEE}},
  address = {{Baltimore, MD, USA}},
  doi = {10.1109/ISCAS.2017.8050988},
  abstract = {Machine Learning (ML) is an attractive application of Non-Volatile Memory (NVM) arrays [1,2]. However, achieving speedup over GPUs will require minimal neuron circuit sharing and thus highly area-efficient peripheral circuitry, so that ML reads and writes are massively parallel and time-multiplexing is minimized [2]. This means that neuron hardware offering full `software-equivalent' functionality is impractical. We analyze neuron circuit needs for implementing back-propagation in NVM arrays and introduce approximations to reduce design complexity and area. We discuss the interplay between circuits and NVM devices, such as the need for an occasional RESET step, the number of programming pulses to use, and the stochastic nature of NVM conductance change. In all cases we show that by leveraging the resilience of the algorithm to error, we can use practical circuit approaches yet maintain competitive test accuracies on ML benchmarks.},
  file = {/Users/x0r/Zotero/storage/26QJVZXR/Narayanan et al. - 2017 - Reducing circuit design complexity for neuromorphi.pdf},
  isbn = {978-1-4673-6853-7},
  language = {en}
}

@article{naud2018,
  title = {Sparse Bursts Optimize Information Transmission in a Multiplexed Neural Code},
  author = {Naud, Richard and Sprekeler, Henning},
  year = {2018},
  month = jul,
  volume = {115},
  pages = {E6329-E6338},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1720995115},
  abstract = {Many cortical neurons combine the information ascending and descending the cortical hierarchy. In the classical view, this information is combined nonlinearly to give rise to a single firing-rate output, which collapses all input streams into one. We analyze the extent to which neurons can simultaneously represent multiple input streams by using a code that distinguishes spike timing patterns at the level of a neural ensemble. Using computational simulations constrained by experimental data, we show that cortical neurons are well suited to generate such multiplexing. Interestingly, this neural code maximizes information for short and sparse bursts, a regime consistent with in vivo recordings. Neurons can also demultiplex this information, using specific connectivity patterns. The anatomy of the adult mammalian cortex suggests that these connectivity patterns are used by the nervous system to maintain sparse bursting and optimal multiplexing. Contrary to firing-rate coding, our findings indicate that the physiology and anatomy of the cortex may be interpreted as optimizing the transmission of multiple independent signals to different targets.},
  file = {/Users/x0r/Zotero/storage/E3USUKTL/Naud and Sprekeler - 2018 - Sparse bursts optimize information transmission in.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {27}
}

@article{nayebi2018,
  title = {Task-{{Driven Convolutional Recurrent Models}} of the {{Visual System}}},
  author = {Nayebi, Aran and Bear, Daniel and Kubilius, Jonas and Kar, Kohitij and Ganguli, Surya and Sussillo, David and DiCarlo, James J. and Yamins, Daniel L. K.},
  year = {2018},
  month = jun,
  abstract = {Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.},
  archivePrefix = {arXiv},
  eprint = {1807.00053},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/VX4NN439/Nayebi et al_2018_Task-Driven Convolutional Recurrent Models of the Visual System.pdf},
  journal = {arXiv:1807.00053 [cs, q-bio]},
  keywords = {dl,neuroscience},
  primaryClass = {cs, q-bio}
}

@article{neftci2019,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O. and Mostafa, Hesham and Zenke, Friedemann},
  year = {2019},
  month = may,
  abstract = {Spiking neural networks are nature's versatile solution to fault-tolerant and energy efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking neural network processors attempt to emulate biological neural networks. These developments have created an imminent need for methods and tools to enable such systems to solve realworld signal processing problems. Like conventional neural networks, spiking neural networks can be trained on real, domain specific data. However, their training requires overcoming a number of challenges linked to their binary and dynamical nature. This article elucidates stepby-step the problems typically encountered when training spiking neural networks, and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. To that end, it gives an overview of existing approaches and provides an introduction to surrogate gradient methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.},
  archivePrefix = {arXiv},
  eprint = {1901.09948},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2BLTSBX6/Neftci et al. - 2019 - Surrogate Gradient Learning in Spiking Neural Netw.pdf},
  journal = {arXiv:1901.09948 [cs, q-bio]},
  keywords = {neuromorphic,SNN},
  language = {en},
  primaryClass = {cs, q-bio}
}

@article{neftci2019a,
  title = {Reinforcement Learning in Artificial and Biological Systems},
  author = {Neftci, Emre O. and Averbeck, Bruno B.},
  year = {2019},
  month = mar,
  volume = {1},
  pages = {133--143},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0025-4},
  file = {/Users/x0r/Zotero/storage/W9VJKGET/Neftci and Averbeck - 2019 - Reinforcement learning in artificial and biologica.pdf},
  journal = {Nature Machine Intelligence},
  keywords = {To read},
  language = {en},
  number = {3}
}

@article{neil2016,
  title = {Phased {{LSTM}}: {{Accelerating Recurrent Network Training}} for {{Long}} or {{Event}}-Based {{Sequences}}},
  author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
  year = {2016},
  pages = {9},
  abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. However, current RNN models are ill-suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors that generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range that produces updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes at runtime.},
  file = {/Users/x0r/Zotero/storage/S3DNQYRE/Neil et al_2016_Phased LSTM.pdf},
  keywords = {dl},
  language = {en}
}

@article{neumann1945,
  title = {First {{Draft}} of a {{Report}} on the {{EDVAC}}},
  author = {Neumann, John Von},
  year = {1945},
  pages = {49},
  file = {/Users/x0r/Zotero/storage/NWN7UUJU/NEUMANN_First Draft of a Report on the EDVAC.pdf},
  keywords = {von-neumann},
  language = {en}
}

@article{nicola2017,
  title = {Supervised {{Learning}} in {{Spiking Neural Networks}} with {{FORCE Training}}},
  author = {Nicola, Wilten and Clopath, Claudia},
  year = {2017},
  month = dec,
  volume = {8},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-01827-3},
  abstract = {Populations of neurons display an extraordinary diversity in the types of problems they solve and behaviors they display. Techniques have recently emerged that allow us to create networks of model neurons that solve tasks of similar complexity. Examples include the FORCE method, a novel technique that harnesses chaos to perform computations. We demonstrate the direct applicability of FORCE training to spiking neurons by training networks to mimic various dynamical systems in addition to reproduce more elaborate tasks such as input classification, storing sequences, reproducing the singing behavior of songbirds, and recalling a scene from a movie. Post-training network analysis reveals behaviors that are consistent with electrophysiological data, such as the stereotypical decrease in voltage variance upon input presentation, reproducing firing rate distributions from songbird data, and reproducing locations of incorrect recall in sequence replay. Finally, we demonstrate that theta oscillations are critical for both learning and recall of episodic memories.},
  archivePrefix = {arXiv},
  eprint = {1609.02545},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/7AA4QWI7/Nicola_Clopath_2017_Supervised Learning in Spiking Neural Networks with FORCE Training.pdf},
  journal = {Nature Communications},
  keywords = {SNN},
  language = {en},
  number = {1}
}

@article{nielsen2016,
  title = {Genetic Circuit Design Automation},
  author = {Nielsen, A. A. K. and Der, B. S. and Shin, J. and Vaidyanathan, P. and Paralanov, V. and Strychalski, E. A. and Ross, D. and Densmore, D. and Voigt, C. A.},
  year = {2016},
  month = apr,
  volume = {352},
  pages = {aac7341-aac7341},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac7341},
  file = {/Users/x0r/Zotero/storage/MFS6N89X/Nielsen et al. - 2016 - Genetic circuit design automation.pdf},
  journal = {Science},
  language = {en},
  number = {6281}
}

@article{niv,
  title = {Reinforcement Learning in the Brain},
  author = {Niv, Yael},
  pages = {38},
  abstract = {A wealth of research focuses on the decision-making processes that animals and humans employ when selecting actions in the face of reward and punishment. Initially such work stemmed from psychological investigations of conditioned behavior, and explanations of these in terms of computational models. Increasingly, analysis at the computational level has drawn on ideas from reinforcement learning, which provide a normative framework within which decision-making can be analyzed. More recently, the fruits of these extensive lines of research have made contact with investigations into the neural basis of decision making. Converging evidence now links reinforcement learning to specific neural substrates, assigning them precise computational roles. Specifically, electrophysiological recordings in behaving animals and functional imaging of human decision-making have revealed in the brain the existence of a key reinforcement learning signal, the temporal difference reward prediction error. Here, we first introduce the formal reinforcement learning framework. We then review the multiple lines of evidence linking reinforcement learning to the function of dopaminergic neurons in the mammalian midbrain and to more recent data from human imaging experiments. We further extend the discussion to aspects of learning not associated with phasic dopamine signals, such as learning of goal-directed responding that may not be dopamine-dependent, and learning about the vigor (or rate) with which actions should be performed that has been linked to tonic aspects of dopaminergic signaling. We end with a brief discussion of some of the limitations of the reinforcement learning framework, highlighting questions for future research.},
  file = {/Users/x0r/Zotero/storage/H9PV44NF/Niv_Reinforcement learning in the brain.pdf},
  keywords = {neuroscience,rl,To read},
  language = {en}
}

@article{nokland2019,
  title = {Training {{Neural Networks}} with {{Local Error Signals}}},
  author = {N{\o}kland, Arild and Eidnes, Lars Hiller},
  year = {2019},
  month = jan,
  abstract = {Supervised training of neural networks for classification is typically performed with a global loss function. The loss function provides a gradient for the output layer, and this gradient is back-propagated to hidden layers to dictate an update direction for the weights. An alternative approach is to train the network with layer-wise loss functions. In this paper we demonstrate, for the first time, that layer-wise training can approach the state-of-the-art on a variety of image datasets. We use single-layer sub-networks and two different supervised loss functions to generate local error signals for the hidden layers, and we show that the combination of these losses help with optimization in the context of local learning. Using local errors could be a step towards more biologically plausible deep learning because the global error does not have to be transported back to hidden layers. A completely backprop free variant outperforms previously reported results among methods aiming for higher biological plausibility. Code is available https://github.com/anokland/local-loss},
  archivePrefix = {arXiv},
  eprint = {1901.06656},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/DCCFH286/Nøkland_Eidnes_2019_Training Neural Networks with Local Error Signals.pdf},
  journal = {arXiv:1901.06656 [cs, stat]},
  keywords = {neuromorphic},
  primaryClass = {cs, stat}
}

@book{nowak2006,
  title = {Evolutionary Dynamics: Exploring the Equations of Life},
  shorttitle = {Evolutionary Dynamics},
  author = {Nowak, M. A.},
  year = {2006},
  publisher = {{Belknap Press of Harvard University Press}},
  address = {{Cambridge, Mass}},
  file = {/Users/x0r/Zotero/storage/UBTTRP8G/Nowak_2006_Evolutionary dynamics.pdf},
  isbn = {978-0-674-02338-3},
  keywords = {evolution},
  lccn = {QH371.3.M37 N69 2006}
}

@article{nowak2008,
  title = {Prevolutionary Dynamics and the Origin of Evolution},
  author = {Nowak, M. A. and Ohtsuki, H.},
  year = {2008},
  month = sep,
  volume = {105},
  pages = {14924--14927},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.0806714105},
  file = {/Users/x0r/Zotero/storage/MSRD8AUV/Nowak_Ohtsuki_2008_Prevolutionary dynamics and the origin of evolution.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {evolution},
  language = {en},
  number = {39}
}

@unpublished{nvidia2016,
  title = {{{NVIDIA Tesla P100 Whitepaper}}},
  author = {NVIDIA},
  year = {2016},
  file = {/Users/x0r/Zotero/storage/WYY8TE2A/GP100 Pascal Whitepaper.pdf},
  keywords = {GPU},
  language = {en}
}

@article{ocko2018,
  title = {Emergent Elasticity in the Neural Code for Space},
  author = {Ocko, Samuel A. and Hardcastle, Kiah and Giocomo, Lisa M. and Ganguli, Surya},
  year = {2018},
  month = nov,
  pages = {201805959},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1805959115},
  abstract = {Upon encountering a novel environment, an animal must construct a consistent environmental map, as well as an internal estimate of its position within that map, by combining information from two distinct sources: self-motion cues and sensory landmark cues. How do known aspects of neural circuit dynamics and synaptic plasticity conspire to accomplish this feat? Here we show analytically how a neural attractor model that combines path integration of self-motion cues with Hebbian plasticity in synaptic weights from landmark cells can self-organize a consistent map of space as the animal explores an environment. Intriguingly, the emergence of this map can be understood as an elastic relaxation process between landmark cells mediated by the attractor network. Moreover, our model makes several experimentally testable predictions, including (i) systematic path-dependent shifts in the firing fields of grid cells toward the most recently encountered landmark, even in a fully learned environment; (ii) systematic deformations in the firing fields of grid cells in irregular environments, akin to elastic deformations of solids forced into irregular containers; and (iii) the creation of topological defects in grid cell firing patterns through specific environmental manipulations. Taken together, our results conceptually link known aspects of neurons and synapses to an emergent solution of a fundamental computational problem in navigation, while providing a unified account of disparate experimental observations.},
  copyright = {\textcopyright{} 2018 . Published under the PNAS license.},
  file = {/Users/x0r/Zotero/storage/L2A5R54Y/Ocko et al_2018_Emergent elasticity in the neural code for space.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {neuroscience},
  language = {en},
  pmid = {30482856}
}

@article{ohshima1996,
  title = {Crystallization of Germanium\textendash Antimony\textendash Tellurium Amorphous Thin Film Sandwiched between Various Dielectric Protective Films},
  author = {Ohshima, Norikazu},
  year = {1996},
  month = jun,
  volume = {79},
  pages = {8357--8363},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.362548},
  file = {/Users/x0r/Zotero/storage/2F2Y47T4/Ohshima_1996_Crystallization of germanium–antimony–tellurium amorphous thin film sandwiched.pdf},
  journal = {Journal of Applied Physics},
  keywords = {GST},
  language = {en},
  number = {11}
}

@article{okuto1975,
  title = {Threshold Energy Effect on Avalanche Breakdown Voltage in Semiconductor Junctions},
  author = {Okuto, Y. and Crowell, C. R.},
  year = {1975},
  month = feb,
  volume = {18},
  pages = {161--168},
  issn = {0038-1101},
  doi = {10.1016/0038-1101(75)90099-4},
  abstract = {The band bending for avalanche breakdown in semiconductor junctions and its temperature dependence are predicted taking account of threshold energy effects on the ionization process in semiconductors. Where experimental results exist, the theoretical predictions and experimental results are in excellent agreement. In the high electric field region inclusion of both bulk and boundary threshold energy effects is essential. The predictions were based on exact solutions in the nonlocalized ionization coefficient formulation developed by Okuto and Crowell who showed that ionization coefficients as usually understood are functions of both electric field and position in a device. Predictions for abrupt and p-i-n junctions in Ge, Si, GaAs and GaP are presented.},
  journal = {Solid-State Electronics},
  keywords = {OTS},
  number = {2}
}

@article{olafsdottir2015,
  title = {Hippocampal Place Cells Construct Reward Related Sequences through Unexplored Space},
  author = {{\'O}lafsd{\'o}ttir, H Freyja and Barry, Caswell and Saleem, Aman B and Hassabis, Demis and Spiers, Hugo J},
  year = {2015},
  month = jun,
  volume = {4},
  issn = {2050-084X},
  doi = {10.7554/eLife.06063},
  file = {/Users/x0r/Zotero/storage/GWN75MSL/Ólafsdóttir et al_2015_Hippocampal place cells construct reward related sequences through unexplored.pdf},
  journal = {eLife},
  keywords = {neuroscience,To read},
  language = {en}
}

@article{omidshafiei2019,
  title = {{$\alpha$}- {{Rank}}: {{Multi}}-{{Agent Evaluation}} by {{Evolution}}},
  shorttitle = {{$\alpha$}- {{Rank}}},
  author = {Omidshafiei, Shayegan and Papadimitriou, Christos and Piliouras, Georgios and Tuyls, Karl and Rowland, Mark and Lespiau, Jean-Baptiste and Czarnecki, Wojciech M. and Lanctot, Marc and Perolat, Julien and Munos, Remi},
  year = {2019},
  month = jul,
  volume = {9},
  pages = {1--29},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-45619-9},
  abstract = {We introduce {$\alpha$}-Rank, a principled evolutionary dynamics methodology, for the evaluation and ranking of agents in large-scale multi-agent interactions, grounded in a novel dynamical game-theoretic solution concept called Markov-Conley chains (MCCs). The approach leverages continuous-time and discrete-time evolutionary dynamical systems applied to empirical games, and scales tractably in the number of agents, in the type of interactions (beyond dyadic), and the type of empirical games (symmetric and asymmetric). Current models are fundamentally limited in one or more of these dimensions, and are not guaranteed to converge to the desired game-theoretic solution concept (typically the Nash equilibrium). {$\alpha$}-Rank automatically provides a ranking over the set of agents under evaluation and provides insights into their strengths, weaknesses, and long-term dynamics in terms of basins of attraction and sink components. This is a direct consequence of the correspondence we establish to the dynamical MCC solution concept when the underlying evolutionary model's ranking-intensity parameter, {$\alpha$}, is chosen to be large, which exactly forms the basis of {$\alpha$}-Rank. In contrast to the Nash equilibrium, which is a static solution concept based solely on fixed points, MCCs are a dynamical solution concept based on the Markov chain formalism, Conley's Fundamental Theorem of Dynamical Systems, and the core ingredients of dynamical systems: fixed points, recurrent sets, periodic orbits, and limit cycles. Our {$\alpha$}-Rank method runs in polynomial time with respect to the total number of pure strategy profiles, whereas computing a Nash equilibrium for a general-sum game is known to be intractable. We introduce mathematical proofs that not only provide an overarching and unifying perspective of existing continuous- and discrete-time evolutionary evaluation models, but also reveal the formal underpinnings of the {$\alpha$}-Rank methodology. We illustrate the method in canonical games and empirically validate it in several domains, including AlphaGo, AlphaZero, MuJoCo Soccer, and Poker.},
  copyright = {2019 The Author(s)},
  file = {/Users/x0r/Zotero/storage/5P3Q5YQ8/s41598-019-45619-9.pdf;/Users/x0r/Zotero/storage/RGUDCTQG/Omidshafiei et al_2019_α- Rank.pdf;/Users/x0r/Zotero/storage/CEN87QWS/s41598-019-45619-9.html},
  journal = {Scientific Reports},
  keywords = {To read},
  language = {en},
  number = {1}
}

@article{omrot,
  title = {{{THRESHOLD ENERGY EFFECT ON AVALANCHE BREAKDOWN VOLTAGE IN SEMICONDUCTOR JUNCTIONS}}},
  author = {Omrot, Y},
  pages = {8},
  abstract = {The band bending for avalanche breakdown in semiconductor junctions and its temperature dependence are predicted taking account of threshold energy effects on the ionization process in semiconductors. Where experimental results exist, the theoretical predictions and experimental results are in excellent agreement. In the high electric field region inclusion of both bulk and boundary threshold energy effects is essential. The predictions were based on exact solutions in the nonlocalized ionization coefficient formulation developed by Okuto and Crowell who showedthat ionizationcoefficientsas usuallyunderstoodarefunctionsof bothelectricfield and position in a device. Predictions for abrupt and p-i-n junctions in Ge, Si, GaAs and GaP are presented.},
  file = {/Users/x0r/Zotero/storage/CHMUCF6T/Omrot_THRESHOLD ENERGY EFFECT ON AVALANCHE BREAKDOWN VOLTAGE IN SEMICONDUCTOR.pdf},
  keywords = {OTS},
  language = {en}
}

@article{oord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/UP5UI4MW/Oord et al_2016_WaveNet.pdf;/Users/x0r/Zotero/storage/DHWTK33H/1609.html},
  journal = {arXiv:1609.03499 [cs]},
  keywords = {deepmind,GAN},
  primaryClass = {cs}
}

@article{oord2016a,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-ofthe-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archivePrefix = {arXiv},
  eprint = {1606.05328},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/PNXEGFAJ/Oord et al_2016_Conditional Image Generation with PixelCNN Decoders.pdf},
  journal = {arXiv:1606.05328 [cs]},
  keywords = {GAN,pixelCNN},
  language = {en},
  primaryClass = {cs}
}

@article{openai2019,
  title = {Learning {{Dexterous In}}-{{Hand Manipulation}}},
  author = {OpenAI and Andrychowicz, Marcin and Baker, Bowen and Chociej, Maciek and Jozefowicz, Rafal and McGrew, Bob and Pachocki, Jakub and Petron, Arthur and Plappert, Matthias and Powell, Glenn and Ray, Alex and Schneider, Jonas and Sidor, Szymon and Tobin, Josh and Welinder, Peter and Weng, Lilian and Zaremba, Wojciech},
  year = {2019},
  month = jan,
  abstract = {We use reinforcement learning (RL) to learn dexterous in-hand manipulation policies which can perform vision-based object reorientation on a physical Shadow Dexterous Hand. The training is performed in a simulated environment in which we randomize many of the physical properties of the system like friction coefficients and an object's appearance. Our policies transfer to the physical robot despite being trained entirely in simulation. Our method does not rely on any human demonstrations, but many behaviors found in human manipulation emerge naturally, including finger gaiting, multi-finger coordination, and the controlled use of gravity. Our results were obtained using the same distributed RL system that was used to train OpenAI Five [43]. We also include a video of our results: https://youtu. be/jwSbzNHGflM.},
  archivePrefix = {arXiv},
  eprint = {1808.00177},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/6QQY4Q8P/OpenAI et al. - 2019 - Learning Dexterous In-Hand Manipulation.pdf},
  journal = {arXiv:1808.00177 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{osband,
  title = {Deep {{Exploration}} via {{Bootstrapped DQN}}},
  author = {Osband, Ian and Blundell, Charles and Pritzel, Alexander and Roy, Benjamin Van},
  pages = {9},
  abstract = {E cient exploration remains a major challenge for reinforcement learning (RL). Common dithering strategies for exploration, such as `-greedy, do not carry out temporally-extended (or deep) exploration; this can lead to exponentially larger data requirements. However, most algorithms for statistically e cient RL are not computationally tractable in complex environments. Randomized value functions o er a promising approach to e cient exploration with generalization, but existing algorithms are not compatible with nonlinearly parameterized value functions. As a first step towards addressing such contexts we develop bootstrapped DQN. We demonstrate that bootstrapped DQN can combine deep exploration with deep neural networks for exponentially faster learning than any dithering strategy. In the Arcade Learning Environment bootstrapped DQN substantially improves learning speed and cumulative performance across most games.},
  file = {/Users/x0r/Zotero/storage/WQ868RLY/Osband et al_Deep Exploration via Bootstrapped DQN.pdf},
  keywords = {dqn,rl},
  language = {en}
}

@article{osband2019,
  title = {Behaviour {{Suite}} for {{Reinforcement Learning}}},
  author = {Osband, Ian and Doron, Yotam and Hessel, Matteo and Aslanides, John and Sezener, Eren and Saraiva, Andre and McKinney, Katrina and Lattimore, Tor and Szepezvari, Csaba and Singh, Satinder and Van Roy, Benjamin and Sutton, Richard and Silver, David and Van Hasselt, Hado},
  year = {2019},
  month = aug,
  abstract = {This paper introduces the Behaviour Suite for Reinforcement Learning, or bsuite for short. bsuite is a collection of carefully-designed experiments that investigate core capabilities of reinforcement learning (RL) agents with two objectives. First, to collect clear, informative and scalable problems that capture key issues in the design of general and efficient learning algorithms. Second, to study agent behaviour through their performance on these shared benchmarks. To complement this effort, we open source github.com/deepmind/bsuite, which automates evaluation and analysis of any agent on bsuite. This library facilitates reproducible and accessible research on the core issues in RL, and ultimately the design of superior learning algorithms. Our code is Python, and easy to use within existing projects. We include examples with OpenAI Baselines, Dopamine as well as new reference implementations. Going forward, we hope to incorporate more excellent experiments from the research community, and commit to a periodic review of bsuite from a committee of prominent researchers.},
  archivePrefix = {arXiv},
  eprint = {1908.03568},
  eprinttype = {arxiv},
  file = {/Users/x0r/switchdrive/zotero/Osband et al_2019_Behaviour Suite for Reinforcement Learning.pdf;/Users/x0r/Zotero/storage/PF4RBUBN/1908.html},
  journal = {arXiv:1908.03568 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{osswald2017,
  title = {A Spiking Neural Network Model of {{3D}} Perception for Event-Based Neuromorphic Stereo Vision Systems},
  author = {Osswald, Marc and Ieng, Sio-Hoi and Benosman, Ryad and Indiveri, Giacomo},
  year = {2017},
  month = feb,
  volume = {7},
  pages = {40703},
  issn = {2045-2322},
  doi = {10.1038/srep40703},
  file = {/Users/x0r/Zotero/storage/2K8SN7I5/Osswald et al. - 2017 - A spiking neural network model of 3D perception fo.pdf},
  journal = {Scientific Reports},
  keywords = {depth,SNN},
  language = {en},
  number = {1}
}

@article{ovshinsky1968,
  title = {Reversible {{Electrical Switching Phenomena}} in {{Disordered Structures}}},
  author = {Ovshinsky, Stanford R},
  year = {1968},
  volume = {21},
  pages = {6},
  file = {/Users/x0r/Zotero/storage/BPQW6I66/Ovshinsky_1968_Reversible Electrical Switching Phenomena in Disordered Structures.pdf},
  journal = {PHYSICAL REVIEW LETTERS},
  keywords = {modeling,OTS,PCM},
  language = {en},
  number = {20}
}

@article{owen1973,
  title = {Electronic Conduction and Switching in Chalcogenide Glasses},
  author = {Owen, A.E. and Robertson, J.M.},
  year = {1973},
  month = feb,
  volume = {20},
  pages = {105--122},
  issn = {0018-9383},
  doi = {10.1109/T-ED.1973.17617},
  file = {/Users/x0r/Zotero/storage/ZCERBT8C/Owen_Robertson_1973_Electronic conduction and switching in chalcogenide glasses.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {PCM,threshold-switching},
  language = {en},
  number = {2}
}

@article{owen1979,
  title = {The Threshold Characteristics of Chalcogenide-Glass Memory Switches},
  author = {Owen, A.E. and Robertson, J.M. and Main, C.},
  year = {1979},
  month = feb,
  volume = {32},
  pages = {29--52},
  issn = {00223093},
  doi = {10.1016/0022-3093(79)90063-2},
  file = {/Users/x0r/Zotero/storage/K466HBMZ/Owen et al_1979_The threshold characteristics of chalcogenide-glass memory switches.pdf},
  journal = {Journal of Non-Crystalline Solids},
  keywords = {PCM,threshold-switching},
  language = {en},
  number = {1-3}
}

@article{paliwal2019,
  title = {Graph {{Representations}} for {{Higher}}-{{Order Logic}} and {{Theorem Proving}}},
  author = {Paliwal, Aditya and Loos, Sarah and Rabe, Markus and Bansal, Kshitij and Szegedy, Christian},
  year = {2019},
  month = may,
  abstract = {This paper presents the first use of graph neural networks (GNNs) for higher-order proof search and demonstrates that GNNs can improve upon state-of-the-art results in this domain. Interactive, higher-order theorem provers allow for the formalization of most mathematical theories and have been shown to pose a significant challenge for deep learning. Higher-order logic is highly expressive and, even though it is well-structured with a clearly defined grammar and semantics, there still remains no well-established method to convert formulas into graph-based representations. In this paper, we consider several graphical representations of higher-order logic and evaluate them against the HOList benchmark for higher-order theorem proving.},
  archivePrefix = {arXiv},
  eprint = {1905.10006},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ILSS9JGX/Paliwal et al_2019_Graph Representations for Higher-Order Logic and Theorem Proving.pdf},
  journal = {arXiv:1905.10006 [cs, stat]},
  keywords = {dl},
  primaryClass = {cs, stat}
}

@article{panda2018,
  title = {A {{Collective Study}} on {{Modeling}} and {{Simulation}} of {{Resistive Random Access Memory}}},
  author = {Panda, Debashis and Sahu, Paritosh Piyush and Tseng, Tseung Yuen},
  year = {2018},
  month = dec,
  volume = {13},
  issn = {1931-7573, 1556-276X},
  doi = {10.1186/s11671-017-2419-8},
  abstract = {In this work, we provide a comprehensive discussion on the various models proposed for the design and description of resistive random access memory (RRAM), being a nascent technology is heavily reliant on accurate models to develop efficient working designs and standardize its implementation across devices. This review provides detailed information regarding the various physical methodologies considered for developing models for RRAM devices. It covers all the important models reported till now and elucidates their features and limitations. Various additional effects and anomalies arising from memristive system have been addressed, and the solutions provided by the models to these problems have been shown as well. All the fundamental concepts of RRAM model development such as device operation, switching dynamics, and current-voltage relationships are covered in detail in this work. Popular models proposed by Chua, HP Labs, Yakopcic, TEAM, Stanford/ASU, Ielmini, Berco-Tseng, and many others have been compared and analyzed extensively on various parameters. The working and implementations of the window functions like Joglekar, Biolek, Prodromakis, etc. has been presented and compared as well. New well-defined modeling concepts have been discussed which increase the applicability and accuracy of the models. The use of these concepts brings forth several improvements in the existing models, which have been enumerated in this work. Following the template presented, highly accurate models would be developed which will vastly help future model developers and the modeling community.},
  file = {/Users/x0r/Zotero/storage/WVIS55N4/Panda et al_2018_A Collective Study on Modeling and Simulation of Resistive Random Access Memory.pdf},
  journal = {Nanoscale Research Letters},
  keywords = {ReRAM,Sungjung},
  language = {en},
  number = {1}
}

@article{pantazi2016,
  title = {All-Memristive Neuromorphic Computing with Level-Tuned Neurons},
  author = {Pantazi, Angeliki and Wo{\'z}niak, Stanis{\l}aw and Tuma, Tomas and Eleftheriou, Evangelos},
  year = {2016},
  month = sep,
  volume = {27},
  pages = {355205},
  issn = {0957-4484, 1361-6528},
  doi = {10.1088/0957-4484/27/35/355205},
  abstract = {In the new era of cognitive computing, systems will be able to learn and interact with the environment in ways that will drastically enhance the capabilities of current processors, especially in extracting knowledge from vast amount of data obtained from many sources. Braininspired neuromorphic computing systems increasingly attract research interest as an alternative to the classical von Neumann processor architecture, mainly because of the coexistence of memory and processing units. In these systems, the basic components are neurons interconnected by synapses. The neurons, based on their nonlinear dynamics, generate spikes that provide the main communication mechanism. The computational tasks are distributed across the neural network, where synapses implement both the memory and the computational units, by means of learning mechanisms such as spike-timing-dependent plasticity. In this work, we present an all-memristive neuromorphic architecture comprising neurons and synapses realized by using the physical properties and state dynamics of phase-change memristors. The architecture employs a novel concept of interconnecting the neurons in the same layer, resulting in level-tuned neuronal characteristics that preferentially process input information. We demonstrate the proposed architecture in the tasks of unsupervised learning and detection of multiple temporal correlations in parallel input streams. The efficiency of the neuromorphic architecture along with the homogenous neuro-synaptic dynamics implemented with nanoscale phase-change memristors represent a significant step towards the development of ultrahighdensity neuromorphic co-processors.},
  file = {/Users/x0r/Zotero/storage/T52ZPYS6/Pantazi et al_2016_All-memristive neuromorphic computing with level-tuned neurons.pdf},
  journal = {Nanotechnology},
  keywords = {neuromorphic},
  language = {en},
  number = {35}
}

@inproceedings{papandreou2011,
  title = {Programming Algorithms for Multilevel Phase-Change Memory},
  author = {Papandreou, N. and Pozidis, H. and Pantazi, A. and Sebastian, A. and Breitwisch, M. and Lam, C. and Eleftheriou, E.},
  year = {2011},
  month = may,
  pages = {329--332},
  publisher = {{IEEE}},
  doi = {10.1109/ISCAS.2011.5937569},
  abstract = {Phase-change memory (PCM) has emerged as one among the most promising technologies for next-generation non\- volatile solid-state memory. Multilevel storage, namely storage of non-binary information in a memory cell, is a key factor for reducing the total cost-per-bit and thus increasing the competi\- tiveness of PCM technology in the nonvolatile memory market. In this paper, we present a family of advanced programming schemes for multilevel storage in PCM. The proposed schemes are based on iterative write-and-verify algorithms that exploit the unique programming characteristics of PCM in order to achieve significant improvements in resistance-level packing den\- sity, robustness to cell variability, programming latency, energy\- per-bit and cell storage capacity. Experimental results from PCM test-arrays are presented to validate the proposed programming schemes. In addition, the reliability issues of multilevel PCM in terms of resistance drift and read noise are discussed.},
  file = {/Users/x0r/Zotero/storage/IHBA3U6S/Papandreou et al_2011_Programming algorithms for multilevel phase-change memory.pdf},
  isbn = {978-1-4244-9473-6},
  keywords = {MLC,neuromorphic,PCM,To read},
  language = {en}
}

@article{parisotto2019,
  title = {Stabilizing {{Transformers}} for {{Reinforcement Learning}}},
  author = {Parisotto, Emilio and Song, H. Francis and Rae, Jack W. and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant M. and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and Botvinick, Matthew M. and Heess, Nicolas and Hadsell, Raia},
  year = {2019},
  month = oct,
  abstract = {Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.},
  archivePrefix = {arXiv},
  eprint = {1910.06764},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/TUXASEPG/Parisotto et al. - 2019 - Stabilizing Transformers for Reinforcement Learnin.pdf},
  journal = {arXiv:1910.06764 [cs, stat]},
  keywords = {rl,transformer},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pascalrobert,
  title = {Towards an Evolutionary Theory of the Origin of Life Based on Kinetics and Thermodynamics},
  author = {{Pascal Robert} and {Pross Addy} and {Sutherland John D.}},
  volume = {3},
  pages = {130156},
  doi = {10.1098/rsob.130156},
  abstract = {A sudden transition in a system from an inanimate state to the living state\textemdash defined on the basis of present day living organisms\textemdash would constitute a highly unlikely event hardly predictable from physical laws. From this uncontroversial idea, a self-consistent representation of the origin of life process is built up, which is based on the possibility of a series of intermediate stages. This approach requires a particular kind of stability for these stages\textemdash dynamic kinetic stability (DKS)\textemdash which is not usually observed in regular chemistry, and which is reflected in the persistence of entities capable of self-reproduction. The necessary connection of this kinetic behaviour with far-from-equilibrium thermodynamic conditions is emphasized and this leads to an evolutionary view for the origin of life in which multiplying entities must be associated with the dissipation of free energy. Any kind of entity involved in this process has to pay the energetic cost of irreversibility, but, by doing so, the contingent emergence of new functions is made feasible. The consequences of these views on the studies of processes by which life can emerge are inferred.},
  file = {/Users/x0r/Zotero/storage/Z24E3MYE/Pascal Robert et al_Towards an evolutionary theory of the origin of life based on kinetics and.pdf},
  journal = {Open Biology},
  keywords = {life},
  number = {11}
}

@techreport{payeur2020,
  title = {Burst-Dependent Synaptic Plasticity Can Coordinate Learning in Hierarchical Circuits},
  author = {Payeur, Alexandre and Guerguiev, Jordan and Zenke, Friedemann and Richards, Blake A. and Naud, Richard},
  year = {2020},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.03.30.015511},
  abstract = {Synaptic plasticity is believed to be a key physiological mechanism for learning. It is well-established that it depends on pre and postsynaptic activity. However, models that rely solely on pre and postsynaptic activity for synaptic changes have, to date, not been able to account for learning complex tasks that demand hierarchical networks. Here, we show that if synaptic plasticity is regulated by high-frequency bursts of spikes, then neurons higher in the hierarchy can coordinate the plasticity of lower-level connections. Using simulations and mathematical analyses, we demonstrate that, when paired with short-term synaptic dynamics, regenerative activity in the apical dendrites, and synaptic plasticity in feedback pathways, a burst-dependent learning rule can solve challenging tasks that require deep network architectures. Our results demonstrate that well-known properties of dendrites, synapses, and synaptic plasticity are sufficient to enable sophisticated learning in hierarchical circuits.},
  file = {/Users/x0r/Zotero/storage/6VA7QHT7/Payeur et al. - 2020 - Burst-dependent synaptic plasticity can coordinate.pdf;/Users/x0r/Zotero/storage/79IZILHZ/media-1.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{Payvand_etal18,
  ids = {payvand2018},
  title = {Event-Based Circuits for Controlling Stochastic Learning with Memristive Devices in Neuromorphic Architectures},
  booktitle = {Circuits and Systems ({{ISCAS}}), 2018 {{IEEE}} International Symposium On},
  author = {Payvand, Melika and Muller, Lorenz K and Indiveri, Giacomo},
  year = {2018},
  pages = {1--5},
  file = {/Users/x0r/Zotero/storage/CKDWJQXZ/Payvand et al_2018_Event-based circuits for controlling stochastic learning with memristive.pdf},
  keywords = {neuromorphic},
  organization = {{[object Object]}}
}

@article{Payvand_etal19,
  ids = {payvand2019a},
  title = {A Neuromorphic Systems Approach to In-Memory Computing with Non-Ideal Memristive Devices: {{From}} Mitigation to Exploitation},
  author = {Payvand, Melika and Nair, Manu V and M{\"u}ller, Lorenz K and Indiveri, Giacomo},
  year = {2019},
  volume = {213},
  pages = {487--510},
  publisher = {{[object Object]}},
  arxiv = {[object Object]},
  file = {/Users/x0r/Zotero/storage/TSDKFGLD/Payvand et al_2019_A neuromorphic systems approach to in-memory computing with non-ideal.pdf},
  journal = {Faraday Discussions},
  keywords = {neuromorphic}
}

@inproceedings{payvand2019,
  title = {Spike-{{Based Plasticity Circuits}} for {{Always}}-on {{On}}-{{Line Learning}} in {{Neuromorphic Systems}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Payvand, Melika and Indiveri, Giacomo},
  year = {2019},
  month = may,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Sapporo, Japan}},
  doi = {10.1109/ISCAS.2019.8702497},
  abstract = {Event-driven neuromorphic hardware with on-line learning capabilities enables the low-power local processing of signals on the edge sensors. Implementing such hardware requires having an always-on online learning operation in order to continuously adapt to the changes in the environment. Therefore, as the data is continuously streaming, there cannot be a separation between the training and the testing phase. Such constraint thus asks for a continuous time learning strategy which includes a mechanism to stop changing the weights when the system has reached an optimal operating point, so that it does not over-fit the input data and it generalizes to unseen patterns of the learned class. In this paper we propose spike-based circuits based on a local gradient-descent based learning rule that comprise also this additional ``stop-learning'' feature and that have a wide range of configurability options over the learning parameters. We describe the circuit behavior and present simulation results for a standard CMOS 180 nm process, showing how the width of the stop-learning region can be controlled along with the learning rate of the system. Such system represents a hardware implementation of a feature which has shown to improves the stability of the learning process and the convergence properties of the network.},
  file = {/Users/x0r/Zotero/storage/JEI9MVNA/Payvand_Indiveri_2019_Spike-Based Plasticity Circuits for Always-on On-Line Learning in Neuromorphic.pdf},
  isbn = {978-1-72810-397-6},
  keywords = {neuromorphic},
  language = {en}
}

@article{peng,
  title = {{{MCP}}: {{Learning Composable Hierarchical Control}} with {{Multiplicative Compositional Policies}}},
  author = {Peng, Xue Bin and Chang, Michael and Zhang, Grace and Abbeel, Pieter and Levine, Sergey},
  pages = {18},
  file = {/Users/x0r/Zotero/storage/B2ZJXDWI/Peng et al_MCP.pdf},
  keywords = {rl},
  language = {en}
}

@inproceedings{peng2019,
  title = {Inference Engine Benchmarking across Technological Platforms from {{CMOS}} to {{RRAM}}},
  booktitle = {Proceedings of the {{International Symposium}} on {{Memory Systems}}},
  author = {Peng, Xiaochen and Kim, Minkyu and Sun, Xiaoyu and Yin, Shihui and Rakshit, Titash and Hatcher, Ryan M. and Kittl, Jorge A. and Seo, Jae-sun and Yu, Shimeng},
  year = {2019},
  month = sep,
  pages = {471--479},
  publisher = {{ACM}},
  address = {{Washington District of Columbia}},
  doi = {10.1145/3357526.3357566},
  abstract = {State-of-the-art deep convolutional neural networks (CNNs) are widely used in current AI systems, and achieve remarkable success in image/speech recognition and classification. A number of recent efforts have attempted to design custom inference engine based on various approaches, including the systolic architecture, near memory processing, and processing-in-memory (PIM) approach with emerging technologies such as resistive random access memory (RRAM). However, a comprehensive comparison of these various approaches in a unified framework is missing, and the benefits of new designs or emerging technologies are mostly based on qualitative projections. In this paper, we evaluate the energy efficiency and frame rate for a VGG-like CNN inference accelerator on CIFAR-10 dataset across the technological platforms from CMOS to post-CMOS, with hardware resource constraint, i.e. comparable on-chip area. We also investigate the effects of off-chip memory DRAM access and interconnect during data movement, which are the bottlenecks of CMOS platforms. Our quantitative analysis shows that the peripheries (ADCs) dominate in energy consumption and area (rather than memory array) in digital RRAM-based parallel readout PIM architecture. Despite presence of ADCs, this architecture shows {$>$}2.5\texttimes{} improvement in energy efficiency (TOPS/W) over systolic arrays or near memory processing, with a comparable frame rate due to reduced DRAM access, high throughput and optimized parallel read out. Further {$>$}10\texttimes{} improvements can be achieved by implementing bit-count reduced XNOR network and pipelining.},
  file = {/Users/x0r/Zotero/storage/KFDJGGK4/Peng et al. - 2019 - Inference engine benchmarking across technological.pdf},
  isbn = {978-1-4503-7206-0},
  language = {en}
}

@article{pfau2016,
  title = {Connecting {{Generative Adversarial Networks}} and {{Actor}}-{{Critic Methods}}},
  author = {Pfau, David and Vinyals, Oriol},
  year = {2016},
  month = oct,
  abstract = {Both generative adversarial networks (GAN) in unsupervised learning and actorcritic methods in reinforcement learning (RL) have gained a reputation for being difficult to optimize. Practitioners in both fields have amassed a large number of strategies to mitigate these instabilities and improve training. Here we show that GANs can be viewed as actor-critic methods in an environment where the actor cannot affect the reward. We review the strategies for stabilizing training for each class of models, both those that generalize between the two and those that are particular to that model. We also review a number of extensions to GANs and RL algorithms with even more complicated information flow. We hope that by highlighting this formal connection we will encourage both GAN and RL communities to develop general, scalable, and stable algorithms for multilevel optimization with deep networks, and to draw inspiration across communities.},
  archivePrefix = {arXiv},
  eprint = {1610.01945},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/3CWIQ5ZN/Pfau_Vinyals_2016_Connecting Generative Adversarial Networks and Actor-Critic Methods.pdf},
  journal = {arXiv:1610.01945 [cs, stat]},
  keywords = {dl,GAN},
  language = {en},
  primaryClass = {cs, stat}
}

@article{phadtare2009,
  title = {Scientific Writing: A Randomized Controlled Trial Comparing Standard and on-Line Instruction},
  shorttitle = {Scientific Writing},
  author = {Phadtare, Amruta and Bahmani, Anu and Shah, Anand and Pietrobon, Ricardo},
  year = {2009},
  month = dec,
  volume = {9},
  issn = {1472-6920},
  doi = {10.1186/1472-6920-9-27},
  abstract = {Background: Writing plays a central role in the communication of scientific ideas and is therefore a key aspect in researcher education, ultimately determining the success and long-term sustainability of their careers. Despite the growing popularity of e-learning, we are not aware of any existing study comparing on-line vs. traditional classroom-based methods for teaching scientific writing.
Methods: Forty eight participants from a medical, nursing and physiotherapy background from US and Brazil were randomly assigned to two groups (n = 24 per group): An on-line writing workshop group (on-line group), in which participants used virtual communication, google docs and standard writing templates, and a standard writing guidance training (standard group) where participants received standard instruction without the aid of virtual communication and writing templates. Two outcomes, manuscript quality was assessed using the scores obtained in Six subgroup analysis scale as the primary outcome measure, and satisfaction scores with Likert scale were evaluated. To control for observer variability, inter-observer reliability was assessed using Fleiss's kappa. A posthoc analysis comparing rates of communication between mentors and participants was performed. Nonparametric tests were used to assess intervention efficacy.
Results: Excellent inter-observer reliability among three reviewers was found, with an Intraclass Correlation Coefficient (ICC) agreement = 0.931882 and ICC consistency = 0.932485. On-line group had better overall manuscript quality (p = 0.0017, SSQSavg score 75.3 {$\pm$} 14.21, ranging from 37 to 94) compared to the standard group (47.27 {$\pm$} 14.64, ranging from 20 to 72). Participant satisfaction was higher in the on-line group (4.3 {$\pm$} 0.73) compared to the standard group (3.09 {$\pm$} 1.11) (p = 0.001). The standard group also had fewer communication events compared to the online group (0.91 {$\pm$} 0.81 vs. 2.05 {$\pm$} 1.23; p = 0.0219).
Conclusion: Our protocol for on-line scientific writing instruction is better than standard face-toface instruction in terms of writing quality and student satisfaction. Future studies should evaluate the protocol efficacy in larger longitudinal cohorts involving participants from different languages.},
  file = {/Users/x0r/Zotero/storage/DMNXQHJW/Phadtare et al_2009_Scientific writing.pdf;/Users/x0r/Zotero/storage/Q47KGSC2/Phadtare et al_2009_Scientific writing.pdf},
  journal = {BMC Medical Education},
  language = {en},
  number = {1}
}

@article{pirovano2004,
  title = {Low-Field Amorphous State Resistance and Threshold Voltage Drift in Chalcogenide Materials},
  author = {Pirovano, A. and Lacaita, A. L. and Pellizzer, F. and Kostylev, S. A. and Benvenuti, A. and Bez, R.},
  year = {2004},
  month = may,
  volume = {51},
  pages = {714--719},
  issn = {0018-9383},
  doi = {10.1109/TED.2004.825805},
  abstract = {A detailed investigation of the time evolution for the low-field resistance Roff and the threshold voltage Vth in chalcogenide-based phase-change memory devices is presented. It is observed that both Roff and Vth increase and become stable with time and temperature, thus improving the cell readout window. Relying on a microscopic model, the drift of Roff and Vth is linked to the dynamic of the intrinsic traps typical of amorphous chalcogenides, thus providing for the first time a unified framework for the comprehension of chalcogenide materials transient behavior.},
  file = {/Users/x0r/Zotero/storage/2VKGN5LT/Pirovano et al_2004_Low-field amorphous state resistance and threshold voltage drift in.pdf;/Users/x0r/Zotero/storage/4V7MSTP5/1303829.html},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {PCM,threshold-switching},
  number = {5}
}

@article{pirovano2004a,
  title = {Electronic {{Switching}} in {{Phase}}-{{Change Memories}}},
  author = {Pirovano, A. and Lacaita, A.L. and Benvenuti, A. and Pellizzer, F. and Bez, R.},
  year = {2004},
  month = mar,
  volume = {51},
  pages = {452--459},
  issn = {0018-9383},
  doi = {10.1109/TED.2003.823243},
  abstract = {A detailed investigation of electronic switching in chalcogenide-based phase-change memory devices is presented. An original bandgap model consistent with the microscopic structure of both crystalline and amorphous chalcogenide is described, and a physical picture of the switching mechanism is proposed. Numerical simulations provide, for the first time, a quantitative description of the peculiar current\textendash voltage curve of a Ge2Sb2Te5 resistor, in good agreement with measurements performed on test devices.},
  file = {/Users/x0r/Zotero/storage/A8ELPLD2/Pirovano et al_2004_Electronic Switching in Phase-Change Memories.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {PCM,threshold-switching},
  language = {en},
  number = {3}
}

@article{plappert2018,
  title = {Parameter {{Space Noise}} for {{Exploration}}},
  author = {Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y. and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  year = {2018},
  month = jan,
  abstract = {Deep reinforcement learning (RL) methods generally engage in exploratory behavior through noise injection in the action space. An alternative is to add noise directly to the agent's parameters, which can lead to more consistent exploration and a richer set of behaviors. Methods such as evolutionary strategies use parameter perturbations, but discard all temporal structure in the process and require significantly more samples. Combining parameter noise with traditional RL methods allows to combine the best of both worlds. We demonstrate that both off- and on-policy methods benefit from this approach through experimental comparison of DQN, DDPG, and TRPO on high-dimensional discrete action environments as well as continuous control tasks.},
  archivePrefix = {arXiv},
  eprint = {1706.01905},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9YNF7BGL/Plappert et al. - 2018 - Parameter Space Noise for Exploration.pdf},
  journal = {arXiv:1706.01905 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pogodin,
  title = {Working Memory Facilitates Reward-Modulated {{Hebbian}} Learning in Recurrent Neural Networks},
  author = {Pogodin, Roman and Corneil, Dane and Seeholzer, Alexander and Heng, Joseph and Gerstner, Wulfram},
  pages = {5},
  abstract = {Reservoir computing is a powerful tool to explain how the brain learns temporal sequences, such as movements, but existing learning schemes are either biologically implausible or too inefficient to explain animal performance. We show that a network can learn complicated sequences with a reward-modulated Hebbian learning rule if the network of reservoir neurons is combined with a second network that serves as a dynamic working memory and provides a spatio-temporal backbone signal to the reservoir. In combination with the working memory, reward-modulated Hebbian learning of the readout neurons performs as well as FORCE learning, but with the advantage of a biologically plausible interpretation of both the learning rule and the learning paradigm.},
  file = {/Users/x0r/Zotero/storage/75CEE8NJ/Pogodin et al. - Working memory facilitates reward-modulated Hebbia.pdf},
  keywords = {neuroscience,RNN},
  language = {en}
}

@article{poirazi2020,
  title = {Illuminating Dendritic Function with Computational Models},
  author = {Poirazi, Panayiota and Papoutsi, Athanasia},
  year = {2020},
  month = jun,
  volume = {21},
  pages = {303--321},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0301-7},
  abstract = {Dendrites have always fascinated researchers: from the artistic drawings by Ramon y Cajal to the beautiful recordings of today{$\mkern1mu$}, neuroscientists have been striving to unravel the mysteries of these structures. Theoretical work in the 1960s predicted important dendritic effects on neuronal processing, establishing computational modelling as a powerful technique for their investigation. Since then, modelling of dendrites has been instrumental in driving neuroscience research in a targeted manner, providing experimentally testable predictions that range from the subcellular level to the systems level, and their relevance extends to fields beyond neuroscience, such as machine learning and artificial intelligence. Validation of modelling predictions often requires \textemdash{} and drives \textemdash{} new technological advances, thus closing the loop with theory-d riven experimentation that moves the field forward. This Review features the most important, to our understanding, contributions of modelling of dendritic computations, including those pending experimental verification, and highlights studies of successful interactions between the modelling and experimental neuroscience communities.},
  file = {/Users/x0r/Zotero/storage/KSTP3TUR/Poirazi and Papoutsi - 2020 - Illuminating dendritic function with computational.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {6}
}

@article{ponce2019,
  title = {Evolving {{Images}} for {{Visual Neurons Using}} a {{Deep Generative Network Reveals Coding Principles}} and {{Neuronal Preferences}}},
  author = {Ponce, Carlos R. and Xiao, Will and Schade, Peter F. and Hartmann, Till S. and Kreiman, Gabriel and Livingstone, Margaret S.},
  year = {2019},
  month = may,
  volume = {177},
  pages = {999-1009.e10},
  issn = {0092-8674, 1097-4172},
  doi = {10.1016/j.cell.2019.04.005},
  file = {/Users/x0r/Zotero/storage/RDPHUWLW/Ponce et al_2019_Evolving Images for Visual Neurons Using a Deep Generative Network Reveals.pdf},
  journal = {Cell},
  language = {English},
  number = {4},
  pmid = {31051108}
}

@article{pozzi2018,
  title = {A {{Biologically Plausible Learning Rule}} for {{Deep Learning}} in the {{Brain}}},
  author = {Pozzi, Isabella and Boht{\'e}, Sander and Roelfsema, Pieter},
  year = {2018},
  month = nov,
  abstract = {Researchers have proposed that deep learning, which is providing important progress in a wide range of high complexity tasks, might inspire new insights into learning in the brain. However, the methods used for deep learning by artificial neural networks are biologically unrealistic and would need to be replaced by biologically realistic counterparts. Previous biologically plausible reinforcement learning rules, like AGREL and AuGMEnT, showed promising results but focused on shallow networks with three layers. Will these learning rules also generalize to networks with more layers and can they handle tasks of higher complexity? We demonstrate the learning scheme on classical and hard image-classification benchmarks, namely MNIST, CIFAR10 and CIFAR100, cast as direct reward tasks, both for fully connected, convolutional and locally connected architectures. We show that our learning rule - Q-AGREL - performs comparably to supervised learning via error-backpropagation, with this type of trial-and-error reinforcement learning requiring only 1.5-2.5 times more epochs, even when classifying 100 different classes as in CIFAR100. Our results provide new insights into how deep learning may be implemented in the brain.},
  archivePrefix = {arXiv},
  eprint = {1811.01768},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/V6HVEVWJ/Pozzi et al_2018_A Biologically Plausible Learning Rule for Deep Learning in the Brain.pdf;/Users/x0r/Zotero/storage/L9ZKU722/1811.html},
  journal = {arXiv:1811.01768 [cs]},
  keywords = {dl,SNN},
  primaryClass = {cs}
}

@book{press2007,
  title = {Numerical Recipes: The Art of Scientific Computing},
  shorttitle = {Numerical Recipes},
  author = {Press, William H},
  year = {2007},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, UK; New York}},
  file = {/Users/x0r/Zotero/storage/BX2J9X8S/Press_2007_Numerical recipes.pdf},
  isbn = {978-0-511-33555-6},
  language = {English}
}

@article{pritzel2017,
  title = {Neural {{Episodic Control}}},
  author = {Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdom{\`e}nech, Adri{\`a} and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  year = {2017},
  month = mar,
  abstract = {Deep reinforcement learning methods attain super-human performance in a wide range of environments. Such methods are grossly inefficient, often taking orders of magnitudes more data than humans to achieve reasonable performance. We propose Neural Episodic Control: a deep reinforcement learning agent that is able to rapidly assimilate new experiences and act upon them. Our agent uses a semi-tabular representation of the value function: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value function. We show across a wide range of environments that our agent learns significantly faster than other state-of-the-art, general purpose deep reinforcement learning agents.},
  archivePrefix = {arXiv},
  eprint = {1703.01988},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/8CDXBKI3/Pritzel et al_2017_Neural Episodic Control.pdf},
  journal = {arXiv:1703.01988 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{privitera2007,
  title = {Phase Change Mechanisms in {{Ge2Sb2Te5}}},
  author = {Privitera, S. and Lombardo, S. and Bongiorno, C. and Rimini, E. and Pirovano, A.},
  year = {2007},
  month = jul,
  volume = {102},
  pages = {013516},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.2752111},
  file = {/Users/x0r/Zotero/storage/TQ2R9CDF/Privitera et al_2007_Phase change mechanisms in Ge2Sb2Te5.pdf},
  journal = {Journal of Applied Physics},
  keywords = {GST,modeling},
  language = {en},
  number = {1}
}

@phdthesis{qi2015a,
  title = {{{FPGA BASED HIGH THROUGHPUT LOW POWER MULTI}}-{{CORE NEUROMORPHIC PROCESSOR}}},
  author = {Qi, Yangjie},
  year = {2015},
  file = {/Users/x0r/Zotero/storage/XFAYE5G9/Qi_2015_FPGA BASED HIGH THROUGHPUT LOW POWER MULTI-CORE NEUROMORPHIC PROCESSOR.pdf},
  keywords = {neuromorphic,To read},
  language = {en}
}

@book{quarteroni2000,
  title = {Numerical Mathematics},
  author = {Quarteroni, Alfio and Sacco, Riccardo and Saleri, Fausto},
  year = {2000},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/Users/x0r/Zotero/storage/VVBFP2ZT/Quarteroni et al_2000_Numerical mathematics.pdf},
  isbn = {978-0-387-98959-4},
  language = {en},
  lccn = {QA297 .Q83 2000},
  number = {37},
  series = {Texts in Applied Mathematics}
}

@article{r.2017,
  title = {Mixed-Precision Training of Deep Neural Networks Using Computational Memory},
  author = {R., Nandakumar S. and Gallo, Manuel Le and Boybat, Irem and Rajendran, Bipin and Sebastian, Abu and Eleftheriou, Evangelos},
  year = {2017},
  month = dec,
  abstract = {Deep neural networks have revolutionized the field of machine learning by providing unprecedented human-like performance in solving many real-world problems such as image and speech recognition. Training of large DNNs, however, is a computationally intensive task, and this necessitates the development of novel computing architectures targeting this application. A computational memory unit where resistive memory devices are organized in crossbar arrays can be used to locally store the synaptic weights in their conductance states. The expensive multiply accumulate operations can be performed in place using Kirchhoff's circuit laws in a non-von Neumann manner. However, a key challenge remains the inability to alter the conductance states of the devices in a reliable manner during the weight update process. We propose a mixed-precision architecture that combines a computational memory unit storing the synaptic weights with a digital processing unit and an additional memory unit accumulating weight updates in high precision. The new architecture delivers classification accuracies comparable to those of floating-point implementations without being constrained by challenges associated with the non-ideal weight update characteristics of emerging resistive memories. A two layer neural network in which the computational memory unit is realized using non-linear stochastic models of phase-change memory devices achieves a test accuracy of 97.40\% on the MNIST handwritten digit classification problem.},
  archivePrefix = {arXiv},
  eprint = {1712.01192},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2NJM6GH3/R. et al_2017_Mixed-precision training of deep neural networks using computational memory.pdf},
  journal = {arXiv:1712.01192 [cs]},
  keywords = {neuromorphic},
  language = {en},
  primaryClass = {cs}
}

@article{radford2019,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/Users/x0r/Zotero/storage/DHZWQIEH/Radford et al_Language Models are Unsupervised Multitask Learners.pdf},
  keywords = {dl,nlp},
  language = {en}
}

@article{rae2016,
  title = {Scaling {{Memory}}-{{Augmented Neural Networks}} with {{Sparse Reads}} and {{Writes}}},
  author = {Rae, Jack W. and Hunt, Jonathan J. and Harley, Tim and Danihelka, Ivo and Senior, Andrew and Wayne, Greg and Graves, Alex and Lillicrap, Timothy P.},
  year = {2016},
  month = oct,
  abstract = {Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows \textemdash{} limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000\texttimes{} faster and with 3,000\texttimes{} less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.},
  archivePrefix = {arXiv},
  eprint = {1610.09027},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/L5IML3QU/Rae et al_2016_Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes.pdf},
  journal = {arXiv:1610.09027 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{rajani2019,
  title = {Explain {{Yourself}}! {{Leveraging Language Models}} for {{Commonsense Reasoning}}},
  author = {Rajani, Nazneen Fatema and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = jun,
  abstract = {Deep learning models perform poorly on tasks that require commonsense reasoning, which often necessitates some form of world-knowledge or reasoning over information not immediately present in the input. We collect human explanations for commonsense reasoning in the form of natural language sequences and highlighted annotations in a new dataset called Common Sense Explanations (CoS-E). We use CoS-E to train language models to automatically generate explanations that can be used during training and inference in a novel Commonsense Auto-Generated Explanation (CAGE) framework. CAGE improves the state-of-the-art by 10\% on the challenging CommonsenseQA task. We further study commonsense reasoning in DNNs using both human and auto-generated explanations including transfer to out-of-domain tasks. Empirical results indicate that we can effectively leverage language models for commonsense reasoning.},
  archivePrefix = {arXiv},
  eprint = {1906.02361},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AAF2XRBT/Rajani et al_2019_Explain Yourself.pdf;/Users/x0r/Zotero/storage/Y6HKPNS9/1906.html},
  journal = {arXiv:1906.02361 [cs]},
  keywords = {nlp},
  primaryClass = {cs}
}

@article{rajendran2019,
  title = {Low-{{Power Neuromorphic Hardware}} for {{Signal Processing Applications}}},
  author = {Rajendran, Bipin and Sebastian, Abu and Schmuker, Michael and Srinivasa, Narayan and Eleftheriou, Evangelos},
  year = {2019},
  month = jan,
  abstract = {Machine learning has emerged as the dominant tool for implementing complex cognitive tasks that require supervised, unsupervised, and reinforcement learning. While the resulting machines have demonstrated in some cases even super-human performance, their energy consumption has often proved to be prohibitive in the absence of costly super-computers. Most state-of-the-art machine learning solutions are based on memory-less models of neurons. This is unlike the neurons in the human brain, which encode and process information using temporal information in spike events. The different computing principles underlying biological neurons and how they combine together to efficiently process information is believed to be a key factor behind their superior efficiency compared to current machine learning systems. Inspired by the time-encoding mechanism used by the brain, third generation spiking neural networks (SNNs) are being studied for building a new class of information processing engines.},
  archivePrefix = {arXiv},
  eprint = {1901.03690},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/HTLDKYIB/Rajendran et al_2019_Low-Power Neuromorphic Hardware for Signal Processing Applications.pdf},
  journal = {arXiv:1901.03690 [cs]},
  keywords = {neuromorphic},
  language = {en},
  primaryClass = {cs}
}

@article{raman2019,
  title = {Fundamental Bounds on Learning Performance in Neural Circuits},
  author = {Raman, Dhruva V. and O'Leary, Timothy},
  year = {2019},
  month = may,
  volume = {116},
  pages = {10537--10546},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1813416116},
  abstract = {How does the size of a neural circuit influence its learning performance? Intuitively, we expect the learning capacity of a neural circuit to grow with the number of neurons and synapses. Larger brains tend to be found in species with higher cognitive function and learning ability. Similarly, adding connections and units to artificial neural networks can allow them to solve more complex tasks. However, we show that in a biologically relevant setting where synapses introduce an unavoidable amount of noise, there is an optimal size of network for a given task. Beneath this optimal size, our analysis shows how adding apparently redundant neurons and connections can make tasks more learnable. Therefore large neural circuits can either devote connectivity to generating complex behaviors, or exploit this connectivity to achieve faster and more precise learning of simpler behaviors. Above the optimal network size, the addition of neurons and synaptic connections starts to impede learning performance. This suggests that overall brain size may be constrained by the need to learn efficiently with unreliable synapses, and may explain why some neurological learning deficits are associated with hyperconnectivity. Our analysis is independent of specific learning rules and uncovers fundamental relationships between learning rate, task performance, network size and intrinsic noise in neural circuits.},
  archivePrefix = {arXiv},
  eprint = {1812.11758},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/P4V8NKNF/Raman and O'Leary - 2019 - Fundamental bounds on learning performance in neur.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {21}
}

@article{ramanujan2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  month = mar,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever modifying the weight values. Hidden in a randomly weighted Wide ResNet-50 [32] we find a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 [9] trained on ImageNet [4]. Not only do these ``untrained subnetworks'' exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an ``untrained subnetwork'' approaches a network with learned weights in accuracy. Our code and pretrained models are available at: https://github.com/allenai/hidden-networks.},
  archivePrefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/KKJPEECZ/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf},
  journal = {arXiv:1911.13299 [cs]},
  keywords = {To read},
  language = {en},
  primaryClass = {cs}
}

@article{rao1999,
  title = {Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects},
  shorttitle = {Predictive Coding in the Visual Cortex},
  author = {Rao, Rajesh P. N. and Ballard, Dana H.},
  year = {1999},
  month = jan,
  volume = {2},
  pages = {79--87},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/4580},
  file = {/Users/x0r/Zotero/storage/E68GZYUF/Rao_Ballard_1999_Predictive coding in the visual cortex.pdf},
  journal = {Nature Neuroscience},
  keywords = {neuroscience,To read},
  language = {en},
  number = {1}
}

@article{rao2019,
  title = {Continual {{Unsupervised Representation Learning}}},
  author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2019},
  month = oct,
  abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
  archivePrefix = {arXiv},
  eprint = {1910.14481},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/DZNJBUU4/Rao et al. - 2019 - Continual Unsupervised Representation Learning.pdf},
  journal = {arXiv:1910.14481 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rasch2019,
  title = {{{RAPA}}-{{ConvNets}}: {{Modified Convolutional Networks}} for {{Accelerated Training}} on {{Architectures With Analog Arrays}}},
  shorttitle = {{{RAPA}}-{{ConvNets}}},
  author = {Rasch, Malte J. and Gokmen, Tayfun and Rigotti, Mattia and Haensch, Wilfried},
  year = {2019},
  volume = {13},
  issn = {1662-453X},
  doi = {10.3389/fnins.2019.00753},
  abstract = {Analog arrays are a promising emerging hardware technology with the potential to drastically speed up deep learning. Their main advantage is that they employ analog circuitry to compute matrix-vector products in constant time, irrespective of the size of the matrix. However, ConvNets map very unfavorably onto analog arrays when done in a straight-forward manner, because kernel matrices are typically small and the constant time operation needs to be sequentially iterated a large number of times. Here, we propose to parallelize the training by replicating the kernel matrix of a convolution layer on distinct analog arrays, and randomly divide parts of the compute among them. With this modification, analog arrays execute ConvNets with a large acceleration factor that is proportional to the number of kernel matrices used per layer (here tested 16-128). Despite having more free parameters, we show analytically and in numerical experiments that this new convolution architecture is self-regularizing and implicitly learns similar filters across arrays. We also report superior performance on a number of datasets and increased robustness to adversarial attacks. Our investigation suggests to revise the notion that emerging architectures that feature analog arrays for fast matrix-vector multiplication are not suitable for ConvNets.},
  file = {/Users/x0r/Zotero/storage/N2FEUZLS/Rasch et al_2019_RAPA-ConvNets.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic,To read},
  language = {English}
}

@article{raty2015,
  title = {Aging Mechanisms in Amorphous Phase-Change Materials},
  author = {Raty, Jean Yves and Zhang, Wei and Luckas, Jennifer and Chen, Chao and Mazzarello, Riccardo and Bichara, Christophe and Wuttig, Matthias},
  year = {2015},
  month = dec,
  volume = {6},
  issn = {2041-1723},
  doi = {10.1038/ncomms8467},
  file = {/Users/x0r/Zotero/storage/WJDVSKNK/Raty et al_2015_Aging mechanisms in amorphous phase-change materials.pdf},
  journal = {Nature Communications},
  keywords = {drift,PCM},
  language = {en},
  number = {1}
}

@article{real2020,
  title = {{{AutoML}}-{{Zero}}: {{Evolving Machine Learning Algorithms From Scratch}}},
  shorttitle = {{{AutoML}}-{{Zero}}},
  author = {Real, Esteban and Liang, Chen and So, David R. and Le, Quoc V.},
  year = {2020},
  month = mar,
  abstract = {Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.},
  archivePrefix = {arXiv},
  eprint = {2003.03384},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/V4DDPLQB/Real et al. - 2020 - AutoML-Zero Evolving Machine Learning Algorithms .pdf;/Users/x0r/Zotero/storage/M6LLHQQ7/2003.html},
  journal = {arXiv:2003.03384 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{rebecq,
  title = {High {{Speed}} and {{High Dynamic Range Video}} with an {{Event Camera}}},
  author = {Rebecq, Henri and Ranftl, Rene and Koltun, Vladlen and Scaramuzza, Davide},
  pages = {26},
  abstract = {Event cameras are novel sensors that report brightness changes in the form of a stream of asynchronous ``events'' instead of intensity frames. They offer significant advantages with respect to conventional cameras: high temporal resolution, high dynamic range, and no motion blur. While the stream of events encodes in principle the complete visual signal, the reconstruction of an intensity image from a stream of events is an ill-posed problem in practice. Existing reconstruction approaches are based on hand-crafted priors and strong assumptions about the imaging process as well as the statistics of natural images. In this work we propose to learn to reconstruct intensity images from event streams directly from data instead of relying on any hand-crafted priors. We propose a novel recurrent network to reconstruct videos from a stream of events, and train it on a large amount of simulated event data. During training we propose to use a perceptual loss to encourage reconstructions to follow natural image statistics. We further extend our approach to synthesize color images from color event streams. Our quantitative experiments show that our network surpasses state-of-the-art reconstruction methods by a large margin in terms of image quality ({$>$} 20\%), while comfortably running in real-time. We show that the network is able to synthesize high framerate videos ({$>$} 5,000 frames per second) of high-speed phenomena (e.g. a bullet hitting an object) and is able to provide high dynamic range reconstructions in challenging lighting conditions. As an additional contribution, we demonstrate the effectiveness of our reconstructions as an intermediate representation for event data. We show that off-the-shelf computer vision algorithms can be applied to our reconstructions for tasks such as object classification and visual-inertial odometry and that this strategy consistently outperforms algorithms that were specifically designed for event data. We release the reconstruction code and a pre-trained model to enable further research.},
  file = {/Users/x0r/Zotero/storage/BBLNCDI3/Rebecq et al_High Speed and High Dynamic Range Video with an Event Camera.pdf},
  keywords = {DVS,neuromorphic,SNN},
  language = {en}
}

@article{reddy2019,
  title = {Learning {{Human Objectives}} by {{Evaluating Hypothetical Behavior}}},
  author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey and Legg, Shane and Leike, Jan},
  year = {2019},
  month = dec,
  abstract = {We seek to align agent behavior with a user's objectives in a reinforcement learning setting with unknown dynamics, an unknown reward function, and unknown unsafe states. The user knows the rewards and unsafe states, but querying the user is expensive. To address this challenge, we propose an algorithm that safely and interactively learns a model of the user's reward function. We start with a generative model of initial states and a forward dynamics model trained on off-policy data. Our method uses these models to synthesize hypothetical behaviors, asks the user to label the behaviors with rewards, and trains a neural network to predict the rewards. The key idea is to actively synthesize the hypothetical behaviors from scratch by maximizing tractable proxies for the value of information, without interacting with the environment. We call this method reward query synthesis via trajectory optimization (ReQueST). We evaluate ReQueST with simulated users on a statebased 2D navigation task and the image-based Car Racing video game. The results show that ReQueST significantly outperforms prior methods in learning reward models that transfer to new environments with different initial state distributions. Moreover, ReQueST safely trains the reward model to detect unsafe states, and corrects reward hacking before deploying the agent.},
  archivePrefix = {arXiv},
  eprint = {1912.05652},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/4BRSYSEE/Reddy et al. - 2019 - Learning Human Objectives by Evaluating Hypothetic.pdf},
  journal = {arXiv:1912.05652 [cs, stat]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{reifenberg2006,
  title = {Multiphysics {{Modeling}} and {{Impact}} of {{Thermal Boundary Resistance}} in {{Phase Change Memory Devices}}},
  author = {Reifenberg, J. and Pop, E. and Gibby, A. and Wong, S. and Goodson, K.},
  year = {2006},
  pages = {106--113},
  publisher = {{IEEE}},
  doi = {10.1109/ITHERM.2006.1645329},
  abstract = {Among the many emerging non-volatile memory technologies, chalcogenide (i.e. GeSbTe/GST) based phase change random access memory (PRAM) has shown particular promise. While accurate simulations are required for reducing programming current and enabling higher integration density, many challenges remain for improved simulation of PRAM cell operation including nanoscale thermal conduction and phase change. This work simulates the fully coupled electrical and thermal transport and phase change in 2D PRAM geometries, with specific attention to the impact of thermal boundary resistance between the GST and surrounding materials. For GST layer thicknesses between 25 and 75nm, the interface resistance reduces the predicted programming current and power by 31\% and 53\%, respectively, for a typical reset transition. The calculations also show the large sensitivity of programming voltage to the GST thermal conductivity. These results show the importance of temperature-dependent thermal properties of materials and interfaces in PRAM cells.},
  file = {/Users/x0r/Zotero/storage/LPFNGY79/Reifenberg et al_2006_Multiphysics Modeling and Impact of Thermal Boundary Resistance in Phase Change.pdf},
  isbn = {978-0-7803-9524-4},
  keywords = {modeling,PCM},
  language = {en}
}

@article{reifenberg2010,
  title = {Thermal {{Boundary Resistance Measurements}} for {{Phase}}-{{Change Memory Devices}}},
  author = {Reifenberg, J.P. and {Kuo-Wei Chang} and Panzer, M.A. and {Sangbum Kim} and Rowlette, J.A. and Asheghi, M. and Wong, H.-S.P. and Goodson, K.E.},
  year = {2010},
  month = jan,
  volume = {31},
  pages = {56--58},
  issn = {0741-3106, 1558-0563},
  doi = {10.1109/LED.2009.2035139},
  abstract = {Thermal interfaces play a key role in determining the programming energy of phase-change memory (PCM) devices. This letter reports the picosecond thermoreflectance measurements of thermal boundary resistance (TBR) at TiN/GST and Al/TiN interfaces, as well as the intrinsic thermal conductivity measurements of fcc GST between 30 {$\smwhtcircle$}C and 325 {$\smwhtcircle$}C. The TiN/GST TBR decreases with temperature from {$\sim$}26 to {$\sim$}18 m2 {$\cdot$} K/GW, and the Al/TiN ranges from {$\sim$}7 to 2.4 m2 {$\cdot$} K/GW. A TBR of 10 m2 {$\cdot$} K/GW is equivalent in thermal resistance to {$\sim$}192 nm of TiN. The fcc GST conductivity increases with temperature between {$\sim$}0.44 and 0.59 W/m/K. A detailed understanding of TBR is essential for optimizing the PCM technology.},
  file = {/Users/x0r/Zotero/storage/BQYCCPPW/Reifenberg et al_2010_Thermal Boundary Resistance Measurements for Phase-Change Memory Devices.pdf},
  journal = {IEEE Electron Device Letters},
  keywords = {modeling,PCM},
  language = {en},
  number = {1}
}

@article{rezende2014,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = may,
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  archivePrefix = {arXiv},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/RQJZNCWG/Rezende et al. - 2014 - Stochastic Backpropagation and Approximate Inferen.pdf},
  journal = {arXiv:1401.4082 [cs, stat]},
  keywords = {dl,generative models,To read},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rezende2016,
  title = {Unsupervised {{Learning}} of {{3D Structure}} from {{Images}}},
  author = {Rezende, Danilo Jimenez and Eslami, S. M. Ali and Mohamed, Shakir and Battaglia, Peter and Jaderberg, Max and Heess, Nicolas},
  year = {2016},
  month = jul,
  abstract = {A key goal of computer vision is to recover the underlying 3D structure from 2D observations of the world. In this paper we learn strong deep generative models of 3D structures, and recover these structures from 3D and 2D images via probabilistic inference. We demonstrate high-quality samples and report log-likelihoods on several datasets, including ShapeNet [2], and establish the first benchmarks in the literature. We also show how these models and their inference networks can be trained end-to-end from 2D images. This demonstrates for the first time the feasibility of learning to infer 3D representations of the world in a purely unsupervised manner.},
  archivePrefix = {arXiv},
  eprint = {1607.00662},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/WKLV3DW3/Rezende et al_2016_Unsupervised Learning of 3D Structure from Images.pdf},
  journal = {arXiv:1607.00662 [cs, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, stat}
}

@article{richards2019,
  title = {Dendritic Solutions to the Credit Assignment Problem},
  author = {Richards, Blake A and Lillicrap, Timothy P},
  year = {2019},
  month = feb,
  volume = {54},
  pages = {28--36},
  issn = {09594388},
  doi = {10.1016/j.conb.2018.08.003},
  file = {/Users/x0r/Zotero/storage/JC3QBH9Q/Richards and Lillicrap - 2019 - Dendritic solutions to the credit assignment probl.pdf},
  journal = {Current Opinion in Neurobiology},
  keywords = {neuroscience},
  language = {en}
}

@article{richards2019a,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  volume = {22},
  pages = {1761--1770},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0520-2},
  file = {/Users/x0r/Zotero/storage/Y7VQ8SBW/Richards et al. - 2019 - A deep learning framework for neuroscience.pdf},
  journal = {Nature Neuroscience},
  keywords = {dl,neuroscience},
  language = {en},
  number = {11}
}

@article{rinke2017,
  title = {A Scalable Algorithm for Simulating the Structural Plasticity of the Brain},
  author = {Rinke, Sebastian and {Butz-Ostendorf}, Markus and Hermanns, Marc-Andr{\'e} and Naveau, Mika{\"e}l and Wolf, Felix},
  year = {2017},
  month = dec,
  issn = {07437315},
  doi = {10.1016/j.jpdc.2017.11.019},
  abstract = {The neural network in the brain is not hard-wired. Even in the mature brain, new connections between neurons are formed and existing ones are deleted, which is called structural plasticity. The dynamics of the connectome is key to understanding how learning, memory, and healing after lesions such as stroke work. However, with current experimental techniques even the creation of an exact static connectivity map, which is required for various brain simulations, is very difficult. One alternative is to use network models to simulate the evolution of synapses between neurons based on their specified activity targets. This is particularly useful as experimental measurements of the spiking frequency of neurons are more easily accessible and reliable than biological connectivity data. The Model of Structural Plasticity (MSP) by Butz and van Ooyen is an example of this approach. However, to predict which neurons connect to each other, the current MSP model computes probabilities for all pairs of neurons, resulting in a complexity O(n2). To enable large-scale simulations with millions of neurons and beyond, this quadratic term is prohibitive. Inspired by hierarchical methods for solving n-body problems in particle physics, we propose a scalable approximation algorithm for MSP that reduces the complexity to O(n log2n) without any notable impact on the quality of the results. We show that an MPI-based parallel implementation of our scalable algorithm can simulate the structural plasticity of up to 109 neurons\textemdash four orders of magnitude more than the na\"ive O(n2) version.},
  file = {/Users/x0r/Zotero/storage/JK85ZL6V/Rinke et al_2017_A scalable algorithm for simulating the structural plasticity of the brain.pdf},
  journal = {Journal of Parallel and Distributed Computing},
  keywords = {neuroscience},
  language = {en}
}

@article{rios2018,
  title = {In-Memory Computing on a Photonic Platform},
  author = {R{\'i}os, Carlos and Youngblood, Nathan and Cheng, Zengguang and Gallo, Manuel Le and Pernice, Wolfram H. P. and Wright, C. David and Sebastian, Abu and Bhaskaran, Harish},
  year = {2018},
  month = jan,
  abstract = {Collocated data processing and storage are the norm in biological systems. Indeed, the von Neumann computing architecture, that physically and temporally separates processing and memory, was born more of pragmatism based on available technology. As our ability to create better hardware improves, new computational paradigms are being explored. Integrated photonic circuits are regarded as an attractive solution for on-chip computing using only light, leveraging the increased speed and bandwidth potential of working in the optical domain, and importantly, removing the need for time and energy sapping electro-optical conversions. Here we show that we can combine the emerging area of integrated optics with collocated data storage and processing to enable all-photonic in-memory computations. By employing non-volatile photonic elements based on the phase-change material, Ge2Sb2Te5, we are able to achieve direct scalar multiplication on single devices. Featuring a novel single-shot Write/Erase and a drift-free process, such elements can multiply two scalar numbers by mapping their values to the energy of an input pulse and to the transmittance of the device, codified in the crystallographic state of the element. The output pulse, carrying the information of the light-matter interaction, is the result of the computation. Our all-optical approach is novel, easy to fabricate and operate, and sets the stage for development of entirely photonic computers.},
  archivePrefix = {arXiv},
  eprint = {1801.06228},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/FAGDX79W/Ríos et al_2018_In-memory computing on a photonic platform.pdf;/Users/x0r/Zotero/storage/IE2IL3DV/1801.html},
  journal = {arXiv:1801.06228 [physics]},
  keywords = {neuromorphic,photonics},
  primaryClass = {physics}
}

@article{risk2009,
  title = {Thermal Conductivities and Phase Transition Temperatures of Various Phase-Change Materials Measured by the 3{$\omega$} Method},
  author = {Risk, W. P. and Rettner, C. T. and Raoux, S.},
  year = {2009},
  month = mar,
  volume = {94},
  pages = {101906},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.3097353},
  file = {/Users/x0r/Zotero/storage/AD7MMTNQ/Risk et al_2009_Thermal conductivities and phase transition temperatures of various.pdf;/Users/x0r/Zotero/storage/N8URTQXL/Risk et al_2009_Thermal conductivities and phase transition temperatures of various.pdf},
  journal = {Applied Physics Letters},
  keywords = {modeling,PCM},
  language = {en},
  number = {10}
}

@article{robertson2004,
  title = {High Dielectric Constant Oxides},
  author = {Robertson, J.},
  year = {2004},
  month = dec,
  volume = {28},
  pages = {265--291},
  issn = {1286-0042, 1286-0050},
  doi = {10.1051/epjap:2004206},
  abstract = {The scaling of complementary metal oxide semiconductor (CMOS) transistors has led to the silicon dioxide layer used as a gate dielectric becoming so thin (1.4 nm) that its leakage current is too large. It is necessary to replace the SiO2 with a physically thicker layer of oxides of higher dielectric constant ({$\kappa$}) or `high K' gate oxides such as hafnium oxide and hafnium silicate. Little was known about such oxides, and it was soon found that in many respects they have inferior electronic properties to SiO2, such as a tendency to crystallise and a high concentration of electronic defects. Intensive research is underway to develop these oxides into new high quality electronic materials. This review covers the choice of oxides, their structural and metallurgical behaviour, atomic diffusion, their deposition, interface structure and reactions, their electronic structure, bonding, band offsets, mobility degradation, flat band voltage shifts and electronic defects. The use of high K oxides in capacitors of dynamic random access memories is also covered.},
  file = {/Users/x0r/Zotero/storage/ZMTBCYFB/Robertson_2004_High dielectric constant oxides.pdf},
  journal = {The European Physical Journal Applied Physics},
  keywords = {material},
  language = {en},
  number = {3}
}

@article{roelfsema2005,
  title = {Attention-{{Gated Reinforcement Learning}} of {{Internal Representations}} for {{Classification}}},
  author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
  year = {2005},
  month = oct,
  volume = {17},
  pages = {2176--2214},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766054615699},
  file = {/Users/x0r/Zotero/storage/CUDWX3MA/Roelfsema and Ooyen - 2005 - Attention-Gated Reinforcement Learning of Internal.pdf},
  journal = {Neural Computation},
  keywords = {back-propagation,SNN,To read},
  language = {en},
  number = {10}
}

@article{roelfsema2005a,
  title = {Attention-{{Gated Reinforcement Learning}} of {{Internal Representations}} for {{Classification}}},
  author = {Roelfsema, Pieter R. and van Ooyen, Arjen},
  year = {2005},
  month = oct,
  volume = {17},
  pages = {2176--2214},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/0899766054615699},
  file = {/Users/x0r/Zotero/storage/VVYK6IUN/Roelfsema and Ooyen - 2005 - Attention-Gated Reinforcement Learning of Internal.pdf},
  journal = {Neural Computation},
  keywords = {To read},
  language = {en},
  number = {10}
}

@article{roelfsema2018,
  title = {Control of Synaptic Plasticity in Deep Cortical Networks},
  author = {Roelfsema, Pieter R. and Holtmaat, Anthony},
  year = {2018},
  month = mar,
  volume = {19},
  pages = {166--180},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2018.6},
  abstract = {Humans and many other animals have an enormous capacity to learn about sensory stimuli and to master new skills. However, many of the mechanisms that enable us to learn remain to be understood. One of the greatest challenges of systems neuroscience is to explain how synaptic connections change to support maximally adaptive behaviour. Here, we provide an overview of factors that determine the change in the strength of synapses, with a focus on synaptic plasticity in sensory cortices. We review the influence of neuromodulators and feedback connections in synaptic plasticity and suggest a specific framework in which these factors can interact to improve the functioning of the entire network.},
  file = {/Users/x0r/Zotero/storage/WU2PEJLE/Roelfsema and Holtmaat - 2018 - Control of synaptic plasticity in deep cortical ne.pdf},
  journal = {Nature Reviews Neuroscience},
  language = {en},
  number = {3}
}

@article{rosenbaum2018,
  title = {Learning Models for Visual {{3D}} Localization with Implicit Mapping},
  author = {Rosenbaum, Dan and Besse, Frederic and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali},
  year = {2018},
  month = jul,
  abstract = {We propose a formulation of visual localization that does not require construction of explicit maps in the form of point clouds or voxels. The goal is to learn an implicit representation of the environment at a higher, more abstract level, for instance that of objects. To study this approach we consider procedurally generated Minecraft worlds, for which we can generate visually rich images along with camera pose coordinates. We first show that Generative Query Networks (GQNs) enhanced with a novel attention mechanism can capture the visual structure of 3D scenes in Minecraft, as evidenced by their samples. We then apply the models to the localization problem, investigating both generative and discriminative approaches, and compare the different ways in which they each capture task uncertainty. Our results show that models with implicit mapping are able to capture the underlying 3D structure of visually complex scenes, and use this to accurately localize new observations, paving the way towards future applications in sequential localization. Supplementary video available at https://youtu.be/iHEXX5wXbCI.},
  archivePrefix = {arXiv},
  eprint = {1807.03149},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/S8VH43NA/Rosenbaum et al_2018_Learning models for visual 3D localization with implicit mapping.pdf},
  journal = {arXiv:1807.03149 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{rossum2001,
  title = {A {{Novel Spike Distance}}},
  author = {van Rossum, M. C. W.},
  year = {2001},
  month = apr,
  volume = {13},
  pages = {751--763},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976601300014321},
  file = {/Users/x0r/Zotero/storage/N8W8CXBI/Rossum_2001_A Novel Spike Distance.pdf;/Users/x0r/Zotero/storage/VXXCJXIR/Rossum_2001_A Novel Spike Distance.pdf},
  journal = {Neural Computation},
  keywords = {backprop,SNN},
  language = {en},
  number = {4}
}

@article{rouhani2020,
  title = {Reward Prediction Errors Create Event Boundaries in Memory},
  author = {Rouhani, Nina and Norman, Kenneth A. and Niv, Yael and Bornstein, Aaron M.},
  year = {2020},
  month = oct,
  volume = {203},
  pages = {104269},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104269},
  abstract = {We remember when things change. Particularly salient are experiences where there is a change in rewards, eliciting reward prediction errors (RPEs). How do RPEs influence our memory of those experiences? One idea is that this signal directly enhances the encoding of memory. Another, not mutually exclusive, idea is that the RPE signals a deeper change in the environment, leading to the mnemonic separation of subsequent experiences from what came before, thereby creating a new latent context and a more separate memory trace. We tested this in four experiments where participants learned to predict rewards associated with a series of trial-unique images. High-magnitude RPEs indicated a change in the underlying distribution of rewards. To test whether these large RPEs created a new latent context, we first assessed recognition priming for sequential pairs that included a high-RPE event or not (Exp. 1: n~=~27 \& Exp. 2: n~=~83). We found evidence of recognition priming for the high-RPE event, indicating that the high-RPE event is bound to its predecessor in memory. Given that high-RPE events are themselves preferentially remembered (Rouhani, Norman, \& Niv, 2018), we next tested whether there was an event boundary across a high-RPE event (i.e., excluding the high-RPE event itself; Exp. 3: n~=~85). Here, sequential pairs across a high RPE no longer showed recognition priming whereas pairs within the same latent reward state did, providing initial evidence for an RPE-modulated event boundary. We then investigated whether RPE event boundaries disrupt temporal memory by asking participants to order and estimate the distance between two events that had either included a high-RPE event between them or not (Exp. 4). We found (n~=~49) and replicated (n~=~77) worse sequence memory for events across a high RPE. In line with our recognition priming results, we did not find sequence memory to be impaired between the high-RPE event and its predecessor, but instead found worse sequence memory for pairs across a high-RPE event. Moreover, greater distance between events at encoding led to better sequence memory for events across a low-RPE event, but not a high-RPE event, suggesting separate mechanisms for the temporal ordering of events within versus across a latent reward context. Altogether, these findings demonstrate that high-RPE events are both more strongly encoded, show intact links with their predecessor, and act as event boundaries that interrupt the sequential integration of events. We captured these effects in a variant of the Context Maintenance and Retrieval model (CMR; Polyn, Norman, \& Kahana, 2009), modified to incorporate RPEs into the encoding process.},
  file = {/Users/x0r/Zotero/storage/W42YUMU2/S0010027720300883.html},
  journal = {Cognition},
  language = {en}
}

@article{rueckert2016,
  title = {Recurrent {{Spiking Networks Solve Planning Tasks}}},
  author = {Rueckert, Elmar and Kappel, David and Tanneberg, Daniel and Pecevski, Dejan and Peters, Jan},
  year = {2016},
  month = aug,
  volume = {6},
  issn = {2045-2322},
  doi = {10.1038/srep21142},
  file = {/Users/x0r/Zotero/storage/EI6ZIJW2/Rueckert et al_2016_Recurrent Spiking Networks Solve Planning Tasks.pdf},
  journal = {Scientific Reports},
  keywords = {rl,SNN},
  language = {en},
  number = {1}
}

@article{rusu2015,
  title = {Policy {{Distillation}}},
  author = {Rusu, Andrei A. and Colmenarejo, Sergio Gomez and Gulcehre, Caglar and Desjardins, Guillaume and Kirkpatrick, James and Pascanu, Razvan and Mnih, Volodymyr and Kavukcuoglu, Koray and Hadsell, Raia},
  year = {2015},
  month = nov,
  abstract = {Policies for complex visual tasks have been successfully learned with deep reinforcement learning, using an approach called deep Q-networks (DQN), but relatively large (task-specific) networks and extensive training are needed to achieve good performance. In this work, we present a novel method called policy distillation that can be used to extract the policy of a reinforcement learning agent and train a new network that performs at the expert level while being dramatically smaller and more efficient. Furthermore, the same method can be used to consolidate multiple task-specific policies into a single policy. We demonstrate these claims using the Atari domain and show that the multi-task distilled agent outperforms the single-task teachers as well as a jointly-trained DQN agent.},
  archivePrefix = {arXiv},
  eprint = {1511.06295},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/XXAAHZ5W/Rusu et al_2015_Policy Distillation.pdf},
  journal = {arXiv:1511.06295 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{rusu2016,
  title = {Progressive {{Neural Networks}}},
  author = {Rusu, Andrei A. and Rabinowitz, Neil C. and Desjardins, Guillaume and Soyer, Hubert and Kirkpatrick, James and Kavukcuoglu, Koray and Pascanu, Razvan and Hadsell, Raia},
  year = {2016},
  month = jun,
  abstract = {Learning to solve complex sequences of tasks\textemdash while both leveraging transfer and avoiding catastrophic forgetting\textemdash remains a key obstacle to achieving human-level intelligence. The progressive networks approach represents a step forward in this direction: they are immune to forgetting and can leverage prior knowledge via lateral connections to previously learned features. We evaluate this architecture extensively on a wide variety of reinforcement learning tasks (Atari and 3D maze games), and show that it outperforms common baselines based on pretraining and finetuning. Using a novel sensitivity measure, we demonstrate that transfer occurs at both low-level sensory and high-level control layers of the learned policy.},
  archivePrefix = {arXiv},
  eprint = {1606.04671},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/K7R2TY7D/Rusu et al_2016_Progressive Neural Networks.pdf},
  journal = {arXiv:1606.04671 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{rutten2015,
  title = {Relation between Bandgap and Resistance Drift in Amorphous Phase Change Materials},
  author = {R{\"u}tten, Martin and Kaes, Matthias and Albert, Andreas and Wuttig, Matthias and Salinga, Martin},
  year = {2015},
  month = dec,
  volume = {5},
  issn = {2045-2322},
  doi = {10.1038/srep17362},
  file = {/Users/x0r/Zotero/storage/4UJ9NAU2/Rütten et al_2015_Relation between bandgap and resistance drift in amorphous phase change.pdf;/Users/x0r/Zotero/storage/WWIW6ALH/Rütten et al_2015_Relation between bandgap and resistance drift in amorphous phase change.pdf},
  journal = {Scientific Reports},
  keywords = {PCM},
  language = {en},
  number = {1}
}

@article{sabour2017,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = oct,
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archivePrefix = {arXiv},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/IL64M6I3/Sabour et al_2017_Dynamic Routing Between Capsules.pdf},
  journal = {arXiv:1710.09829 [cs]},
  keywords = {capsnet,dl},
  language = {en},
  primaryClass = {cs}
}

@article{sacramento2018,
  title = {Dendritic Cortical Microcircuits Approximate the Backpropagation Algorithm},
  author = {Sacramento, Jo{\~a}o and Costa, Rui Ponte and Bengio, Yoshua and Senn, Walter},
  year = {2018},
  month = oct,
  abstract = {Deep learning has seen remarkable developments over the last years, many of them inspired by neuroscience. However, the main learning mechanism behind these advances - error backpropagation - appears to be at odds with neurobiology. Here, we introduce a multilayer neuronal network model with simplified dendritic compartments in which error-driven synaptic plasticity adapts the network towards a global desired output. In contrast to previous work our model does not require separate phases and synaptic learning is driven by local dendritic prediction errors continuously in time. Such errors originate at apical dendrites and occur due to a mismatch between predictive input from lateral interneurons and activity from actual top-down feedback. Through the use of simple dendritic compartments and different cell-types our model can represent both error and normal activity within a pyramidal neuron. We demonstrate the learning capabilities of the model in regression and classification tasks, and show analytically that it approximates the error backpropagation algorithm. Moreover, our framework is consistent with recent observations of learning between brain areas and the architecture of cortical microcircuits. Overall, we introduce a novel view of learning on dendritic cortical circuits and on how the brain may solve the long-standing synaptic credit assignment problem.},
  archivePrefix = {arXiv},
  eprint = {1810.11393},
  eprinttype = {arxiv},
  journal = {arXiv:1810.11393 [cs, q-bio]},
  keywords = {neuromorphic,To read},
  primaryClass = {cs, q-bio}
}

@article{salimans2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = mar,
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Qlearning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archivePrefix = {arXiv},
  eprint = {1703.03864},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9PDWYGLF/Salimans et al_2017_Evolution Strategies as a Scalable Alternative to Reinforcement Learning.pdf},
  journal = {arXiv:1703.03864 [cs, stat]},
  keywords = {evolutionary-strategies},
  language = {en},
  primaryClass = {cs, stat}
}

@article{salinga2018,
  title = {Monatomic Phase Change Memory},
  author = {Salinga, Martin and Kersting, Benedikt and Ronneberger, Ider and Jonnalagadda, Vara Prasad and Vu, Xuan Thang and Le Gallo, Manuel and Giannopoulos, Iason and {Cojocaru-Mir{\'e}din}, Oana and Mazzarello, Riccardo and Sebastian, Abu},
  year = {2018},
  month = aug,
  volume = {17},
  pages = {681--685},
  issn = {1476-1122, 1476-4660},
  doi = {10.1038/s41563-018-0110-9},
  file = {/Users/x0r/Zotero/storage/N2A7Q3S8/Salinga et al_2018_Monatomic phase change memory.pdf},
  journal = {Nature Materials},
  keywords = {PCM},
  language = {en},
  number = {8}
}

@article{santoro2017,
  title = {A Simple Neural Network Module for Relational Reasoning},
  author = {Santoro, Adam and Raposo, David and Barrett, David G. T. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  year = {2017},
  month = jun,
  abstract = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  archivePrefix = {arXiv},
  eprint = {1706.01427},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/PRQMXYIS/Santoro et al_2017_A simple neural network module for relational reasoning.pdf},
  journal = {arXiv:1706.01427 [cs]},
  keywords = {dl,To read},
  language = {en},
  primaryClass = {cs}
}

@article{saunders2019,
  title = {Locally {{Connected Spiking Neural Networks}} for {{Unsupervised Feature Learning}}},
  author = {Saunders, Daniel J. and Patel, Devdhar and Hazan, Hananel and Siegelmann, Hava T. and Kozma, Robert},
  year = {2019},
  month = apr,
  abstract = {In recent years, Spiking Neural Networks (SNNs) have demonstrated great successes in completing various Machine Learning tasks. We introduce a method for learning image features by \textbackslash textit\{locally connected layers\} in SNNs using spike-timing-dependent plasticity (STDP) rule. In our approach, sub-networks compete via competitive inhibitory interactions to learn features from different locations of the input space. These \textbackslash textit\{Locally-Connected SNNs\} (LC-SNNs) manifest key topological features of the spatial interaction of biological neurons. We explore biologically inspired n-gram classification approach allowing parallel processing over various patches of the the image space. We report the classification accuracy of simple two-layer LC-SNNs on two image datasets, which match the state-of-art performance and are the first results to date. LC-SNNs have the advantage of fast convergence to a dataset representation, and they require fewer learnable parameters than other SNN approaches with unsupervised learning. Robustness tests demonstrate that LC-SNNs exhibit graceful degradation of performance despite the random deletion of large amounts of synapses and neurons.},
  archivePrefix = {arXiv},
  eprint = {1904.06269},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/BMZHCBIG/Saunders et al_2019_Locally Connected Spiking Neural Networks for Unsupervised Feature Learning.pdf},
  journal = {arXiv:1904.06269 [cs]},
  primaryClass = {cs}
}

@article{sawa2008,
  title = {Resistive Switching in Transition Metal Oxides},
  author = {Sawa, Akihito},
  year = {2008},
  month = jun,
  volume = {11},
  pages = {28--36},
  issn = {13697021},
  doi = {10.1016/S1369-7021(08)70119-6},
  file = {/Users/x0r/Zotero/storage/XEYWSFUV/Sawa_2008_Resistive switching in transition metal oxides.pdf},
  journal = {Materials Today},
  language = {en},
  number = {6}
}

@article{saxton2019,
  title = {Analysing {{Mathematical Reasoning Abilities}} of {{Neural Models}}},
  author = {Saxton, David and Grefenstette, Edward and Hill, Felix and Kohli, Pushmeet},
  year = {2019},
  month = apr,
  abstract = {Mathematical reasoning---a core ability within human intelligence---presents some unique challenges as a domain: we do not come to understand and solve mathematical problems primarily on the back of experience and evidence, but on the basis of inferring, learning, and exploiting laws, axioms, and symbol manipulation rules. In this paper, we present a new challenge for the evaluation (and eventually the design) of neural architectures and similar system, developing a task suite of mathematics problems involving sequential questions and answers in a free-form textual input/output format. The structured nature of the mathematics domain, covering arithmetic, algebra, probability and calculus, enables the construction of training and test splits designed to clearly illuminate the capabilities and failure-modes of different architectures, as well as evaluate their ability to compose and relate knowledge and learned processes. Having described the data generation process and its potential future expansions, we conduct a comprehensive analysis of models from two broad classes of the most powerful sequence-to-sequence architectures and find notable differences in their ability to resolve mathematical problems and generalize their knowledge.},
  archivePrefix = {arXiv},
  eprint = {1904.01557},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/B282HMQS/Saxton et al_2019_Analysing Mathematical Reasoning Abilities of Neural Models.pdf},
  journal = {arXiv:1904.01557 [cs, stat]},
  keywords = {AGI,dl},
  primaryClass = {cs, stat}
}

@article{schacter2012,
  title = {The {{Future}} of {{Memory}}: {{Remembering}}, {{Imagining}}, and the {{Brain}}},
  shorttitle = {The {{Future}} of {{Memory}}},
  author = {Schacter, Daniel L. and Addis, Donna Rose and Hassabis, Demis and Martin, Victoria C. and Spreng, R. Nathan and Szpunar, Karl K.},
  year = {2012},
  month = nov,
  volume = {76},
  pages = {677--694},
  issn = {08966273},
  doi = {10.1016/j.neuron.2012.11.001},
  file = {/Users/x0r/Zotero/storage/WEV33WQ9/Schacter et al_2012_The Future of Memory.pdf},
  journal = {Neuron},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {4}
}

@article{schaul,
  title = {Universal {{Value Function Approximators}}},
  author = {Schaul, Tom and Horgan, Dan and Gregor, Karol and Silver, David},
  pages = {9},
  abstract = {Value functions are a core component of reinforcement learning systems. The main idea is to to construct a single function approximator V (s; \texttheta ) that estimates the long-term reward from any state s, using parameters \texttheta. In this paper we introduce universal value function approximators (UVFAs) V (s, g; \texttheta ) that generalise not just over states s but also over goals g. We develop an efficient technique for supervised learning of UVFAs, by factoring observed values into separate embedding vectors for state and goal, and then learning a mapping from s and g to these factored embedding vectors. We show how this technique may be incorporated into a reinforcement learning algorithm that updates the UVFA solely from observed rewards. Finally, we demonstrate that a UVFA can successfully generalise to previously unseen goals.},
  file = {/Users/x0r/Zotero/storage/MNJKTX5D/Schaul et al_Universal Value Function Approximators.pdf},
  keywords = {dl},
  language = {en}
}

@techreport{scherr2020,
  title = {One-Shot Learning with Spiking Neural Networks},
  author = {Scherr, Franz and Stoeckl, Christoph and Maass, Wolfgang},
  year = {2020},
  month = jun,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.06.17.156513},
  abstract = {Understanding how one-shot learning can be accomplished through synaptic plasticity in neural networks of the brain is a major open problem. We propose that approximations to BPTT in recurrent networks of spiking neurons (RSNNs) such as e-prop cannot achieve this because their local synaptic plasticity is gated by learning signals that are rather ad hoc from a biological perspective: Random projections of instantaneously arising losses at the network outputs, analogously as in Broadcast Alignment for feedforward networks. In contrast, synaptic plasticity is gated in the brain by learning signals such as dopamine, which are emitted by specialized brain areas, e.g. VTA. These brain areas have arguably been optimized by evolution to gate synaptic plasticity in such a way that fast learning of survivalrelevant tasks is enabled. We found that a corresponding model architecture, where learning signals are emitted by a separate RSNN that is optimized to facilitate fast learning, enables one-shot learning via local synaptic plasticity in RSNNs for large families of learning tasks. The same learning approach also supports fast spike-based learning of posterior probabilities of potential input sources, thereby providing a new basis for probabilistic reasoning in RSNNs. Our new learning approach also solves an open problem in neuromorphic engineering, where onchip one-shot learning capability is highly desirable for spike-based neuromorphic devices, but could so far not be achieved. Our method can easily be mapped into neuromorphic hardware, and thereby solves this problem.},
  file = {/Users/x0r/Zotero/storage/USCUDAYP/Scherr et al. - 2020 - One-shot learning with spiking neural networks.pdf},
  language = {en},
  type = {Preprint}
}

@article{schmidhuber,
  title = {Making the {{World Differentiable}}: {{On Using Self}}-{{Supervised Fully Recurrent Neural Networks}} for {{Dynamic Reinforcement Learning}} and {{Planning}} in {{Non}}-{{Stationary Environments}}},
  author = {Schmidhuber, Jiirgen},
  pages = {26},
  file = {/Users/x0r/Zotero/storage/EFWY8VKI/Schmidhuber_Making the World Differentiable.pdf},
  language = {en}
}

@article{schmidhuber1991,
  title = {A {{Possibility}} for {{Implementing Curiosity}} and {{Boredom}} in {{Model}}-{{Building Neural Controler}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {1991},
  file = {/Users/x0r/Zotero/storage/454J9AWY/Schmidhuber_1991_A Possibility for Implementing Curiosity and Boredom in Model-Building Neural.pdf}
}

@article{schmidhuber1997,
  title = {What's Interesting?},
  author = {Schmidhuber, J{\"u}rgen},
  year = {1997},
  file = {/Users/x0r/Zotero/storage/Q8AJL6TB/Schmidhuber_1997_What's interesting.pdf},
  keywords = {To read}
}

@article{schmidhuber2003,
  title = {Exploring the {{Predictable}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2003},
  file = {/Users/x0r/Zotero/storage/84LD65CM/Schmidhuber_2003_Exploring the Predictable.pdf}
}

@article{schmidhuber2011,
  title = {{{POWERPLAY}}: {{Training}} an {{Increasingly General Problem Solver}} by {{Continually Searching}} for the {{Simplest Still Unsolvable Problem}}},
  shorttitle = {{{POWERPLAY}}},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2011},
  month = dec,
  abstract = {Most of computer science focuses on automatically solving given computational problems. I focus on automatically inventing or discovering problems in a way inspired by the playful behavior of animals and humans, to train a more and more general problem solver from scratch in an unsupervised fashion. Consider the infinite set of all computable descriptions of tasks with possibly computable solutions. Given a general problem solving architecture, at any given time, the novel algorithmic framework POWERPLAY [46] searches the space of possible pairs of new tasks and modifications of the current problem solver, until it finds a more powerful problem solver that provably solves all previously learned tasks plus the new one, while the unmodified predecessor does not. Newly invented tasks may require to achieve a wow-effect by making previously learned skills more efficient such that they require less time and space. New skills may (partially) re-use previously learned skills. The greedy search of typical POWERPLAY variants uses time-optimal program search to order candidate pairs of tasks and solver modifications by their conditional computational (time \& space) complexity, given the stored experience so far. The new task and its corresponding task-solving skill are those first found and validated. This biases the search towards pairs that can be described compactly and validated quickly. The computational costs of validating new tasks need not grow with task repertoire size. Standard problem solver architectures of personal computers or neural networks tend to generalize by solving numerous tasks outside the self-invented training set; POWERPLAY's ongoing search for novelty keeps breaking the generalization abilities of its present solver. This is related to Go\textasciidieresis del's sequence of increasingly powerful formal theories based on adding formerly unprovable statements to the axioms without affecting previously provable theorems. The continually increasing repertoire of problem solving procedures can be exploited by a parallel search for solutions to additional externally posed tasks. POWERPLAY may be viewed as a greedy but practical implementation of basic principles of creativity [42, 45]. A first experimental analysis can be found in separate papers [53, 52].},
  archivePrefix = {arXiv},
  eprint = {1112.5309},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/USQ8HLL4/Schmidhuber_2011_POWERPLAY.pdf},
  journal = {arXiv:1112.5309 [cs]},
  keywords = {AGI,rl,To read},
  language = {en},
  primaryClass = {cs}
}

@article{schmidhubera,
  title = {Artificial {{Curiosity Based}} on {{Discovering Novel Algorithmic Predictability Through Coevolution}}},
  author = {Schmidhuber, Jurgen},
  pages = {7},
  abstract = {How to explore a spatio-temporal simulations include an example where surprisedomain? By predicting and learning from suc- generation of this kind helps to speed up external cess/failure what's predictable and what's not. reward.},
  file = {/Users/x0r/Zotero/storage/PINKVSA6/Schmidhuber_Artificial Curiosity Based on Discovering Novel Algorithmic Predictability.pdf},
  language = {en}
}

@book{schoner2016,
  title = {Dynamic Thinking: A Primer on Dynamic Field Theory},
  shorttitle = {Dynamic Thinking},
  author = {Sch{\"o}ner, Gregor},
  year = {2016},
  publisher = {{Oxford University Press}},
  address = {{Oxford ; New York}},
  file = {/Users/x0r/Zotero/storage/AB9B2GH6/Schöner - 2016 - Dynamic thinking a primer on dynamic field theory.pdf},
  isbn = {978-0-19-930056-3},
  language = {en},
  lccn = {BF201 .S36 2016},
  series = {Oxford Series in Developmental Cognitive Neuroscience}
}

@article{schrimpf2018,
  title = {Brain-{{Score}}: {{Which Artificial Neural Network}} for {{Object Recognition}} Is Most {{Brain}}-{{Like}}?},
  shorttitle = {Brain-{{Score}}},
  author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and {Prescott-Roy}, Jonathan and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
  year = {2018},
  month = sep,
  pages = {407007},
  doi = {10.1101/407007},
  abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score - a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain's mechanisms for core object recognition - and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at {$>$}= 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain's network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
  copyright = {\textcopyright{} 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  file = {/Users/x0r/Zotero/storage/EYLP54UU/Schrimpf et al_2018_Brain-Score.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{schulman2015,
  title = {Trust {{Region Policy Optimization}}},
  author = {Schulman, John and Levine, Sergey and Moritz, Philipp and Jordan, Michael I. and Abbeel, Pieter},
  year = {2015},
  month = feb,
  abstract = {We describe an iterative procedure for optimizing policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified procedure, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is similar to natural policy gradient methods and is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  archivePrefix = {arXiv},
  eprint = {1502.05477},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/UH32Y9ZU/Schulman et al_2015_Trust Region Policy Optimization.pdf},
  journal = {arXiv:1502.05477 [cs]},
  keywords = {rl,spinning-up},
  primaryClass = {cs}
}

@article{schulman2015a,
  title = {High-{{Dimensional Continuous Control Using Generalized Advantage Estimation}}},
  author = {Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  year = {2015},
  month = jun,
  abstract = {Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks. Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.},
  archivePrefix = {arXiv},
  eprint = {1506.02438},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/C3HV3CND/Schulman et al_2015_High-Dimensional Continuous Control Using Generalized Advantage Estimation.pdf},
  journal = {arXiv:1506.02438 [cs]},
  keywords = {spinning-up},
  primaryClass = {cs}
}

@article{schulman2017,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = jul,
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a ``surrogate'' objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archivePrefix = {arXiv},
  eprint = {1707.06347},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/M5R2BPB8/Schulman et al_2017_Proximal Policy Optimization Algorithms.pdf},
  journal = {arXiv:1707.06347 [cs]},
  keywords = {rl,spinning-up},
  language = {en},
  primaryClass = {cs}
}

@article{schuman2017,
  title = {A {{Survey}} of {{Neuromorphic Computing}} and {{Neural Networks}} in {{Hardware}}},
  author = {Schuman, Catherine D. and Potok, Thomas E. and Patton, Robert M. and Birdwell, J. Douglas and Dean, Mark E. and Rose, Garrett S. and Plank, James S.},
  year = {2017},
  month = may,
  abstract = {Neuromorphic computing has come to refer to a variety of brain-inspired computers, devices, and models that contrast the pervasive von Neumann computer architecture. This biologically inspired approach has created highly connected synthetic neurons and synapses that can be used to model neuroscience theories as well as solve challenging machine learning problems. The promise of the technology is to create a brainlike ability to learn and adapt, but the technical challenges are significant, starting with an accurate neuroscience model of how the brain works, to finding materials and engineering breakthroughs to build devices to support these models, to creating a programming framework so the systems can learn, to creating applications with brain-like capabilities. In this work, we provide a comprehensive survey of the research and motivations for neuromorphic computing over its history. We begin with a 35-year review of the motivations and drivers of neuromorphic computing, then look at the major research areas of the field, which we define as neuro-inspired models, algorithms and learning approaches, hardware and devices, supporting systems, and finally applications. We conclude with a broad discussion on the major research topics that need to be addressed in the coming years to see the promise of neuromorphic computing fulfilled. The goals of this work are to provide an exhaustive review of the research conducted in neuromorphic computing since the inception of the term, and to motivate further work by illuminating gaps in the field where new research is needed.},
  archivePrefix = {arXiv},
  eprint = {1705.06963},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9WXVFMT8/Schuman et al_2017_A Survey of Neuromorphic Computing and Neural Networks in Hardware.pdf},
  journal = {arXiv:1705.06963 [cs]},
  keywords = {neuromorphic,perf},
  language = {en},
  primaryClass = {cs}
}

@article{Sebastian_etal14,
  ids = {sebastian2014},
  title = {Crystal Growth within a Phase Change Memory Cell},
  author = {Sebastian, Abu and Le Gallo, Manuel and Krebs, Daniel},
  year = {2014},
  volume = {5},
  pages = {4314},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/DMPH6P9N/Sebastian et al_2014_Crystal growth within a phase change memory cell.pdf},
  journal = {Nature communications},
  keywords = {modeling,PCM}
}

@article{Sebastian_etal17,
  ids = {sebastian2017},
  title = {Temporal Correlation Detection Using Computational Phase-Change Memory},
  author = {Sebastian, Abu and Tuma, Tomas and Papandreou, Nikolaos and Le Gallo, Manuel and Kull, Lukas and Parnell, Thomas and Eleftheriou, Evangelos},
  year = {2017},
  volume = {8},
  pages = {1115},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/IPA4CLNB/Sebastian et al_2017_Temporal correlation detection using computational phase-change memory.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic,PCM},
  number = {1}
}

@inproceedings{sebastian2015,
  title = {A Collective Relaxation Model for Resistance Drift in Phase Change Memory Cells},
  author = {Sebastian, Abu and Krebs, Daniel and Le Gallo, Manuel and Pozidis, Haralampos and Eleftheriou, Evangelos},
  year = {2015},
  month = apr,
  pages = {MY.5.1-MY.5.6},
  publisher = {{IEEE}},
  doi = {10.1109/IRPS.2015.7112808},
  abstract = {Phase change memory (PCM) cells rely on the orders of magnitude difference in resistivity between the crystalline and amorphous phases to store information. However, the temporal evolution of the resistance of the amorphous phase, commonly referred to as resistance drift, is a key challenge for the realization of multi-level PCM. In this article, we present a comprehensive description of the time-temperature dependence of the resistance variation in a PCM cell. Our model consists of a structural relaxation model and an electrical transport model. The structural relaxation model is based on the idea that the atomic configuration of the melt-quenched amorphous phase as a whole collectively relaxes towards a more favorable equilibrium state. Experimental results obtained over a wide range of temperatures and times show remarkable agreement with the proposed model.},
  file = {/Users/x0r/Zotero/storage/6C3QUNIL/Sebastian et al_2015_A collective relaxation model for resistance drift in phase change memory cells.pdf},
  isbn = {978-1-4673-7362-3},
  keywords = {drift,modeling,PCM},
  language = {en}
}

@article{sebastian2016,
  title = {Multi-Level Storage in Phase-Change Memory Devices},
  author = {Sebastian, A and Gallo, M Le and Koelmans, W W and Papandreou, N and Pozidis, H and Eleftheriou, E},
  year = {2016},
  pages = {6},
  abstract = {Phase-change memory (PCM) is a promising technology for both storage class memory and emerging nonvon Neumann computing systems. For both applications, a key enabling technology is the ability to store multiple resistance levels in a single device. Multi-level storage is achieved by modulating the size of the crystalline/amorphous phase configuration. A key challenge, in this respect, is the device variability, which can be addressed by iterative programming schemes. When retrieving the stored information, the two additional challenges are resistance drift and low-frequency noise. Resistance drift is attributed to a spontaneous structural relaxation of the unstable amorphous states to a more stable ``ideal glass'' state and is well captured by a collective relaxation model. This model, in conjunction with the electrical transport models, provides a complete description of the time/temperature dependence of electrical transport in PCM devices. To counter resistance drift, several strategies have been devised, such as drift-resilient read-out mechanisms as well as coding and detection schemes. These techniques have helped to demonstrate storing up to 8 levels of information in a single PCM device. Yet another fascinating new approach is that of driftresilient device architectures. Experimental results on prototype devices show remarkable promise in terms of eliminating drift as well as low-frequency noise.},
  file = {/Users/x0r/Zotero/storage/W46ME22G/Sebastian et al_2016_Multi-level storage in phase-change memory devices.pdf},
  keywords = {MLC,PCM},
  language = {en}
}

@article{senkader2004,
  title = {Models for Phase-Change of {{Ge2Sb2Te5}} in Optical and Electrical Memory Devices},
  author = {Senkader, S. and Wright, C. D.},
  year = {2004},
  month = jan,
  volume = {95},
  pages = {504--511},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.1633984},
  file = {/Users/x0r/Zotero/storage/N3TGL7P5/Senkader_Wright_2004_Models for phase-change of Ge2Sb2Te5 in optical and electrical memory devices.pdf},
  journal = {Journal of Applied Physics},
  keywords = {GST},
  language = {en},
  number = {2}
}

@article{seo2015,
  title = {On-{{Chip Sparse Learning Acceleration With CMOS}} and {{Resistive Synaptic Devices}}},
  author = {Seo, Jae-sun and Lin, Binbin and Kim, Minkyu and Chen, Pai-Yu and Kadetotad, Deepak and Xu, Zihan and Mohanty, Abinash and Vrudhula, Sarma and Yu, Shimeng and Ye, Jieping and Cao, Yu},
  year = {2015},
  month = nov,
  volume = {14},
  pages = {969--979},
  issn = {1536-125X, 1941-0085},
  doi = {10.1109/TNANO.2015.2478861},
  abstract = {Many recent advances in sparse coding led its wide adoption in signal processing, pattern classification, and object recognition applications. Even with improved performance in state-of-the-art algorithms and the hardware platform of CPUs/GPUs, solving a sparse coding problem still requires expensive computations, making real-time large-scale learning a very challenging problem. In this paper, we cooptimize algorithm, architecture, circuit, and device for real-time energy-efficient on-chip hardware acceleration of sparse coding. The principle of hardware acceleration is to recognize the properties of learning algorithms, which involve many parallel operations of data fetch and matrix/vector multiplication/addition. Today's von Neumann architecture, however, is not suitable for such parallelization, due to the separation of memory and the computing unit that makes sequential operations inevitable. Such principle drives both the selection of algorithms and the design evolution from CPU to CMOS application-specific integrated circuits (ASIC) to parallel architecture with resistive crosspoint array (PARCA) that we propose. The CMOS ASIC scheme implements sparse coding with SRAM dictionaries and all-digital circuits, and PARCA employs resistive-RAM dictionaries with special read and write circuits. We show that 65 nm implementation of the CMOS ASIC and PARCA scheme accelerates sparse coding computation by 394 and 2140\texttimes, respectively, compared to software running on a eight-core CPU. Simulated power for both hardware schemes lie in the milli-Watt range, making it viable for portable single-chip learning applications.},
  file = {/Users/x0r/Zotero/storage/YWK9EKAM/Seo et al_2015_On-Chip Sparse Learning Acceleration With CMOS and Resistive Synaptic Devices.pdf},
  journal = {IEEE Transactions on Nanotechnology},
  keywords = {neuromorphic},
  language = {en},
  number = {6}
}

@article{seok2014,
  title = {A {{Review}} of {{Three}}-{{Dimensional Resistive Switching Cross}}-{{Bar Array Memories}} from the {{Integration}} and {{Materials Property Points}} of {{View}}},
  author = {Seok, Jun Yeong and Song, Seul Ji and Yoon, Jung Ho and Yoon, Kyung Jean and Park, Tae Hyung and Kwon, Dae Eun and Lim, Hyungkwang and Kim, Gun Hwan and Jeong, Doo Seok and Hwang, Cheol Seong},
  year = {2014},
  month = sep,
  volume = {24},
  pages = {5316--5339},
  issn = {1616301X},
  doi = {10.1002/adfm.201303520},
  file = {/Users/x0r/Zotero/storage/9JPD3LCE/Seok et al_2014_A Review of Three-Dimensional Resistive Switching Cross-Bar Array Memories from.pdf;/Users/x0r/Zotero/storage/C592QRJN/Seok et al_2014_A Review of Three-Dimensional Resistive Switching Cross-Bar Array Memories from.pdf},
  journal = {Advanced Functional Materials},
  keywords = {neuromorphic},
  language = {en},
  number = {34}
}

@article{Serb_etal16,
  ids = {serb2016},
  title = {Unsupervised Learning in Probabilistic Neural Networks with Multi-State Metal-Oxide Memristive Synapses},
  author = {Serb, Alexander and Bill, Johannes and Khiat, Ali and Berdan, Radu and Legenstein, Robert and Prodromakis, Themis},
  year = {2016},
  volume = {7},
  pages = {12611},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/WQ5QBHAE/Serb et al_2016_Unsupervised learning in probabilistic neural networks with multi-state.pdf},
  journal = {Nature communications},
  keywords = {neuromorphic,STDP}
}

@article{serb2020,
  title = {Memristive Synapses Connect Brain and Silicon Spiking Neurons},
  author = {Serb, Alexantrou and Corna, Andrea and George, Richard and Khiat, Ali and Rocchi, Federico and Reato, Marco and Maschietto, Marta and Mayr, Christian and Indiveri, Giacomo and Vassanelli, Stefano and Prodromakis, Themistoklis},
  year = {2020},
  month = dec,
  volume = {10},
  pages = {2590},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-58831-9},
  file = {/Users/x0r/Zotero/storage/RVQH9IJB/Serb et al. - 2020 - Memristive synapses connect brain and silicon spik.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{sercu2016,
  title = {Very {{Deep Multilingual Convolutional Neural Networks}} for {{LVCSR}}},
  author = {Sercu, Tom and Puhrsch, Christian and Kingsbury, Brian and LeCun, Yann},
  year = {2016},
  month = jan,
  abstract = {Convolutional neural networks (CNNs) are a standard component of many current state-of-the-art Large Vocabulary Continuous Speech Recognition (LVCSR) systems. However, CNNs in LVCSR have not kept pace with recent advances in other domains where deeper neural networks provide superior performance. In this paper we propose a number of architectural advances in CNNs for LVCSR. First, we introduce a very deep convolutional network architecture with up to 14 weight layers. There are multiple convolutional layers before each pooling layer, with small 3\texttimes 3 kernels, inspired by the VGG Imagenet 2014 architecture. Then, we introduce multilingual CNNs with multiple untied layers. Finally, we introduce multi-scale input features aimed at exploiting more context at negligible computational cost. We evaluate the improvements first on a Babel task for low resource speech recognition, obtaining an absolute 5.77\% WER improvement over the baseline PLP DNN by training our CNN on the combined data of six different languages. We then evaluate the very deep CNNs on the Hub5'00 benchmark (using the 262 hours of SWB-1 training data) achieving a word error rate of 11.8\% after cross-entropy training, a 1.4\% WER improvement (10.6\% relative) over the best published CNN result so far.},
  archivePrefix = {arXiv},
  eprint = {1509.08967},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Y2HD4FPR/Sercu et al. - 2016 - Very Deep Multilingual Convolutional Neural Networ.pdf},
  journal = {arXiv:1509.08967 [cs]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs}
}

@article{Serrano-Gotarredona_etal13,
  ids = {serrano-gotarredona2013},
  title = {{{STDP}} and {{STDP}} Variations with Memristors for Spiking Neuromorphic Learning Systems},
  author = {{Serrano-Gotarredona}, T. and Masquelier, T. and Prodromakis, T. and Indiveri, G. and {Linares-Barranco}, B.},
  year = {2013},
  volume = {7},
  issn = {1662-453X},
  doi = {10.3389/fnins.2013.00002},
  abstract = {In this paper we review several ways of realizing asynchronous Spike-Timing Dependent Plasticity (STDP) using memristors as synapses. Our focus is on how to use individual memristors to implement synaptic weight multiplications, in a way such that it is not necessary to (a) introduce global synchronization and (b) to separate memristor learning phases from memristor performing phases. In the approaches described, neurons fire spikes asynchronously when they wish and memristive synapses perform computation and learn at their own pace, as it happens in biological neural systems. We distinguish between two different memristor physics, depending on whether they respond to the original ``moving wall'' or to the ``filament creation and annihilation'' models. Independent of the memristor physics, we discuss two different types of STDP rules that can be implemented with memristors: either the pure timing-based rule that takes into account the arrival time of the spikes from the pre- and the post-synaptic neurons, or a hybrid rule that takes into account only the timing of pre-synaptic spikes and the membrane potential and other state variables of the post-synaptic neuron. We show how to implement these rules in cross-bar architectures that comprise massive arrays of memristors, and we discuss applications for artificial vision.},
  file = {/Users/x0r/Zotero/storage/I3BCUG8W/Serrano-Gotarredona et al_2013_STDP and STDP variations with memristors for spiking neuromorphic learning.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic,STDP},
  number = {2}
}

@article{severa2019,
  title = {Whetstone: {{A Method}} for {{Training Deep Artificial Neural Networks}} for {{Binary Communication}}},
  shorttitle = {Whetstone},
  author = {Severa, William and Vineyard, Craig M. and Dellana, Ryan and Verzi, Stephen J. and Aimone, James B.},
  year = {2019},
  month = jan,
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0015-y},
  abstract = {This paper presents a new technique for training networks for low-precision communication. Targeting minimal communication between nodes not only enables the use of emerging spiking neuromorphic platforms, but may additionally streamline processing conventionally. Low-power and embedded neuromorphic processors potentially offer dramatic performance-per-Watt improvements over traditional von Neumann processors, however programming these brain-inspired platforms generally requires platform-specific expertise which limits their applicability. To date, the majority of artificial neural networks have not operated using discrete spike-like communication. We present a method for training deep spiking neural networks using an iterative modification of the backpropagation optimization algorithm. This method, which we call Whetstone, effectively and reliably configures a network for a spiking hardware target with little, if any, loss in performance. Whetstone networks use single time step binary communication and do not require a rate code or other spike-based coding scheme, thus producing networks comparable in timing and size to conventional ANNs, albeit with binarized communication. We demonstrate Whetstone on a number of image classification networks, describing how the sharpening process interacts with different training optimizers and changes the distribution of activity within the network. We further note that Whetstone is compatible with several non-classification neural network applications, such as autoencoders and semantic segmentation. Whetstone is widely extendable and currently implemented using custom activation functions within the Keras wrapper to the popular TensorFlow machine learning framework.},
  archivePrefix = {arXiv},
  eprint = {1810.11521},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/2LASFAQT/Severa et al_2019_Whetstone.pdf},
  journal = {Nature Machine Intelligence}
}

@inproceedings{shahrabi2018,
  title = {The Key Impact of Incorporated {{Al2O3}} Barrier Layer on {{W}}-Based {{ReRAM}} Switching Performance},
  booktitle = {2018 14th {{Conference}} on {{Ph}}.{{D}}. {{Research}} in {{Microelectronics}} and {{Electronics}} ({{PRIME}})},
  author = {Shahrabi, Elmira and Giovinazzo, Cecilia and Sandrini, Jury and Leblebici, Yusuf},
  year = {2018},
  month = jul,
  pages = {69--72},
  publisher = {{IEEE}},
  address = {{Prague}},
  doi = {10.1109/PRIME.2018.8430371},
  abstract = {In this article, we inspected the bipolar resistive switching behavior of W-based ReRAMs, using HfO2 as switching layer. We have shown that the switching properties can be significantly enhanced by incorporating an Al2O3 layer as a barrier layer. It stabilizes the resistance states and lowers the operating current. Al2O3 acts as an oxygen scavenging blocking layer at W sides, results in the filament path constriction at the Al2O3/HfO2 interface. This leads to the more controllable reset operation and consecutively the HRS properties improvement. This allows the W/Al2O3/HfO2/Pt to switch at 10 times lower operating current of 100 {$\mu$}A and 2 times higher memory window compared to the W/HfO2/Pt stacks. The LRS conduction of devices with the barrier layer is in perfect agreement with the Poole-Frenkel model.},
  file = {/Users/x0r/Zotero/storage/FTHMK4WS/Shahrabi et al_2018_The key impact of incorporated Al2O3 barrier layer on W-based ReRAM switching.pdf},
  isbn = {978-1-5386-5387-6},
  keywords = {ReRAM},
  language = {en}
}

@article{shai2015,
  title = {Physiology of {{Layer}} 5 {{Pyramidal Neurons}} in {{Mouse Primary Visual Cortex}}: {{Coincidence Detection}} through {{Bursting}}},
  shorttitle = {Physiology of {{Layer}} 5 {{Pyramidal Neurons}} in {{Mouse Primary Visual Cortex}}},
  author = {Shai, Adam S. and Anastassiou, Costas A. and Larkum, Matthew E. and Koch, Christof},
  editor = {Sporns, Olaf},
  year = {2015},
  month = mar,
  volume = {11},
  pages = {e1004090},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004090},
  file = {/Users/x0r/Zotero/storage/KL2XG9VM/Shai et al. - 2015 - Physiology of Layer 5 Pyramidal Neurons in Mouse P.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {3}
}

@article{shainline2018,
  title = {Superconducting {{Optoelectronic Neurons I}}: {{General Principles}}},
  shorttitle = {Superconducting {{Optoelectronic Neurons I}}},
  author = {Shainline, Jeffrey M. and Buckley, Sonia M. and McCaughan, Adam N. and Chiles, Jeff and Mirin, Richard P. and Nam, Sae Woo},
  year = {2018},
  month = may,
  abstract = {The design of neural hardware is informed by the prominence of differentiated processing and information integration in cognitive systems. The central role of communication leads to the principal assumption of the hardware platform: signals between neurons should be optical to enable fanout and communication with minimal delay. The requirement of energy efficiency leads to the utilization of superconducting detectors to receive single-photon signals. We discuss the potential of superconducting optoelectronic hardware to achieve the spatial and temporal information integration advantageous for cognitive processing, and we consider physical scaling limits based on light-speed communication. We introduce the superconducting optoelectronic neurons and networks that are the subject of the subsequent papers in this series.},
  archivePrefix = {arXiv},
  eprint = {1805.01929},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/UEUKTX9V/Shainline et al_2018_Superconducting Optoelectronic Neurons I.pdf},
  journal = {arXiv:1805.01929 [cs]},
  keywords = {neuromorphic,optoelectronic},
  language = {en},
  primaryClass = {cs}
}

@article{shanks1970,
  title = {Ovonic Threshold Switching Characteristics},
  author = {Shanks, Roy R.},
  year = {1970},
  month = jan,
  volume = {2},
  pages = {504--514},
  issn = {00223093},
  doi = {10.1016/0022-3093(70)90164-X},
  file = {/Users/x0r/Zotero/storage/M52YJ5H4/Shanks_1970_Ovonic threshold switching characteristics.pdf},
  journal = {Journal of Non-Crystalline Solids},
  keywords = {modeling,OTS},
  language = {en}
}

@inproceedings{sharifshazileh2019,
  title = {A {{Neuromorphic Device}} for {{Detecting High}}-{{Frequency Oscillations}} in {{Human iEEG}}},
  booktitle = {2019 26th {{IEEE International Conference}} on {{Electronics}}, {{Circuits}} and {{Systems}} ({{ICECS}})},
  author = {Sharifshazileh, Mohammadali and Burelo, Karla and Fedele, Tommaso and Sarnthein, Johannes and Indiveri, Giacomo},
  year = {2019},
  month = nov,
  pages = {69--72},
  publisher = {{IEEE}},
  address = {{Genoa, Italy}},
  doi = {10.1109/ICECS46596.2019.8965192},
  abstract = {Among diagnostic biomarkers, high frequency oscillations in human iEEG are used to identify epileptogenic brain tissue during epilepsy surgery. However, current methods typically analyse the raw data offline using complex time-consuming algorithms. We developed a compact neuromorphic sensoryprocessing system-on-chip that can monitor the iEEG signals and detect high frequency oscillations in real-time using spiking neural networks. To this end, we present an integrated device with an analog frontend that can extract predefined spectral features and encode them as address-events, and a neuromorphic processor core that implements a network of integrate and fire neurons with dynamic synapses. The device was fabricated using a standard 0.18\dbend m CMOS technology node. The estimated power consumption of the front-end is 6.2\dbend W /channel and the area-on-chip for a single channel is 0.15 square millimetres. The SNN classifier provides 90.5\% sensitivity and 67.7\% specificity for detecting high frequency oscillations. This is the first feasibility study towards identifying relevant features in intracranial human data in real-time on-chip using event-base processors.},
  file = {/Users/x0r/Zotero/storage/7L49SI6J/Sharifshazileh et al. - 2019 - A Neuromorphic Device for Detecting High-Frequency.pdf},
  isbn = {978-1-72810-996-1},
  language = {en}
}

@article{shazeer2018,
  title = {Mesh-{{TensorFlow}}: {{Deep Learning}} for {{Supercomputers}}},
  shorttitle = {Mesh-{{TensorFlow}}},
  author = {Shazeer, Noam and Cheng, Youlong and Parmar, Niki and Tran, Dustin and Vaswani, Ashish and Koanantakool, Penporn and Hawkins, Peter and Lee, HyoukJoong and Hong, Mingsheng and Young, Cliff and Sepassi, Ryan and Hechtman, Blake},
  year = {2018},
  month = nov,
  abstract = {Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer [21] sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 Englishto-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at https://github.com/tensorflow/mesh .},
  archivePrefix = {arXiv},
  eprint = {1811.02084},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/76JE8XR9/Shazeer et al_2018_Mesh-TensorFlow.pdf},
  journal = {arXiv:1811.02084 [cs, stat]},
  keywords = {dl,neuromorphic},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sheeny,
  title = {{{POL}}-{{LWIR Vehicle Detection}}: {{Convolutional Neural Networks Meet Polarised Infrared Sensors}}},
  author = {Sheeny, Marcel and Wallace, Andrew and Emambakhsh, Mehryar and Wang, Sen and Connor, Barry},
  pages = {7},
  file = {/Users/x0r/Zotero/storage/ICHEN679/Sheeny et al_POL-LWIR Vehicle Detection.pdf},
  keywords = {dl,polarcam},
  language = {en}
}

@inproceedings{shen2013,
  title = {Study on Temperature Oscillation Phenomenon in {{FEM}} Numerical Simulation},
  author = {Shen, Bo and Fan, Tao and Pan, Zaiyong and {jin}, Quanlin and Wang, Xinyun},
  year = {2013},
  pages = {1112--1117},
  doi = {10.1063/1.4806960},
  abstract = {The reasons of temperature field oscillation in numerical simulation are studied in this paper by theoretical analysis and simple calculation examples. The temperature derivative of an actual temperature field is continuous, but after FEM discretization, temperature derivative is not continuous on the both side of inner boundaries. It means heat flux vectors flowing in and out an inner boundary are not equal, which lead to temperature oscillation of numerical simulation result. After programing verification, it shows that replacing coordinate heat capacity matrix by lumped heat capacity matrix can reduce temperature oscillation effectively.},
  file = {/Users/x0r/Zotero/storage/EJUR5MUZ/Shen et al_2013_Study on temperature oscillation phenomenon in FEM numerical simulation.pdf},
  keywords = {modeling},
  language = {en}
}

@article{shen2017,
  title = {Deep Learning with Coherent Nanophotonic Circuits},
  author = {Shen, Yichen and Harris, Nicholas C. and Skirlo, Scott and Prabhu, Mihika and {Baehr-Jones}, Tom and Hochberg, Michael and Sun, Xin and Zhao, Shijie and Larochelle, Hugo and Englund, Dirk and Solja{\v c}i{\'c}, Marin},
  year = {2017},
  month = jul,
  volume = {11},
  pages = {441--446},
  issn = {1749-4893},
  doi = {10.1038/nphoton.2017.93},
  abstract = {Artificial neural networks are computational network models inspired by signal processing in the brain. These models have dramatically improved performance for many machine-learning tasks, including speech and image recognition. However, today's computing hardware is inefficient at implementing neural networks, in large part because much of it was designed for von Neumann computing schemes. Significant effort has been made towards developing electronic architectures tuned to implement artificial neural networks that exhibit improved computational speed and accuracy. Here, we propose a new architecture for a fully optical neural network that, in principle, could offer an enhancement in computational speed and power efficiency over state-of-the-art electronics for conventional inference tasks. We experimentally demonstrate the essential part of the concept using a programmable nanophotonic processor featuring a cascaded array of 56 programmable Mach\textendash Zehnder interferometers in a silicon photonic integrated circuit and show its utility for vowel recognition.},
  copyright = {2017 Nature Publishing Group},
  file = {/Users/x0r/Zotero/storage/SX9RRAHE/Shen et al_2017_Deep learning with coherent nanophotonic circuits.pdf},
  journal = {Nature Photonics},
  keywords = {dl,photonics},
  language = {en},
  number = {7}
}

@article{sheridan2016,
  title = {Feature {{Extraction Using Memristor Networks}}},
  author = {Sheridan, Patrick M. and Du, Chao and Lu, Wei D.},
  year = {2016},
  month = nov,
  volume = {27},
  pages = {2327--2336},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2015.2482220},
  abstract = {Crossbar arrays of memristive elements are investigated for the implementation of dictionary learning and sparse coding of natural images. A winner-take-all training algorithm, in conjunction with Oja's rule, is used to learn an overcomplete dictionary of feature primitives that resemble Gabor filters. The dictionary is then used in the locally competitive algorithm to form a sparse representation of input images. The impacts of device nonlinearity and parameter variations are evaluated and a compensating procedure is proposed to ensure the robustness of the sparsification. It is shown that, with proper compensation, the memristor crossbar architecture can effectively perform sparse coding with distortion comparable with ideal software implementations at high sparsity, even in the presence of large device-to-device variations in the excess of 100\%.},
  file = {/Users/x0r/Zotero/storage/NE4D4DLH/Sheridan et al_2016_Feature Extraction Using Memristor Networks.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {neuromorphic},
  language = {en},
  number = {11}
}

@article{shi2018,
  title = {Electronic Synapses Made of Layered Two-Dimensional Materials},
  author = {Shi, Yuanyuan and Liang, Xianhu and Yuan, Bin and Chen, Victoria and Li, Haitong and Hui, Fei and Yu, Zhouchangwan and Yuan, Fang and Pop, Eric and Wong, H.-S. Philip and Lanza, Mario},
  year = {2018},
  month = aug,
  volume = {1},
  pages = {458--465},
  issn = {2520-1131},
  doi = {10.1038/s41928-018-0118-9},
  abstract = {Vertically structured electronic synapses, which exhibit both short- and long-term plasticity, can be created using layered two-dimensional hexagonal boron nitride.},
  copyright = {2018 The Author(s)},
  file = {/Users/x0r/Zotero/storage/HW7LVVR3/Shi et al_2018_Electronic synapses made of layered two-dimensional materials.pdf},
  journal = {Nature Electronics},
  language = {en},
  number = {8}
}

@article{shneidman1995,
  title = {Theory of Time-dependent Nucleation and Growth during a Rapid Quench},
  author = {Shneidman, Vitaly A.},
  year = {1995},
  month = dec,
  volume = {103},
  pages = {9772--9781},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.469941},
  file = {/Users/x0r/Zotero/storage/MTHPYXKI/Shneidman_1995_Theory of time‐dependent nucleation and growth during a rapid quench.pdf},
  journal = {The Journal of Chemical Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {22}
}

@article{shouval2010,
  title = {Spike Timing Dependent Plasticity: A Consequence of More Fundamental Learning Rules},
  shorttitle = {Spike Timing Dependent Plasticity},
  author = {Shouval, Harel},
  year = {2010},
  issn = {16625188},
  doi = {10.3389/fncom.2010.00019},
  abstract = {Spike timing dependent plasticity (STDP) is a phenomenon in which the precise timing of spikes affects the sign and magnitude of changes in synaptic strength. STDP is often interpreted as the comprehensive learning rule for a synapse \textendash{} the ``first law'' of synaptic plasticity. This interpretation is made explicit in theoretical models in which the total plasticity produced by complex spike patterns results from a superposition of the effects of all spike pairs. Although such models are appealing for their simplicity, they can fail dramatically. For example, the measured single-spike learning rule between hippocampal CA3 and CA1 pyramidal neurons does not predict the existence of long-term potentiation one of the best-known forms of synaptic plasticity. Layers of complexity have been added to the basic STDP model to repair predictive failures, but they have been outstripped by experimental data. We propose an alternate first law: neural activity triggers changes in key biochemical intermediates, which act as a more direct trigger of plasticity mechanisms. One particularly successful model uses intracellular calcium as the intermediate and can account for many observed properties of bidirectional plasticity. In this formulation, STDP is not itself the basis for explaining other forms of plasticity, but is instead a consequence of changes in the biochemical intermediate, calcium. Eventually a mechanism-based framework for learning rules should include other messengers, discrete change at individual synapses, spread of plasticity among neighboring synapses, and priming of hidden processes that change a synapse's susceptibility to future change. Mechanism-based models provide a rich framework for the computational representation of synaptic plasticity.},
  file = {/Users/x0r/Zotero/storage/Y3IUVJ8P/Shouval_2010_Spike timing dependent plasticity.pdf},
  journal = {Frontiers in Computational Neuroscience},
  keywords = {STDP},
  language = {en}
}

@article{shyam2018,
  title = {Model-{{Based Active Exploration}}},
  author = {Shyam, Pranav and Ja{\'s}kowski, Wojciech and Gomez, Faustino},
  year = {2018},
  month = oct,
  abstract = {Efficient exploration is an unsolved problem in Reinforcement Learning which is usually addressed by reactively rewarding the agent for fortuitously encountering novel situations. This paper introduces an efficient active exploration algorithm, Model-Based Active eXploration (MAX), which uses an ensemble of forward models to plan to observe novel events. This is carried out by optimizing agent behaviour with respect to a measure of novelty derived from the Bayesian perspective of exploration, which is estimated using the disagreement between the futures predicted by the ensemble members. We show empirically that in semi-random discrete environments where directed exploration is critical to make progress, MAX is at least an order of magnitude more efficient than strong baselines. MAX scales to high-dimensional continuous environments where it builds task-agnostic models that can be used for any downstream task.},
  archivePrefix = {arXiv},
  eprint = {1810.12162},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9HQ6X5MI/Shyam et al_2018_Model-Based Active Exploration.pdf},
  journal = {arXiv:1810.12162 [cs, math, stat]},
  keywords = {rl},
  primaryClass = {cs, math, stat}
}

@inproceedings{sidler2016,
  title = {Large-Scale Neural Networks Implemented with {{Non}}-{{Volatile Memory}} as the Synaptic Weight Element: {{Impact}} of Conductance Response},
  shorttitle = {Large-Scale Neural Networks Implemented with {{Non}}-{{Volatile Memory}} as the Synaptic Weight Element},
  author = {Sidler, Severin and Boybat, Irem and Shelby, Robert M. and Narayanan, Pritish and Jang, Junwoo and Fumarola, Alessandro and Moon, Kibong and Leblebici, Yusuf and Hwang, Hyunsang and Burr, Geoffrey W.},
  year = {2016},
  month = sep,
  pages = {440--443},
  publisher = {{IEEE}},
  doi = {10.1109/ESSDERC.2016.7599680},
  abstract = {We assess the impact of the conductance response of Non-Volatile Memory (NVM) devices employed as the synaptic weight element for on-chip acceleration of the training of large-scale artificial neural networks (ANN). We briefly review our previous work towards achieving competitive performance (classification accuracies) for such ANN with both Phase-Change Memory (PCM) [1], [2] and non-filamentary ReRAM based on PrCaMnO (PCMO) [3], and towards assessing the potential advantages for ML training over GPU\textendash based hardware in terms of speed (up to 25\texttimes{} faster) and power (from 120\textendash 2850\texttimes{} lower power) [4]. We then discuss the ``jump-table'' concept, previously introduced to model real-world NVM such as PCM [1] or PCMO, to describe the full cumulative distribution function (CDF) of conductance-change at each device conductance value, for both potentiation (SET) and depression (RESET). Using several types of artificially\textendash constructed jump-tables, we assess the relative importance of deviations from an ideal NVM with perfectly linear conductance response.},
  file = {/Users/x0r/Zotero/storage/M22L5F47/Sidler et al_2016_Large-scale neural networks implemented with Non-Volatile Memory as the.pdf},
  isbn = {978-1-5090-2969-3},
  keywords = {neuromorphic},
  language = {en}
}

@article{silver2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16961},
  file = {/Users/x0r/Zotero/storage/HHMYYS8Y/Silver et al_2016_Mastering the game of Go with deep neural networks and tree search.pdf},
  journal = {Nature},
  keywords = {dl},
  language = {en},
  number = {7587}
}

@article{silver2017,
  title = {Mastering the Game of {{Go}} without Human Knowledge},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and {van den Driessche}, George and Graepel, Thore and Hassabis, Demis},
  year = {2017},
  month = oct,
  volume = {550},
  pages = {354--359},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature24270},
  file = {/Users/x0r/Zotero/storage/YCXR8ZLU/Silver et al_2017_Mastering the game of Go without human knowledge.pdf},
  journal = {Nature},
  keywords = {rl,spinning-up},
  language = {en},
  number = {7676}
}

@article{silver2017a,
  title = {Mastering {{Chess}} and {{Shogi}} by {{Self}}-{{Play}} with a {{General Reinforcement Learning Algorithm}}},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2017},
  month = dec,
  abstract = {The game of chess is the most widely-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. In contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go, by tabula rasa reinforcement learning from games of self-play. In this paper, we generalise this approach into a single AlphaZero algorithm that can achieve, tabula rasa, superhuman performance in many challenging domains. Starting from random play, and given no domain knowledge except the game rules, AlphaZero achieved within 24 hours a superhuman level of play in the games of chess and shogi (Japanese chess) as well as Go, and convincingly defeated a world-champion program in each case.},
  archivePrefix = {arXiv},
  eprint = {1712.01815},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LF6PG9S8/Silver et al_2017_Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning.pdf},
  journal = {arXiv:1712.01815 [cs]},
  keywords = {rl,self-play,spinning-up},
  language = {en},
  primaryClass = {cs}
}

@article{silver2018,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  year = {2018},
  month = dec,
  volume = {362},
  pages = {1140--1144},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aar6404},
  abstract = {One program to rule them all
Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system.
Science, this issue p. 1140; see also pp. 1087 and 1118
The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.
AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each.},
  copyright = {Copyright \textcopyright{} 2018 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works. http://www.sciencemag.org/about/science-licenses-journal-article-reuseThis is an article distributed under the terms of the Science Journals Default License.},
  file = {/Users/x0r/Zotero/storage/IU5J4N56/Alphazero pseudocode.md;/Users/x0r/Zotero/storage/PLQ96Q52/Silver et al_2018_A general reinforcement learning algorithm that masters chess, shogi, and Go.pdf;/Users/x0r/Zotero/storage/YFIVASEG/Silver et al_2018_A general reinforcement learning algorithm that masters chess, shogi, and Go.pdf},
  journal = {Science},
  keywords = {AGI},
  language = {en},
  number = {6419}
}

@article{singh,
  title = {{{STABILITY LIMIT OF SUPERCOOLED LIQUIDS}}},
  author = {Singh, H B and Holz, A},
  pages = {4},
  file = {/Users/x0r/Zotero/storage/SLC9THGY/Singh_Holz_STABILITY LIMIT OF SUPERCOOLED LIQUIDS.pdf},
  keywords = {GST},
  language = {en}
}

@article{singhb,
  title = {Reinforcement Learning with Replacing Eligibility Traces},
  author = {Singh, Satinder P and Sutton, Richard S},
  pages = {36},
  abstract = {The eligibility trace is one of the basic mechanisms used in reinforcement learmng to handle delayed reward. In this paper we introduce a new kind of eligibility trace, the replacing trace, analyze it theoretically, and show that it results in faster, more reliable learning than the conventional trace. Both kinds of trace assign credit to prior events according to how recently they occurred, but only the conventional trace gives greater credit to repeated events. Our analysis is for conventional and replace-trace versions of the offline TD(1) algorithm applied to undiscounted absorbing Markov chains. First, we show that these methods converge under repeated presentations of the training set to the same predictions as two well known Monte Carlo methods. We then analyze the relative efficiency of the two Monte Carlo methods. We show that the method corresponding to conventional TD is biased, whereas the method corresponding to replace-trace TD is unbiased. In addition, we show that the method corresponding to replacing traces is closely related to the maximum likelihood solution for these tasks, and that its mean squared error is always lower in the long run. Computational results confirm these analyses and show that they are applicable more generally. In pa\textasciitilde icular, we show that replacing traces significantly improve performance and reduce parameter sensitivity on the "Mountain-Car" task, a full reinforcement-learning problem with a continuous state space, when using a feature-based function approximator.},
  file = {/Users/x0r/Zotero/storage/IWM9PQV8/Singh and Sutton - Reinforcement learning with replacing eligibility .pdf},
  language = {en}
}

@article{smith2018,
  title = {A {{BAYESIAN PERSPECTIVE ON GENERALIZATION AND STOCHASTIC GRADIENT DESCENT}}},
  author = {Smith, Samuel L and Le, Quoc V},
  year = {2018},
  pages = {13},
  file = {/Users/x0r/Zotero/storage/M7ZMZ6ES/Smith_Le_2018_A BAYESIAN PERSPECTIVE ON GENERALIZATION AND STOCHASTIC GRADIENT DESCENT.pdf},
  keywords = {dl},
  language = {en}
}

@article{sodhani2018,
  title = {Towards {{Training Recurrent Neural Networks}} for {{Lifelong Learning}}},
  author = {Sodhani, Shagun and Chandar, Sarath and Bengio, Yoshua},
  year = {2018},
  month = nov,
  abstract = {Catastrophic forgetting and capacity saturation are the central challenges of any parametric lifelong learning system. In this work, we study these challenges in the context of sequential supervised learning with an emphasis on recurrent neural networks. To evaluate the models in the lifelong learning setting, we propose a curriculum-based, simple, and intuitive benchmark where the models are trained on tasks with increasing levels of difficulty. To measure the impact of catastrophic forgetting, the model is tested on all the previous tasks as it completes any task. As a step towards developing true lifelong learning systems, we unify Gradient Episodic Memory (a catastrophic forgetting alleviation approach) and Net2Net (a capacity expansion approach). Both these models are proposed in the context of feedforward networks and we evaluate the feasibility of using them for recurrent networks. Evaluation on the proposed benchmark shows that the unified model is more suitable than the constituent models for lifelong learning setting.},
  archivePrefix = {arXiv},
  eprint = {1811.07017},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AJQYBBKP/Sodhani et al. - 2018 - Towards Training Recurrent Neural Networks for Lif.pdf},
  journal = {arXiv:1811.07017 [cs, stat]},
  keywords = {RNN,To read},
  language = {en},
  primaryClass = {cs, stat}
}

@article{Soudry_etal15,
  ids = {soudry2015},
  title = {Memristor-Based Multilayer Neural Networks with Online Gradient Descent Training},
  author = {Soudry, Daniel and Di Castro, Dotan and Gal, Asaf and Kolodny, Avinoam and Kvatinsky, Shahar},
  year = {2015},
  volume = {26},
  pages = {2408--2421},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/MT2BGNQ4/Soudry et al_2015_Memristor-Based Multilayer Neural Networks With Online Gradient Descent Training.pdf},
  journal = {IEEE transactions on neural networks and learning systems},
  keywords = {backprop,neuromorphic,SNN},
  number = {10}
}

@article{stachenfeld2017,
  title = {The Hippocampus as a Predictive Map},
  author = {Stachenfeld, Kimberly L and Botvinick, Matthew M and Gershman, Samuel J},
  year = {2017},
  month = oct,
  volume = {20},
  pages = {1643--1653},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.4650},
  file = {/Users/x0r/Zotero/storage/CG3MG4GL/Stachenfeld et al_2017_The hippocampus as a predictive map.pdf},
  journal = {Nature Neuroscience},
  keywords = {neuroscience,To read},
  language = {en},
  number = {11}
}

@article{stadie2018,
  title = {Some {{Considerations}} on {{Learning}} to {{Explore}} via {{Meta}}-{{Reinforcement Learning}}},
  author = {Stadie, Bradly C. and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
  year = {2018},
  month = mar,
  abstract = {We consider the problem of exploration in meta reinforcement learning. Two new meta reinforcement learning algorithms are suggested: E-MAML and E-\$\textbackslash text\{RL\}\^2\$. Results are presented on a novel environment we call `Krazy World' and a set of maze environments. We show E-MAML and E-\$\textbackslash text\{RL\}\^2\$ deliver better performance on tasks where exploration is important.},
  archivePrefix = {arXiv},
  eprint = {1803.01118},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/V6ZTKIT9/Stadie et al_2018_Some Considerations on Learning to Explore via Meta-Reinforcement Learning.pdf;/Users/x0r/Zotero/storage/6JMKX3GW/1803.html},
  journal = {arXiv:1803.01118 [cs]},
  keywords = {meta-rl},
  primaryClass = {cs}
}

@article{stalter2020,
  title = {Dopamine {{Gates Visual Signals}} in {{Monkey Prefrontal Cortex Neurons}}},
  author = {Stalter, Maximilian and Westendorff, Stephanie and Nieder, Andreas},
  year = {2020},
  month = jan,
  volume = {30},
  pages = {164-172.e4},
  issn = {22111247},
  doi = {10.1016/j.celrep.2019.11.082},
  abstract = {The neurotransmitter dopamine, which acts via the D1-like receptor (D1R) and D2-like receptor (D2R) family, may play an important role in gating sensory information to the prefrontal cortex (PFC). We tested this hypothesis in awake macaques and recorded visual motion-direction tuning functions of single PFC neurons. Using micro-iontophoretic drug application combined with single-unit recordings, we simulated receptor-specific dopaminergic input to the PFC and explored cellular gating mechanisms. We find that stimulating D1Rs, and particularly D2Rs, enhances the single-neuron and population coding quality in PFC neurons. D2R stimulation causes a clear increase of the neurons' responses to the preferred motion direction and a decrease to the non-preferred motion direction, thus enhancing neuronal signal-to-noise ratio. Neither D1R nor D2R stimulation had any impact on the neurons' tuning sharpness. These results elucidate the mechanisms of how receptor-specific dopamine effects can act as a gating signal that enables privileged access of sensory information to PFC circuits.},
  file = {/Users/x0r/Zotero/storage/KYGTKD9L/Stalter et al. - 2020 - Dopamine Gates Visual Signals in Monkey Prefrontal.pdf},
  journal = {Cell Reports},
  keywords = {To read},
  language = {en},
  number = {1}
}

@article{stanley2002,
  title = {Evolving {{Neural Networks}} through {{Augmenting Topologies}}},
  author = {Stanley, Kenneth O. and Miikkulainen, Risto},
  year = {2002},
  month = jun,
  volume = {10},
  pages = {99--127},
  issn = {1063-6560, 1530-9304},
  doi = {10.1162/106365602320169811},
  abstract = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is significantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
  file = {/Users/x0r/Zotero/storage/Y3FFKXGM/Stanley and Miikkulainen - 2002 - Evolving Neural Networks through Augmenting Topolo.pdf},
  journal = {Evolutionary Computation},
  language = {en},
  number = {2}
}

@article{stanley2007,
  title = {Compositional Pattern Producing Networks: {{A}} Novel Abstraction of Development},
  shorttitle = {Compositional Pattern Producing Networks},
  author = {Stanley, Kenneth O.},
  year = {2007},
  month = jun,
  volume = {8},
  pages = {131--162},
  issn = {1389-2576, 1573-7632},
  doi = {10.1007/s10710-007-9028-8},
  abstract = {Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two-dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
  file = {/Users/x0r/Zotero/storage/VI97HRX7/Stanley - 2007 - Compositional pattern producing networks A novel .pdf},
  journal = {Genetic Programming and Evolvable Machines},
  language = {en},
  number = {2}
}

@article{Stathopoulos_etal17,
  ids = {stathopoulos2017},
  title = {Multibit Memory Operation of Metal-Oxide Bi-Layer Memristors},
  author = {Stathopoulos, Spyros and Khiat, Ali and Trapatseli, Maria and Cortese, Simone and Serb, Alexantrou and Valov, Ilia and Prodromakis, Themis},
  year = {2017},
  volume = {7},
  pages = {17532},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/F33UVWMX/Stathopoulos et al_2017_Multibit memory operation of metal-oxide bi-layer memristors.pdf},
  journal = {Scientific reports},
  keywords = {MLC,neuromorphic},
  number = {1}
}

@article{steinberg2013,
  title = {A Causal Link between Prediction Errors, Dopamine Neurons and Learning},
  author = {Steinberg, Elizabeth E and Keiflin, Ronald and Boivin, Josiah R and Witten, Ilana B and Deisseroth, Karl and Janak, Patricia H},
  year = {2013},
  month = jul,
  volume = {16},
  pages = {966--973},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/nn.3413},
  file = {/Users/x0r/Zotero/storage/SL5V4R94/Steinberg et al. - 2013 - A causal link between prediction errors, dopamine .pdf},
  journal = {Nature Neuroscience},
  keywords = {To read},
  language = {en},
  number = {7}
}

@misc{sterkenburg2018,
  title = {Universal {{Prediction}}},
  author = {Sterkenburg, Tom F.},
  year = {2018},
  month = jan,
  abstract = {In this dissertation I investigate the theoretical possibility of a universal method of prediction. A prediction method is universal if it is always able to learn what there is to learn from data: if it is always able to extrapolate given data about past observations to maximally successful predictions about future observations. The context of this investigation is the broader philosophical question into the possibility of a formal specification of inductive or scientific reasoning, a question that also touches on modern-day speculation about a fully automatized data-driven science.

I investigate, in particular, a specific mathematical definition of a universal prediction method, that goes back to the early days of artificial intelligence and that has a direct line to modern developments in machine learning. This definition essentially aims to combine all possible prediction algorithms. An alternative interpretation is that this definition formalizes the idea that learning from data is equivalent to compressing data. In this guise, the definition is often presented as an implementation and even as a justification of Occam's razor, the principle that we should look for simple explanations.

The conclusions of my investigation are negative. I show that the proposed definition cannot be interpreted as a universal prediction method, as turns out to be exposed by a mathematical argument that it was actually intended to overcome. Moreover, I show that the suggested justification of Occam's razor does not work, and I argue that the relevant notion of simplicity as compressibility is problematic itself.},
  copyright = {cc\_by\_nc\_4},
  file = {/Users/x0r/Zotero/storage/VI7DD36L/Sterkenburg_2018_Universal Prediction.pdf},
  howpublished = {https://ir.cwi.nl/pub/27326},
  keywords = {AGI},
  language = {en},
  type = {Published {{Article}} or {{Volume}}}
}

@article{subramoney2019,
  title = {Reservoirs Learn to Learn},
  author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
  year = {2019},
  month = sep,
  abstract = {We consider reservoirs in the form of liquid state machines, i.e., recurrently connected networks of spiking neurons with randomly chosen weights. So far only the weights of a linear readout were adapted for a specific task. We wondered whether the performance of liquid state machines can be improved if the recurrent weights are chosen with a purpose, rather than randomly. After all, weights of recurrent connections in the brain are also not assumed to be randomly chosen. Rather, these weights were probably optimized during evolution, development, and prior learning experiences for specific task domains. In order to examine the benefits of choosing recurrent weights within a liquid with a purpose, we applied the Learning-to-Learn (L2L) paradigm to our model: We optimized the weights of the recurrent connections \textendash{} and hence the dynamics of the liquid state machine \textendash{} for a large family of potential learning tasks, which the network might have to learn later through modification of the weights of readout neurons. We found that this two-tiered process substantially improves the learning speed of liquid state machines for specific tasks. In fact, this learning speed increases further if one does not train the weights of linear readouts at all, and relies instead on the internal dynamics and fading memory of the network for remembering salient information that it could extract from preceding examples for the current learning task. This second type of learning has recently been proposed to underlie fast learning in the prefrontal cortex and motor cortex, and hence it is of interest to explore its performance also in models. Since liquid state machines share many properties with other types of reservoirs, our results raise the question whether L2L conveys similar benefits also to these other reservoirs.},
  archivePrefix = {arXiv},
  eprint = {1909.07486},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/9SLUTIHK/Subramoney et al. - 2019 - Reservoirs learn to learn.pdf},
  journal = {arXiv:1909.07486 [cs]},
  keywords = {neuromorphic,reservoir-computing},
  language = {en},
  primaryClass = {cs}
}

@article{subramoney2019a,
  title = {Reservoirs Learn to Learn},
  author = {Subramoney, Anand and Scherr, Franz and Maass, Wolfgang},
  year = {2019},
  month = sep,
  abstract = {We consider reservoirs in the form of liquid state machines, i.e., recurrently connected networks of spiking neurons with randomly chosen weights. So far only the weights of a linear readout were adapted for a specific task. We wondered whether the performance of liquid state machines can be improved if the recurrent weights are chosen with a purpose, rather than randomly. After all, weights of recurrent connections in the brain are also not assumed to be randomly chosen. Rather, these weights were probably optimized during evolution, development, and prior learning experiences for specific task domains. In order to examine the benefits of choosing recurrent weights within a liquid with a purpose, we applied the Learning-to-Learn (L2L) paradigm to our model: We optimized the weights of the recurrent connections \textendash{} and hence the dynamics of the liquid state machine \textendash{} for a large family of potential learning tasks, which the network might have to learn later through modification of the weights of readout neurons. We found that this two-tiered process substantially improves the learning speed of liquid state machines for specific tasks. In fact, this learning speed increases further if one does not train the weights of linear readouts at all, and relies instead on the internal dynamics and fading memory of the network for remembering salient information that it could extract from preceding examples for the current learning task. This second type of learning has recently been proposed to underlie fast learning in the prefrontal cortex and motor cortex, and hence it is of interest to explore its performance also in models. Since liquid state machines share many properties with other types of reservoirs, our results raise the question whether L2L conveys similar benefits also to these other reservoirs.},
  archivePrefix = {arXiv},
  eprint = {1909.07486},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/TZCZJTKZ/Subramoney et al. - 2019 - Reservoirs learn to learn.pdf},
  journal = {arXiv:1909.07486 [cs]},
  keywords = {To read},
  language = {en},
  primaryClass = {cs}
}

@article{sun2015,
  title = {Thermal Crosstalk in 3-Dimensional {{RRAM}} Crossbar Array},
  author = {Sun, Pengxiao and Lu, Nianduan and Li, Ling and Li, Yingtao and Wang, Hong and Lv, Hangbing and Liu, Qi and Long, Shibing and Liu, Su and Liu, Ming},
  year = {2015},
  month = oct,
  volume = {5},
  issn = {2045-2322},
  doi = {10.1038/srep13504},
  file = {/Users/x0r/Zotero/storage/NBRGUHC2/Sun et al_2015_Thermal crosstalk in 3-dimensional RRAM crossbar array.pdf},
  journal = {Scientific Reports},
  keywords = {ReRAM},
  language = {en},
  number = {1}
}

@article{sunehag2015,
  title = {Deep {{Reinforcement Learning}} with {{Attention}} for {{Slate Markov Decision Processes}} with {{High}}-{{Dimensional States}} and {{Actions}}},
  author = {Sunehag, Peter and Evans, Richard and {Dulac-Arnold}, Gabriel and Zwols, Yori and Visentin, Daniel and Coppin, Ben},
  year = {2015},
  month = dec,
  abstract = {Many real-world problems come with action spaces represented as feature vectors. Although high-dimensional control is a largely unsolved problem, there has recently been progress for modest dimensionalities. Here we report on a successful attempt at addressing problems of dimensionality as high as 2000, of a particular form. Motivated by important applications such as recommendation systems that do not fit the standard reinforcement learning frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A Slate-MDP is an MDP with a combinatorial action space consisting of slates (tuples) of primitive actions of which one is executed in an underlying MDP. The agent does not control the choice of this executed action and the action might not even be from the slate, e.g., for recommendation systems for which all recommendations can be ignored. We use deep Q-learning based on feature representations of both the state and action to learn the value of whole slates. Unlike existing methods, we optimize for both the combinatorial and sequential aspects of our tasks. The new agent's superiority over agents that either ignore the combinatorial or sequential long-term value aspect is demonstrated on a range of environments with dynamics from a real-world recommendation system. Further, we use deep deterministic policy gradients to learn a policy that for each position of the slate, guides attention towards the part of the action space in which the value is the highest and we only evaluate actions in this area. The attention is used within a sequentially greedy procedure leveraging submodularity. Finally, we show how introducing risk-seeking can dramatically imporve the agents performance and ability to discover more far reaching strategies.},
  archivePrefix = {arXiv},
  eprint = {1512.01124},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/5BXUSN4S/Sunehag et al_2015_Deep Reinforcement Learning with Attention for Slate Markov Decision Processes.pdf},
  journal = {arXiv:1512.01124 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{sunghokim2014,
  title = {Crossbar {{RRAM Arrays}}: {{Selector Device Requirements During Write Operation}}},
  shorttitle = {Crossbar {{RRAM Arrays}}},
  author = {{Sungho Kim} and {Jiantao Zhou} and Lu, Wei D.},
  year = {2014},
  month = aug,
  volume = {61},
  pages = {2820--2826},
  issn = {0018-9383, 1557-9646},
  doi = {10.1109/TED.2014.2327514},
  abstract = {A comprehensive analysis of write operations (SET and RESET) in a resistance-change memory (resistive random access memory) crossbar array is carried out. Three types of resistive switching memory cells-nonlinear, rectifyingSET, and rectifying-RESET-are compared with each other in terms of voltage delivery, current delivery, and power consumption. Two different write schemes, V/2 and V/3, were considered, and the V/2 write scheme is preferred due to much lower power consumption. A simple numerical method was developed that simulates entire current flows and node voltages within a crossbar array and provides a quantitative tool for the accurate analysis of crossbar arrays and guidelines for developing reliable write operation.},
  file = {/Users/x0r/Zotero/storage/HEU4P4G3/Sungho Kim et al_2014_Crossbar RRAM Arrays.pdf},
  journal = {IEEE Transactions on Electron Devices},
  keywords = {neuromorphic,ReRAM},
  language = {en},
  number = {8}
}

@article{surace2018,
  title = {The Limitations of Gradient Descent as a Principle of Brain Function},
  author = {Surace, Simone Carlo and Brea, Johanni},
  year = {2018},
  month = may,
  abstract = {The idea that the brain functions in a way that minimizes certain costs pervades theoretical neuroscience. Since a cost function by itself does not predict how the brain finds its minima, additional assumptions about the optimization method are usually made in order to obtain predictions about physiological quantities. In this context, steepest ascent or descent is often suggested as a biologically implemented optimization algorithm. However, the notion of a steepest direction depends on the choice of a Riemannian metric. Since this choice involves infinite degrees of freedom, the predictive power of models that are based on this principle can be called into question. Despite its great success in machine learning, especially in training deep neural networks, arguments about brain function that rely on gradient-based optimization have to be closely examined.},
  archivePrefix = {arXiv},
  eprint = {1805.11851},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/USKADSTH/Surace_Brea_2018_The limitations of gradient descent as a principle of brain function.pdf},
  journal = {arXiv:1805.11851 [q-bio]},
  keywords = {backprop,neuroscience,SNN},
  language = {en},
  primaryClass = {q-bio}
}

@article{surace2020,
  title = {On the Choice of Metric in Gradient-Based Theories of Brain Function},
  author = {Surace, Simone Carlo and Pfister, Jean-Pascal and Gerstner, Wulfram and Brea, Johanni},
  editor = {Ouellette, Francis},
  year = {2020},
  month = apr,
  volume = {16},
  pages = {e1007640},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1007640},
  abstract = {This is a PLOS Computational Biology Education paper. The idea that the brain functions so as to minimize certain costs pervades theoretical neuroscience. Because a cost function by itself does not predict how the brain finds its minima, additional assumptions about the optimization method need to be made to predict the dynamics of physiological quantities. In this context, steepest descent (also called gradient descent) is often suggested as an algorithmic principle of optimization potentially implemented by the brain. In practice, researchers often consider the vector of partial derivatives as the gradient. However, the definition of the gradient and the notion of a steepest direction depend on the choice of a metric. Because the choice of the metric involves a large number of degrees of freedom, the predictive power of models that are based on gradient descent must be called into question, unless there are strong constraints on the choice of the metric. Here, we provide a didactic review of the mathematics of gradient descent, illustrate common pitfalls of using gradient descent as a principle of brain function with examples from the literature, and propose ways forward to constrain the metric.},
  file = {/Users/x0r/Zotero/storage/MT2LJ9M7/Surace et al. - 2020 - On the choice of metric in gradient-based theories.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {4}
}

@article{sussillo2016,
  title = {Making Brain\textendash Machine Interfaces Robust to Future Neural Variability},
  author = {Sussillo, David and Stavisky, Sergey D. and Kao, Jonathan C. and Ryu, Stephen I. and Shenoy, Krishna V.},
  year = {2016},
  month = dec,
  volume = {7},
  pages = {13749},
  issn = {2041-1723},
  doi = {10.1038/ncomms13749},
  file = {/Users/x0r/Zotero/storage/XR7XJFAC/Sussillo et al_2016_Making brain–machine interfaces robust to future neural variability.pdf},
  journal = {Nature Communications},
  keywords = {BMI},
  language = {en}
}

@phdthesis{sutskever2013,
  title = {Training Recurrent Neural Networks},
  author = {Sutskever, Ilya},
  year = {2013},
  file = {/Users/x0r/Zotero/storage/MHK4XFYS/Sutskever_2013_Training recurrent neural networks.pdf},
  school = {University of Toronto}
}

@article{sutton,
  title = {Policy {{Gradient Methods}} for {{Reinforcement Learning}} with {{Function Approximation}}},
  author = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  pages = {7},
  abstract = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a value function and determining a policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is explicitly represented by its own function approximator, independent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a version of policy iteration with arbitrary differentiable function approximation is convergent to a locally optimal policy.},
  file = {/Users/x0r/Zotero/storage/RH53G2NU/Sutton et al_Policy Gradient Methods for Reinforcement Learning with Function Approximation.pdf},
  language = {en}
}

@incollection{sutton1990,
  title = {Integrated {{Architectures}} for {{Learning}}, {{Planning}}, and {{Reacting Based}} on {{Approximating Dynamic Programming}}},
  booktitle = {Machine {{Learning Proceedings}} 1990},
  author = {Sutton, Richard S.},
  year = {1990},
  pages = {216--224},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-141-3.50030-4},
  file = {/Users/x0r/Zotero/storage/8V29MM35/Sutton_1990_Integrated Architectures for Learning, Planning, and Reacting Based on.pdf;/Users/x0r/Zotero/storage/D3K3AA3P/Sutton_1990_Integrated Architectures for Learning, Planning, and Reacting Based on.pdf},
  isbn = {978-1-55860-141-3},
  language = {en}
}

@book{sutton1998,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {1998},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  file = {/Users/x0r/Zotero/storage/3LPENC53/Sutton_Barto_1998_Reinforcement learning.pdf},
  isbn = {978-0-262-19398-6},
  language = {en},
  lccn = {Q325.6 .S88 1998},
  series = {Adaptive Computation and Machine Learning}
}

@book{sze2007,
  title = {Physics of Semiconductor Devices},
  author = {Sze, S. M. and Ng, Kwok Kwok},
  year = {2007},
  edition = {3rd ed},
  publisher = {{Wiley-Interscience}},
  address = {{Hoboken, N.J}},
  file = {/Users/x0r/Zotero/storage/9JR9D9BC/Sze_Ng_2007_Physics of semiconductor devices.pdf},
  isbn = {978-0-471-14323-9},
  language = {en},
  lccn = {TK7871.85 .S988 2007}
}

@article{sze2017,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}: {{A Tutorial}} and {{Survey}}},
  shorttitle = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  year = {2017},
  month = dec,
  volume = {105},
  pages = {2295--2329},
  issn = {1558-2256},
  doi = {10.1109/JPROC.2017.2761740},
  abstract = {Deep neural networks (DNNs) are currently widely used for many artificial intelligence (AI) applications including computer vision, speech recognition, and robotics. While DNNs deliver state-of-the-art accuracy on many AI tasks, it comes at the cost of high computational complexity. Accordingly, techniques that enable efficient processing of DNNs to improve energy efficiency and throughput without sacrificing application accuracy or increasing hardware cost are critical to the wide deployment of DNNs in AI systems. This article aims to provide a comprehensive tutorial and survey about the recent advances toward the goal of enabling efficient processing of DNNs. Specifically, it will provide an overview of DNNs, discuss various hardware platforms and architectures that support DNNs, and highlight key trends in reducing the computation cost of DNNs either solely via hardware design changes or via joint hardware design and DNN algorithm changes. It will also summarize various development resources that enable researchers and practitioners to quickly get started in this field, and highlight important benchmarking metrics and design considerations that should be used for evaluating the rapidly growing number of DNN hardware designs, optionally including algorithmic codesigns, being proposed in academia and industry. The reader will take away the following concepts from this article: understand the key design considerations for DNNs; be able to evaluate different DNN hardware implementations with benchmarks and comparison metrics; understand the tradeoffs between various hardware architectures and platforms; be able to evaluate the utility of various DNN design techniques for efficient processing; and understand recent implementation trends and opportunities.},
  file = {/Users/x0r/switchdrive/zotero/Sze et al_2017_Efficient Processing of Deep Neural Networks.pdf;/Users/x0r/Zotero/storage/E9ZJSY49/8114708.html},
  journal = {Proceedings of the IEEE},
  number = {12}
}

@article{sze2020,
  title = {Efficient {{Processing}} of {{Deep Neural Networks}}},
  author = {Sze, Vivienne and Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel S.},
  year = {2020},
  month = jun,
  volume = {15},
  pages = {1--341},
  issn = {1935-3235, 1935-3243},
  doi = {10.2200/S01004ED1V01Y202004CAC050},
  file = {/Users/x0r/switchdrive/zotero/Sze et al_2020_Efficient Processing of Deep Neural Networks.pdf},
  journal = {Synthesis Lectures on Computer Architecture},
  language = {en},
  number = {2}
}

@article{tanaka2019,
  title = {Recent Advances in Physical Reservoir Computing: {{A}} Review},
  shorttitle = {Recent Advances in Physical Reservoir Computing},
  author = {Tanaka, Gouhei and Yamane, Toshiyuki and H{\'e}roux, Jean Benoit and Nakane, Ryosho and Kanazawa, Naoki and Takeda, Seiji and Numata, Hidetoshi and Nakano, Daiju and Hirose, Akira},
  year = {2019},
  month = jul,
  volume = {115},
  pages = {100--123},
  issn = {08936080},
  doi = {10.1016/j.neunet.2019.03.005},
  abstract = {Reservoir computing is a computational framework suited for temporal/sequential data processing. It is derived from several recurrent neural network models, including echo state networks and liquid state machines. A reservoir computing system consists of a reservoir for mapping inputs into a high-dimensional space and a readout for pattern analysis from the high-dimensional states in the reservoir. The reservoir is fixed and only the readout is trained with a simple method such as linear regression and classification. Thus, the major advantage of reservoir computing compared to other recurrent neural networks is fast learning, resulting in low training cost. Another advantage is that the reservoir without adaptive updating is amenable to hardware implementation using a variety of physical systems, substrates, and devices. In fact, such physical reservoir computing has attracted increasing attention in diverse fields of research. The purpose of this review is to provide an overview of recent advances in physical reservoir computing by classifying them according to the type of the reservoir. We discuss the current issues and perspectives related to physical reservoir computing, in order to further expand its practical applications and develop next-generation machine learning systems.},
  file = {/Users/x0r/Zotero/storage/6DGVQZP4/Tanaka et al. - 2019 - Recent advances in physical reservoir computing A.pdf},
  journal = {Neural Networks},
  keywords = {reservoir-computing},
  language = {en}
}

@article{teh2017,
  title = {Distral: {{Robust Multitask Reinforcement Learning}}},
  shorttitle = {Distral},
  author = {Teh, Yee Whye and Bapst, Victor and Czarnecki, Wojciech Marian and Quan, John and Kirkpatrick, James and Hadsell, Raia and Heess, Nicolas and Pascanu, Razvan},
  year = {2017},
  month = jul,
  abstract = {Most deep reinforcement learning algorithms are data inefficient in complex and rich environments, limiting their applicability to many scenarios. One direction for improving data efficiency is multitask learning with shared neural network parameters, where efficiency may be improved through transfer across related tasks. In practice, however, this is not usually observed, because gradients from different tasks can interfere negatively, making learning unstable and sometimes even less data efficient. Another issue is the different reward schemes between tasks, which can easily lead to one task dominating the learning of a shared model. We propose a new approach for joint training of multiple tasks, which we refer to as Distral (Distill \& transfer learning). Instead of sharing parameters between the different workers, we propose to share a "distilled" policy that captures common behaviour across tasks. Each worker is trained to solve its own task while constrained to stay close to the shared policy, while the shared policy is trained by distillation to be the centroid of all task policies. Both aspects of the learning process are derived by optimizing a joint objective function. We show that our approach supports efficient transfer on complex 3D environments, outperforming several related methods. Moreover, the proposed learning process is more robust and more stable---attributes that are critical in deep reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1707.04175},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/AX8DFKRS/Teh et al_2017_Distral.pdf;/Users/x0r/Zotero/storage/65JMDFWB/1707.html},
  journal = {arXiv:1707.04175 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{teufel2020,
  title = {Forms of Prediction in the Nervous System},
  author = {Teufel, Christoph and Fletcher, Paul C.},
  year = {2020},
  month = apr,
  volume = {21},
  pages = {231--242},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/s41583-020-0275-5},
  abstract = {The idea that predictions shape how we perceive and comprehend the world has become increasingly influential in the field of systems neuroscience. It also forms an important framework for understanding neuropsychiatric disorders, which are proposed to be the result of disturbances in the mechanisms through which prior information influences perception and belief, leading to the production of suboptimal models of the world. There is a widespread tendency to conceptualize the influence of predictions exclusively in terms of `top-down' processes, whereby predictions generated in higher-level areas exert their influence on lower-level areas within an information processing hierarchy. However, this excludes from consideration the predictive information embedded in the `bottom-up' stream of information processing. We describe evidence for the importance of this distinction and argue that it is critical for the development of the predictive processing framework and, ultimately, for an understanding of the perturbations that drive the emergence of neuropsychiatric symptoms and experiences.},
  file = {/Users/x0r/Zotero/storage/MA5I8X7C/Teufel and Fletcher - 2020 - Forms of prediction in the nervous system.pdf},
  journal = {Nature Reviews Neuroscience},
  keywords = {To read},
  language = {en},
  number = {4}
}

@article{Thakur_etal18,
  ids = {thakur2018},
  title = {Large-Scale Neuromorphic Spiking Array Processors: {{A}} Quest to Mimic the Brain},
  author = {Thakur, Chetan Singh and Molin, Jamal Lottier and Cauwenberghs, Gert and Indiveri, Giacomo and Kumar, Kundan and Qiao, Ning and Schemmel, Johannes and Wang, Runchun and Chicca, Elisabetta and Olson Hasler, Jennifer and Seo, Jae-sun and Yu, Shimeng and Cao, Yu and {van Schaik}, Andr{\'e} and {Etienne-Cummings}, Ralph},
  year = {2018},
  volume = {12},
  pages = {891},
  doi = {10.3389/fnins.2018.00891},
  file = {/Users/x0r/Zotero/storage/MRAI7RC2/Thakur et al_2018_Large-Scale Neuromorphic Spiking Array Processors.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic}
}

@article{thakur2019,
  title = {Corrigendum: {{Large}}-{{Scale Neuromorphic Spiking Array Processors}}: {{A Quest}} to {{Mimic}} the {{Brain}}},
  shorttitle = {Corrigendum},
  author = {Thakur, Chetan Singh and Molin, Jamal Lottier and Cauwenberghs, Gert and Indiveri, Giacomo and Kumar, Kundan and Qiao, Ning and Schemmel, Johannes and Wang, Runchun and Chicca, Elisabetta and Hasler, Jennifer Olson and Seo, Jae-sun and Yu, Shimeng and Cao, Yu and {van Schaik}, Andr{\'e} and {Etienne-Cummings}, Ralph},
  year = {2019},
  volume = {12},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00991},
  abstract = {Corrigendum: Large-Scale Neuromorphic Spiking Array Processors: A Quest to Mimic the Brain},
  file = {/Users/x0r/Zotero/storage/6QMNWNBK/Thakur et al_2019_Corrigendum.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic},
  language = {English}
}

@article{thalmeier2016,
  title = {Learning Universal Computations with Spikes},
  author = {Thalmeier, Dominik and Uhlmann, Marvin and Kappen, Hilbert J. and Memmesheimer, Raoul-Martin},
  year = {2016},
  month = jun,
  volume = {12},
  pages = {e1004895},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004895},
  abstract = {Providing the neurobiological basis of information processing in higher animals, spiking neural networks must be able to learn a variety of complicated computations, including the generation of appropriate, possibly delayed reactions to inputs and the self-sustained generation of complex activity patterns, e.g. for locomotion. Many such computations require previous building of intrinsic world models. Here we show how spiking neural networks may solve these different tasks. Firstly, we derive constraints under which classes of spiking neural networks lend themselves to substrates of powerful general purpose computing. The networks contain dendritic or synaptic nonlinearities and have a constrained connectivity. We then combine such networks with learning rules for outputs or recurrent connections. We show that this allows to learn even difficult benchmark tasks such as the self-sustained generation of desired low-dimensional chaotic dynamics or memory-dependent computations. Furthermore, we show how spiking networks can build models of external world systems and use the acquired knowledge to control them.},
  archivePrefix = {arXiv},
  eprint = {1505.07866},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/WKJCMKFP/Thalmeier et al_2016_Learning universal computations with spikes.pdf},
  journal = {PLOS Computational Biology},
  keywords = {SNN,To read},
  language = {en},
  number = {6}
}

@article{thiele2019,
  title = {A {{Spiking Network}} for {{Inference}} of {{Relations Trained}} with {{Neuromorphic Backpropagation}}},
  author = {Thiele, Johannes C. and Bichler, Olivier and Dupret, Antoine and Solinas, Sergio and Indiveri, Giacomo},
  year = {2019},
  month = mar,
  abstract = {The increasing need for intelligent sensors in a wide range of everyday objects requires the existence of low power information processing systems which can operate autonomously in their environment. In particular, merging and processing the outputs of different sensors efficiently is a necessary requirement for mobile agents with cognitive abilities. In this work, we present a multi-layer spiking neural network for inference of relations between stimuli patterns in dedicated neuromorphic systems. The system is trained with a new version of the backpropagation algorithm adapted to on-chip learning in neuromorphic hardware: Error gradients are encoded as spike signals which are propagated through symmetric synapses, using the same integrate-andfire hardware infrastructure as used during forward propagation. We demonstrate the strength of the approach on an arithmetic relation inference task and on visual XOR on the MNIST dataset. Compared to previous, biologically-inspired implementations of networks for learning and inference of relations, our approach is able to achieve better performance with less neurons. Our architecture is the first spiking neural network architecture with on-chip learning capabilities, which is able to perform relational inference on complex visual stimuli. These features make our system interesting for sensor fusion applications and embedded learning in autonomous neuromorphic agents.},
  archivePrefix = {arXiv},
  eprint = {1903.04341},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/4ERYUXEH/Thiele et al_2019_A Spiking Network for Inference of Relations Trained with Neuromorphic.pdf},
  journal = {arXiv:1903.04341 [cs]},
  keywords = {neuromorphic,SNN},
  language = {en},
  primaryClass = {cs}
}

@article{thorpe2001,
  title = {Spike-Based Strategies for Rapid Processing},
  author = {Thorpe, Simon and Delorme, Arnaud and Van Rullen, Rufin},
  year = {2001},
  month = jul,
  volume = {14},
  pages = {715--725},
  issn = {08936080},
  doi = {10.1016/S0893-6080(01)00083-1},
  abstract = {Most experimental and theoretical studies of brain function assume that neurons transmit information as a rate code, but recent studies on the speed of visual processing impose temporal constraints that appear incompatible with such a coding scheme. Other coding schemes that use the pattern of spikes across a population a neurons may be much more ef\textregistered cient. For example, since strongly activated neurons tend to \textregistered re \textregistered rst, one can use the order of \textregistered ring as a code. We argue that Rank Order Coding is not only very ef\textregistered cient, but also easy to implement in biological hardware: neurons can be made sensitive to the order of activation of their inputs by including a feed-forward shunting inhibition mechanism that progressively desensitizes the neuronal population during a wave of afferent activity. In such a case, maximum activation will only be produced when the afferent inputs are activated in the order of their synaptic weights. q 2001 Published by Elsevier Science Ltd.},
  file = {/Users/x0r/Zotero/storage/6XIWHYNJ/Thorpe et al. - 2001 - Spike-based strategies for rapid processing.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {6-7}
}

@inproceedings{todorov2012,
  title = {{{MuJoCo}}: {{A}} Physics Engine for Model-Based Control},
  shorttitle = {{{MuJoCo}}},
  booktitle = {2012 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}}},
  author = {Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  year = {2012},
  month = oct,
  pages = {5026--5033},
  issn = {2153-0858},
  doi = {10.1109/IROS.2012.6386109},
  abstract = {We describe a new physics engine tailored to model-based control. Multi-joint dynamics are represented in generalized coordinates and computed via recursive algorithms. Contact responses are computed via efficient new algorithms we have developed, based on the modern velocity-stepping approach which avoids the difficulties with spring-dampers. Models are specified using either a high-level C++ API or an intuitive XML file format. A built-in compiler transforms the user model into an optimized data structure used for runtime computation. The engine can compute both forward and inverse dynamics. The latter are well-defined even in the presence of contacts and equality constraints. The model can include tendon wrapping as well as actuator activation states (e.g. pneumatic cylinders or muscles). To facilitate optimal control applications and in particular sampling and finite differencing, the dynamics can be evaluated for different states and controls in parallel. Around 400,000 dynamics evaluations per second are possible on a 12-core machine, for a 3D homanoid with 18 dofs and 6 active contacts. We have already used the engine in a number of control applications. It will soon be made publicly available.},
  file = {/Users/x0r/Zotero/storage/AND5SRW5/6386109.html}
}

@article{tomasev2019,
  title = {A Clinically Applicable Approach to Continuous Prediction of Future Acute Kidney Injury},
  author = {Toma{\v s}ev, Nenad and Glorot, Xavier and Rae, Jack W. and Zielinski, Michal and Askham, Harry and Saraiva, Andre and Mottram, Anne and Meyer, Clemens and Ravuri, Suman and Protsyuk, Ivan and Connell, Alistair and Hughes, C{\'i}an O. and Karthikesalingam, Alan and Cornebise, Julien and Montgomery, Hugh and Rees, Geraint and Laing, Chris and Baker, Clifton R. and Peterson, Kelly and Reeves, Ruth and Hassabis, Demis and King, Dominic and Suleyman, Mustafa and Back, Trevor and Nielson, Christopher and Ledsam, Joseph R. and Mohamed, Shakir},
  year = {2019},
  month = aug,
  volume = {572},
  pages = {116--119},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1390-1},
  file = {/Users/x0r/Zotero/storage/IE29ISMG/Tomašev et al_2019_A clinically applicable approach to continuous prediction of future acute.pdf},
  journal = {Nature},
  keywords = {dl},
  language = {en},
  number = {7767}
}

@article{tononi2016,
  title = {Integrated Information Theory: From Consciousness to Its Physical Substrate},
  shorttitle = {Integrated Information Theory},
  author = {Tononi, Giulio and Boly, Melanie and Massimini, Marcello and Koch, Christof},
  year = {2016},
  month = jul,
  volume = {17},
  pages = {450--461},
  issn = {1471-003X, 1471-0048},
  doi = {10.1038/nrn.2016.44},
  abstract = {In this Opinion article, we discuss how integrated information theory accounts for several aspects of the relationship between consciousness and the brain. Integrated information theory starts from the essential properties of phenomenal experience, from which it derives the requirements for the physical substrate of consciousness. It argues that the physical substrate of consciousness must be a maximum of intrinsic cause\textendash effect power and provides a means to determine, in principle, the quality and quantity of experience. The theory leads to some counterintuitive predictions and can be used to develop new tools for assessing consciousness in non-communicative patients.},
  file = {/Users/x0r/Zotero/storage/HJ9NW4M9/Tononi et al_2016_Integrated information theory.pdf},
  journal = {Nature Reviews Neuroscience},
  keywords = {IIT},
  language = {en},
  number = {7}
}

@article{trask2018,
  title = {Neural {{Arithmetic Logic Units}}},
  author = {Trask, Andrew and Hill, Felix and Reed, Scott and Rae, Jack and Dyer, Chris and Blunsom, Phil},
  year = {2018},
  month = aug,
  abstract = {Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images. In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.},
  archivePrefix = {arXiv},
  eprint = {1808.00508},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/EA2UJSWJ/Trask et al_2018_Neural Arithmetic Logic Units.pdf},
  journal = {arXiv:1808.00508 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{traversa2015,
  title = {Universal {{Memcomputing Machines}}},
  author = {Traversa, Fabio L. and Di Ventra, Massimiliano},
  year = {2015},
  month = nov,
  volume = {26},
  pages = {2702--2715},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2015.2391182},
  abstract = {We introduce the notion of universal memcomputing machines (UMMs): a class of brain-inspired general-purpose computing machines based on systems with memory, whereby processing and storing of information occur on the same physical location. We analytically prove that the memory properties of UMMs endow them with universal computing power\textemdash they are Turing-complete\textemdash, intrinsic parallelism, functional polymorphism, and information overhead, namely their collective states can support exponential data compression directly in memory. We also demonstrate that a UMM has the same computational power as a non-deterministic Turing machine, namely it can solve NP\textendash complete problems in polynomial time. However, by virtue of its information overhead, a UMM needs only an amount of memory cells (memprocessors) that grows polynomially with the problem size. As an example we provide the polynomial-time solution of the subset-sum problem and a simple hardware implementation of the same. Even though these results do not prove the statement NP=P within the Turing paradigm, the practical realization of these UMMs would represent a paradigm shift from present von Neumann architectures bringing us closer to brain-like neural computation.},
  archivePrefix = {arXiv},
  eprint = {1405.0931},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/LIQ6SNVD/Traversa_Di Ventra_2015_Universal Memcomputing Machines.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {neuromorphic,To read},
  language = {en},
  number = {11}
}

@article{tsao2018,
  title = {Integrating Time from Experience in the Lateral Entorhinal Cortex},
  author = {Tsao, Albert and Sugar, J{\o}rgen and Lu, Li and Wang, Cheng and Knierim, James J. and Moser, May-Britt and Moser, Edvard I.},
  year = {2018},
  month = sep,
  volume = {561},
  pages = {57--62},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-018-0459-6},
  file = {/Users/x0r/Zotero/storage/E78XCUM3/Tsao et al_2018_Integrating time from experience in the lateral entorhinal cortex.pdf},
  journal = {Nature},
  keywords = {neuroscience},
  language = {en},
  number = {7721}
}

@techreport{tsuda2020,
  title = {A Modeling Framework for Adaptive Lifelong Learning with Transfer and Savings through Gating in the Prefrontal Cortex},
  author = {Tsuda, Ben and Tye, Kay M. and Siegelmann, Hava T. and Sejnowski, Terrence J.},
  year = {2020},
  month = mar,
  institution = {{Neuroscience}},
  doi = {10.1101/2020.03.11.984757},
  abstract = {The prefrontal cortex encodes and stores numerous, often disparate, schemas and flexibly switches between them. Recent research on artificial neural networks trained by reinforcement learning has made it possible to model fundamental processes underlying schema encoding and storage. Yet how the brain is able to create new schemas while preserving and utilizing old schemas remains unclear. Here we propose a simple neural network framework based on a modification of the mixture of experts architecture to model the prefrontal cortex's ability to flexibly encode and use multiple disparate schemas. We show how incorporation of gating naturally leads to transfer learning and robust memory savings. We then show how phenotypic impairments observed in patients with prefrontal damage are mimicked by lesions of our network. Our architecture, which we call DynaMoE, provides a fundamental framework for how the prefrontal cortex may handle the abundance of schemas necessary to navigate the real world.},
  file = {/Users/x0r/Zotero/storage/FVLUYHAN/Tsuda et al. - 2020 - A modeling framework for adaptive lifelong learnin.pdf},
  keywords = {To read},
  language = {en},
  type = {Preprint}
}

@article{Tuma_etal16,
  ids = {tuma2016},
  title = {Stochastic Phase-Change Neurons},
  author = {Tuma, Tomas and Pantazi, Angeliki and Le Gallo, Manuel and Sebastian, Abu and Eleftheriou, Evangelos},
  year = {2016},
  volume = {11},
  pages = {693--699},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/V57ZB8IJ/Tuma et al_2016_Stochastic phase-change neurons.pdf},
  journal = {Nature nanotechnology},
  keywords = {neuromorphic,PCM},
  number = {8}
}

@article{Urbanczik_Senn14,
  ids = {urbanczik2014},
  title = {Learning by the Dendritic Prediction of Somatic Spiking},
  author = {Urbanczik, Robert and Senn, Walter},
  year = {2014},
  volume = {81},
  pages = {521--528},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/7XGPBT6I/Urbanczik_Senn_2014_Learning by the Dendritic Prediction of Somatic Spiking.pdf},
  journal = {Neuron},
  keywords = {backprop,SNN,To read},
  number = {3},
  pmid = {24507189}
}

@misc{vachoux2011,
  title = {Top-{{Down Digital Design Flow}}},
  author = {Vachoux, Alain},
  year = {2011},
  file = {/Users/x0r/Zotero/storage/9FZAXNIM/Vachoux_2011_Top-Down Digital Design Flow.pdf},
  keywords = {VLSI},
  language = {en}
}

@article{vanhasselt2015,
  title = {Deep {{Reinforcement Learning}} with {{Double Q}}-Learning},
  author = {{van Hasselt}, Hado and Guez, Arthur and Silver, David},
  year = {2015},
  month = sep,
  abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
  archivePrefix = {arXiv},
  eprint = {1509.06461},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/A67VUYAU/van Hasselt et al_2015_Deep Reinforcement Learning with Double Q-learning.pdf},
  journal = {arXiv:1509.06461 [cs]},
  keywords = {rl},
  language = {en},
  primaryClass = {cs}
}

@article{vanhasselt2016,
  title = {Learning Values across Many Orders of Magnitude},
  author = {{van Hasselt}, Hado and Guez, Arthur and Hessel, Matteo and Mnih, Volodymyr and Silver, David},
  year = {2016},
  month = feb,
  abstract = {Most learning algorithms are not invariant to the scale of the function that is being approximated. We propose to adaptively normalize the targets used in learning. This is useful in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were all clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using the adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.},
  archivePrefix = {arXiv},
  eprint = {1602.07714},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/WDBYE5GK/van Hasselt et al_2016_Learning values across many orders of magnitude.pdf},
  journal = {arXiv:1602.07714 [cs, stat]},
  keywords = {rl,To read},
  language = {en},
  primaryClass = {cs, stat}
}

@article{varshni1967,
  title = {Temperature {{Dependence}} of {{The Energy Gap}} in {{Semiconductor}}},
  author = {Varshni, Y P},
  year = {1967},
  volume = {34},
  pages = {149--154},
  doi = {10.1016/0031-8914(67)90062-6},
  file = {/Users/x0r/Zotero/storage/8KW5VEYN/Varshni_1967_Temperature Dependence of The Energy Gap in Semiconductor.pdf},
  journal = {Physica},
  keywords = {GST},
  language = {en},
  number = {1}
}

@article{vasilaki2009,
  title = {Spike-{{Based Reinforcement Learning}} in {{Continuous State}} and {{Action Space}}: {{When Policy Gradient Methods Fail}}},
  shorttitle = {Spike-{{Based Reinforcement Learning}} in {{Continuous State}} and {{Action Space}}},
  author = {Vasilaki, Eleni and Fr{\'e}maux, Nicolas and Urbanczik, Robert and Senn, Walter and Gerstner, Wulfram},
  year = {2009},
  month = dec,
  volume = {5},
  pages = {e1000586},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1000586},
  abstract = {Changes of synaptic connections between neurons are thought to be the physiological basis of learning. These changes can be gated by neuromodulators that encode the presence of reward. We study a family of reward-modulated synaptic learning rules for spiking neurons on a learning task in continuous space inspired by the Morris Water maze. The synaptic update rule modifies the release probability of synaptic transmission and depends on the timing of presynaptic spike arrival, postsynaptic action potentials, as well as the membrane potential of the postsynaptic neuron. The family of learning rules includes an optimal rule derived from policy gradient methods as well as reward modulated Hebbian learning. The synaptic update rule is implemented in a population of spiking neurons using a network architecture that combines feedforward input with lateral connections. Actions are represented by a population of hypothetical action cells with strong mexican-hat connectivity and are read out at theta frequency. We show that in this architecture, a standard policy gradient rule fails to solve the Morris watermaze task, whereas a variant with a Hebbian bias can learn the task within 20 trials, consistent with experiments. This result does not depend on implementation details such as the size of the neuronal populations. Our theoretical approach shows how learning new behaviors can be linked to reward-modulated plasticity at the level of single synapses and makes predictions about the voltage and spike-timing dependence of synaptic plasticity and the influence of neuromodulators such as dopamine. It is an important step towards connecting formal theories of reinforcement learning with neuronal and synaptic properties.},
  file = {/Users/x0r/Zotero/storage/8D3JJ5WK/Vasilaki et al. - 2009 - Spike-Based Reinforcement Learning in Continuous S.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {12}
}

@article{vaswani2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/4SWUTNLE/Vaswani et al_2017_Attention Is All You Need.pdf},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {dl},
  primaryClass = {cs}
}

@article{velea2017,
  title = {Te-Based Chalcogenide Materials for Selector Applications},
  author = {Velea, A. and Opsomer, K. and Devulder, W. and Dumortier, J. and Fan, J. and Detavernier, C. and Jurczak, M. and Govoreanu, B.},
  year = {2017},
  month = dec,
  volume = {7},
  issn = {2045-2322},
  doi = {10.1038/s41598-017-08251-z},
  file = {/Users/x0r/Zotero/storage/7YLQUEME/Velea et al_2017_Te-based chalcogenide materials for selector applications.pdf},
  journal = {Scientific Reports},
  keywords = {OTS},
  language = {en},
  number = {1}
}

@article{vinyals2016,
  title = {Matching {{Networks}} for {{One Shot Learning}}},
  author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
  year = {2016},
  month = jun,
  file = {/Users/x0r/Zotero/storage/XLCK3E2U/Vinyals et al_2016_Matching Networks for One Shot Learning.pdf},
  language = {en}
}

@article{vinyals2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = oct,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  file = {/Users/x0r/Zotero/storage/SBESBY4R/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf},
  journal = {Nature},
  keywords = {AGI,rl},
  language = {en}
}

@article{wang2010,
  title = {Hippocampal-{{Neocortical Interactions}} in {{Memory Formation}}, {{Consolidation}}, and {{Reconsolidation}}},
  author = {Wang, Szu-Han and Morris, Richard G.M.},
  year = {2010},
  month = jan,
  volume = {61},
  pages = {49--79},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.psych.093008.100523},
  abstract = {This review, focusing on work using animals, updates a theoretical approach whose aim is to translate neuropsychological ideas about the psychological and anatomical organization of memory into the neurobiological domain. It is suggested that episodic-like memory consists of both automatic and controlled components, with the medial temporal mediation of memory encoding including neurobiological mechanisms that are primarily automatic or incidental. These ideas, in the cognitive and behavioral domain, are linked to neurophysiological ideas about cellular consolidation concerning synaptic potentiation, particularly the relationship between protein synthesis-dependent long-term changes and shorter-lasting post-translational mechanisms. Ideas from psychology about mental schemas are considered in relation to the phenomenon of systems consolidation and, specifically, about how prior knowledge can alter the rate at which consolidation occurs. Finally, the hippocampal-neocortical interactions theory is updated in relation to reconsolidation, a process that enables updating of stored memory traces in response to novelty.},
  file = {/Users/x0r/Zotero/storage/43KF8KQD/Wang_Morris_2010_Hippocampal-Neocortical Interactions in Memory Formation, Consolidation, and.pdf},
  journal = {Annual Review of Psychology},
  keywords = {neuroscience,To read},
  language = {en},
  number = {1}
}

@article{wang2018,
  title = {Prefrontal Cortex as a Meta-Reinforcement Learning System},
  author = {Wang, Jane X. and {Kurth-Nelson}, Zeb and Kumaran, Dharshan and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z. and Hassabis, Demis and Botvinick, Matthew},
  year = {2018},
  month = jun,
  volume = {21},
  pages = {860--868},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0147-8},
  file = {/Users/x0r/Zotero/storage/FBQEDK5M/Wang et al_2018_Prefrontal cortex as a meta-reinforcement learning system.pdf},
  journal = {Nature Neuroscience},
  keywords = {meta-learning,To read},
  language = {en},
  number = {6}
}

@article{wang2018a,
  title = {Capacitive Neural Network with Neuro-Transistors},
  author = {Wang, Zhongrui and Rao, Mingyi and Han, Jin-Woo and Zhang, Jiaming and Lin, Peng and Li, Yunning and Li, Can and Song, Wenhao and Asapu, Shiva and Midya, Rivu and Zhuo, Ye and Jiang, Hao and Yoon, Jung Ho and Upadhyay, Navnidhi Kumar and Joshi, Saumil and Hu, Miao and Strachan, John Paul and Barnell, Mark and Wu, Qing and Wu, Huaqiang and Qiu, Qinru and Williams, R. Stanley and Xia, Qiangfei and Yang, J. Joshua},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {3208},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-05677-5},
  file = {/Users/x0r/Zotero/storage/UPT6Q8UE/Wang et al. - 2018 - Capacitive neural network with neuro-transistors.pdf},
  journal = {Nature Communications},
  keywords = {material,neuromorphic},
  language = {en},
  number = {1}
}

@article{wang2019,
  title = {{{PaperRobot}}: {{Incremental Draft Generation}} of {{Scientific Ideas}}},
  shorttitle = {{{PaperRobot}}},
  author = {Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  year = {2019},
  month = may,
  abstract = {We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30\%, 24\% and 12\% of the time, respectively.},
  archivePrefix = {arXiv},
  eprint = {1905.07870},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/PA9GATSF/Wang et al_2019_PaperRobot.pdf},
  journal = {arXiv:1905.07870 [cs]},
  primaryClass = {cs}
}

@article{wang2019a,
  title = {Knowing {{When}} to {{Stop}}: {{Evaluation}} and {{Verification}} of {{Conformity}} to {{Output}}-Size {{Specifications}}},
  shorttitle = {Knowing {{When}} to {{Stop}}},
  author = {Wang, Chenglong and Bunel, Rudy and Dvijotham, Krishnamurthy and Huang, Po-Sen and Grefenstette, Edward and Kohli, Pushmeet},
  year = {2019},
  month = apr,
  abstract = {Models such as Sequence-to-Sequence and Image-to-Sequence are widely used in real world applications. While the ability of these neural architectures to produce variable-length outputs makes them extremely effective for problems like Machine Translation and Image Captioning, it also leaves them vulnerable to failures of the form where the model produces outputs of undesirable length. This behavior can have severe consequences such as usage of increased computation and induce faults in downstream modules that expect outputs of a certain length. Motivated by the need to have a better understanding of the failures of these models, this paper proposes and studies the novel output-size modulation problem and makes two key technical contributions. First, to evaluate model robustness, we develop an easy-to-compute differentiable proxy objective that can be used with gradient-based algorithms to find output-lengthening inputs. Second and more importantly, we develop a verification approach that can formally verify whether a network always produces outputs within a certain length. Experimental results on Machine Translation and Image Captioning show that our output-lengthening approach can produce outputs that are 50 times longer than the input, while our verification approach can, given a model and input domain, prove that the output length is below a certain size.},
  archivePrefix = {arXiv},
  eprint = {1904.12004},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/RDMQE22Z/Wang et al_2019_Knowing When to Stop.pdf},
  journal = {arXiv:1904.12004 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wang2019b,
  title = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}: {{Where We}}'ve {{Been}}, {{Where We}}'re {{Going}}},
  shorttitle = {Deep {{Neural Network Approximation}} for {{Custom Hardware}}},
  author = {Wang, Erwei and Davis, James J. and Zhao, Ruizhe and Ng, Ho-Cheung and Niu, Xinyu and Luk, Wayne and Cheung, Peter Y. K. and Constantinides, George A.},
  year = {2019},
  month = jan,
  abstract = {Deep neural networks have proven to be particularly effective in visual and audio recognition tasks. Existing models tend to be computationally expensive and memory intensive, however, and so methods for hardware-oriented approximation have become a hot topic. Research has shown that custom hardware-based neural network accelerators can surpass their general-purpose processor equivalents in terms of both throughput and energy efficiency. Application-tailored accelerators, when co-designed with approximation-based network training methods, transform large, dense and computationally expensive networks into small, sparse and hardware-efficient alternatives, increasing the feasibility of network deployment. In this article, we provide a comprehensive evaluation of approximation methods for high-performance network inference along with in-depth discussion of their effectiveness for custom hardware implementation. We also include proposals for future research based on a thorough analysis of current trends. This article represents the first survey providing detailed comparisons of custom hardware accelerators featuring approximation for both convolutional and recurrent neural networks, through which we hope to inspire exciting new developments in the field.},
  archivePrefix = {arXiv},
  eprint = {1901.06955},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ILE49CGM/Wang et al_2019_Deep Neural Network Approximation for Custom Hardware.pdf},
  journal = {arXiv:1901.06955 [cs]},
  keywords = {neuromorphic},
  primaryClass = {cs}
}

@article{wang2020,
  title = {Enhanced {{POET}}: {{Open}}-{{Ended Reinforcement Learning}} through {{Unbounded Invention}} of {{Learning Challenges}} and Their {{Solutions}}},
  shorttitle = {Enhanced {{POET}}},
  author = {Wang, Rui and Lehman, Joel and Rawal, Aditya and Zhi, Jiale and Li, Yulun and Clune, Jeff and Stanley, Kenneth O.},
  year = {2020},
  month = apr,
  abstract = {Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges, and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of sophisticated behaviors that solve a wide range of environmental challenges, many of which cannot be solved through other means.},
  archivePrefix = {arXiv},
  eprint = {2003.08536},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/KUJDGL22/Wang et al. - 2020 - Enhanced POET Open-Ended Reinforcement Learning t.pdf},
  journal = {arXiv:2003.08536 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{Waser_Aono07,
  ids = {waser2007},
  title = {Nanoionics-Based Resistive Switching Memories},
  author = {Waser, Rainer and Aono, Masakazu},
  year = {2007},
  volume = {6},
  pages = {833--840},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/BAVLTQV4/Waser_Aono_2007_Nanoionics-based resistive switching memories.pdf;/Users/x0r/Zotero/storage/WUI9C4YM/nmat2023.html},
  journal = {Nature materials},
  number = {11}
}

@article{watter2015,
  title = {Embed to {{Control}}: {{A Locally Linear Latent Dynamics Model}} for {{Control}} from {{Raw Images}}},
  shorttitle = {Embed to {{Control}}},
  author = {Watter, Manuel and Springenberg, Jost Tobias and Boedecker, Joschka and Riedmiller, Martin},
  year = {2015},
  month = jun,
  abstract = {We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.},
  archivePrefix = {arXiv},
  eprint = {1506.07365},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/GSXJ6EIC/Watter et al_2015_Embed to Control.pdf},
  journal = {arXiv:1506.07365 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{weber2017,
  title = {Imagination-{{Augmented Agents}} for {{Deep Reinforcement Learning}}},
  author = {Weber, Th{\'e}ophane and Racani{\`e}re, S{\'e}bastien and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo Jimenez and Badia, Adria Puigdom{\`e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
  year = {2017},
  month = jul,
  file = {/Users/x0r/Zotero/storage/KAXGRDKK/Weber et al_2017_Imagination-Augmented Agents for Deep Reinforcement Learning.pdf},
  keywords = {rl,spinning-up},
  language = {en}
}

@article{webster2015,
  title = {Visual {{Adaptation}}},
  author = {Webster, Michael A.},
  year = {2015},
  month = nov,
  volume = {1},
  pages = {547--567},
  issn = {2374-4642, 2374-4650},
  doi = {10.1146/annurev-vision-082114-035509},
  file = {/Users/x0r/Zotero/storage/2U2223IK/Webster_2015_Visual Adaptation.pdf},
  journal = {Annual Review of Vision Science},
  keywords = {neuroscience,vision},
  language = {en},
  number = {1}
}

@article{wedeen2012,
  title = {The {{Geometric Structure}} of the {{Brain Fiber Pathways}}},
  author = {Wedeen, V. J. and Rosene, D. L. and Wang, R. and Dai, G. and Mortazavi, F. and Hagmann, P. and Kaas, J. H. and Tseng, W.-Y. I.},
  year = {2012},
  month = mar,
  volume = {335},
  pages = {1628--1634},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1215280},
  file = {/Users/x0r/Zotero/storage/CN8VLIAJ/Wedeen et al_2012_The Geometric Structure of the Brain Fiber Pathways.pdf},
  journal = {Science},
  keywords = {neuromorphic},
  language = {en},
  number = {6076}
}

@article{wei2016,
  title = {Network {{Morphism}}},
  author = {Wei, Tao and Wang, Changhu and Rui, Yong and Chen, Chang Wen},
  year = {2016},
  month = mar,
  abstract = {We present in this paper a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement for this network morphism is its ability to deal with nonlinearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  archivePrefix = {arXiv},
  eprint = {1603.01670},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Y74F5KTB/Wei et al_2016_Network Morphism.pdf},
  journal = {arXiv:1603.01670 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{weidenhof2001,
  title = {Laser Induced Crystallization of Amorphous {{Ge2Sb2Te5}} Films},
  author = {Weidenhof, V. and Friedrich, I. and Ziegler, S. and Wuttig, M.},
  year = {2001},
  month = mar,
  volume = {89},
  pages = {3168--3176},
  issn = {0021-8979, 1089-7550},
  doi = {10.1063/1.1351868},
  file = {/Users/x0r/Zotero/storage/NMIJL647/Weidenhof et al_2001_Laser induced crystallization of amorphous Ge2Sb2Te5 films.pdf},
  journal = {Journal of Applied Physics},
  keywords = {GST},
  language = {en},
  number = {6}
}

@article{werfel2005,
  title = {Learning {{Curves}} for {{Stochastic Gradient Descent}} in {{Linear Feedforward Networks}}},
  author = {Werfel, Justin and Xie, Xiaohui and Seung, H. Sebastian},
  year = {2005},
  month = dec,
  volume = {17},
  pages = {2699--2718},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976605774320539},
  file = {/Users/x0r/Zotero/storage/J7U6E7EC/Werfel et al. - 2005 - Learning Curves for Stochastic Gradient Descent in.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {12}
}

@article{whittington2017,
  title = {An {{Approximation}} of the {{Error Backpropagation Algorithm}} in a {{Predictive Coding Network}} with {{Local Hebbian Synaptic Plasticity}}},
  author = {Whittington, James C. R. and Bogacz, Rafal},
  year = {2017},
  month = may,
  volume = {29},
  pages = {1229--1262},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/NECO_a_00949},
  abstract = {To efficiently learn from feedback, cortical networks need to update synaptic weights on multiple levels of cortical hierarchy. An effective and well-known algorithm for computing such changes in synaptic weights is the error backpropagation algorithm. However, in this algorithm, the change in synaptic weights is a complex function of weights and activities of neurons not directly connected with the synapse being modified, whereas the changes in biological synapses are determined only by the activity of presynaptic and postsynaptic neurons. Several models have been proposed that approximate the backpropagation algorithm with local synaptic plasticity, but these models require complex external control over the network or relatively complex plasticity rules. Here we show that a network developed in the predictive coding framework can efficiently perform supervised learning fully autonomously, employing only simple local Hebbian plasticity. Furthermore, for certain parameters, the weight change in the predictive coding model converges to that of the backpropagation algorithm. This suggests that it is possible for cortical networks with simple Hebbian synaptic plasticity to implement efficient learning algorithms in which synapses in areas on multiple levels of hierarchy are modified to minimize the error on the output.},
  file = {/Users/x0r/Zotero/storage/5YAKBNWX/Whittington and Bogacz - 2017 - An Approximation of the Error Backpropagation Algo.pdf},
  journal = {Neural Computation},
  keywords = {backprop,neuroscience},
  language = {en},
  number = {5}
}

@article{williams1992,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  volume = {8},
  pages = {229--256},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992696},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  file = {/Users/x0r/Zotero/storage/HWWLLVAY/Williams_1992_Simple statistical gradient-following algorithms for connectionist.pdf;/Users/x0r/Zotero/storage/BTC8P755/BF00992696.html},
  journal = {Machine Learning},
  keywords = {backprop,rl},
  language = {en},
  number = {3-4}
}

@article{wong2010,
  title = {Phase {{Change Memory}}},
  author = {Wong, H.-S. Philip and Raoux, Simone and Kim, SangBum and Liang, Jiale and Reifenberg, John P. and Rajendran, Bipin and Asheghi, Mehdi and Goodson, Kenneth E.},
  year = {2010},
  month = dec,
  volume = {98},
  pages = {2201--2227},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2010.2070050},
  abstract = {In this paper, recent progress of phase change memory (PCM) is reviewed. The electrical and thermal properties of phase change materials are surveyed with a focus on the scalability of the materials and their impact on device design. Innovations in the device structure, memory cell selector, and strategies for achieving multibit operation and 3-D, multilayer high-density memory arrays are described. The scaling properties of PCM are illustrated with recent experimental results using special device test structures and novel material synthesis. Factors affecting the reliability of PCM are discussed.},
  file = {/Users/x0r/Zotero/storage/M6KMHCJC/Wong et al_2010_Phase Change Memory.pdf},
  journal = {Proceedings of the IEEE},
  keywords = {PCM},
  language = {en},
  number = {12}
}

@article{wong2012,
  title = {Metal\textendash{{Oxide RRAM}}},
  author = {Wong, H.-S. Philip and Lee, Heng-Yuan and Yu, Shimeng and Chen, Yu-Sheng and Wu, Yi and Chen, Pang-Shiu and Lee, Byoungil and Chen, Frederick T. and Tsai, Ming-Jinn},
  year = {2012},
  month = jun,
  volume = {100},
  pages = {1951--1970},
  issn = {0018-9219, 1558-2256},
  doi = {10.1109/JPROC.2012.2190369},
  abstract = {In this paper, recent progress of binary metal\textendash oxide resistive switching random access memory (RRAM) is reviewed. The physical mechanism, material properties, and electrical characteristics of a variety of binary metal\textendash oxide RRAM are discussed, with a focus on the use of RRAM for nonvolatile memory application. A review of recent development of large-scale RRAM arrays is given. Issues such as uniformity, endurance, retention, multibit operation, and scaling trends are discussed.},
  file = {/Users/x0r/Zotero/storage/WASI449W/Wong et al_2012_Metal–Oxide RRAM.pdf},
  journal = {Proceedings of the IEEE},
  keywords = {ReRAM},
  language = {en},
  number = {6}
}

@article{woo2013,
  title = {Thermally Activated Non-Linearity of Device in Resistance-Switching Memory for Cross-Point Array Applications},
  author = {Woo, Jiyong and Kim, Seonghyun and Lee, Wootae and Lee, Daeseok and Park, Sangsu and Choi, Godeuni and Cha, Euijun and Hwang, Hyunsang},
  year = {2013},
  month = mar,
  volume = {102},
  pages = {122115},
  issn = {0003-6951, 1077-3118},
  doi = {10.1063/1.4799148},
  file = {/Users/x0r/Zotero/storage/J8MS2AQG/Woo et al_2013_Thermally activated non-linearity of device in resistance-switching memory for.pdf},
  journal = {Applied Physics Letters},
  keywords = {neuromorphic,threshold-switching},
  language = {en},
  number = {12}
}

@article{Wozniak_etal17,
  ids = {wozniak2017a},
  title = {Neuromorphic Architecture with {{1M}} Memristive Synapses for Detection of Weakly Correlated Inputs},
  author = {Wozniak, Stanislaw and Pantazi, Angeliki and Sidler, Severin and Papandreou, Nikolaos and Leblebici, Yusuf and Eleftheriou, Evangelos},
  year = {2017},
  pages = {1342--1346},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/8DXW9NST/Wozniak et al_2017_Neuromorphic Architecture With 1M Memristive Synapses for Detection of Weakly.pdf},
  journal = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  keywords = {GST,neuromorphic}
}

@inproceedings{wozniak2016,
  title = {Learning Spatio-Temporal Patterns in the Presence of Input Noise Using Phase-Change Memristors},
  author = {Wozniak, Stanislaw and Tuma, Tomas and Pantazi, Angeliki and Eleftheriou, Evangelos},
  year = {2016},
  month = may,
  pages = {365--368},
  publisher = {{IEEE}},
  doi = {10.1109/ISCAS.2016.7527246},
  abstract = {Neuromorphic systems increasingly attract research interest owing to their ability to provide biologically inspired methods of computing, alternative to the classic von Neumann architecture. In these systems, computing relies on spike-based communication between neurons, and memory is represented by evolving states of the synaptic interconnections. In this work, we first demonstrate how spike-timing-dependent plasticity (STDP) based synapses can be realized using the crystal-growth dynamics of phase-change memristors. Then, we present a novel learning architecture comprising an integrate-and-fire neuron and an array of phase-change synapses that is capable of detecting temporal correlations in parallel input streams. We demonstrate a continuous re-learning operation on a sequence of binary 20\texttimes 20 pixel images in the presence of significant background noise. Experimental results using an array of phase-change cells as synaptic elements confirm the functionality and performance of the proposed learning architecture.},
  file = {/Users/x0r/Zotero/storage/86XZYQAS/Wozniak et al_2016_Learning spatio-temporal patterns in the presence of input noise using.pdf},
  isbn = {978-1-4799-5341-7},
  keywords = {neuromorphic,PCM},
  language = {en}
}

@inproceedings{wozniak2017,
  title = {Neuromorphic System with Phase-Change Synapses for Pattern Learning and Feature Extraction},
  author = {Wozniak, Stanislaw and Pantazi, Angeliki and Leblebici, Yusuf and Eleftheriou, Evangelos},
  year = {2017},
  month = may,
  pages = {3724--3732},
  publisher = {{IEEE}},
  doi = {10.1109/IJCNN.2017.7966325},
  abstract = {Neuromorphic systems provide biologically inspired methods of computing, alternative to the classical von Neumann approach. In these systems, computation is performed by a network of spiking neurons controlled by the values of their synaptic weights, which are updated in the process of learning. Providing efficient synaptic learning rules, such as spike-timing-dependent plasticity (STDP), is a challenging task. These rules need to primarily use local information, but simultaneously develop a knowledge representation that is useful in the global context. From the implementation viewpoint, they also need to be suited for particular hardware technology. In this work, we propose a system with spiking neurons and synapses realized using phasechange devices. We design in a bottom-up manner an architecture for pattern learning and feature extraction. Experimental results from a prototype hardware platform demonstrate the capabilities of the proposed neuromorphic system.},
  file = {/Users/x0r/Zotero/storage/DDTVC7L6/Wozniak et al_2017_Neuromorphic system with phase-change synapses for pattern learning and feature.pdf},
  isbn = {978-1-5090-6182-2},
  keywords = {GST,neuromorphic},
  language = {en}
}

@article{wu2018,
  title = {Toward an {{AI Physicist}} for {{Unsupervised Learning}}},
  author = {Wu, Tailin and Tegmark, Max},
  year = {2018},
  month = oct,
  abstract = {We investigate opportunities and challenges for improving unsupervised machine learning using four common strategies with a long history in physics: divide-and-conquer, Occam's Razor, unification, and lifelong learning. Instead of using one model to learn everything, we propose a novel paradigm centered around the learning and manipulation of *theories*, which parsimoniously predict both aspects of the future (from past observations) and the domain in which these predictions are accurate. Specifically, we propose a novel generalized-mean-loss to encourage each theory to specialize in its comparatively advantageous domain, and a differentiable description length objective to downweight bad data and "snap" learned theories into simple symbolic formulas. Theories are stored in a "theory hub", which continuously unifies learned theories and can propose theories when encountering new environments. We test our implementation, the "AI Physicist" learning agent, on a suite of increasingly complex physics environments. From unsupervised observation of trajectories through worlds involving random combinations of gravity, electromagnetism, harmonic motion and elastic bounces, our agent typically learns faster and produces mean-squared prediction errors about a billion times smaller than a standard feedforward neural net of comparable complexity, typically recovering integer and rational theory parameters exactly. Our agent successfully identifies domains with different laws of motion also for a nonlinear chaotic double pendulum in a piecewise constant force field.},
  archivePrefix = {arXiv},
  eprint = {1810.10525},
  eprinttype = {arxiv},
  journal = {arXiv:1810.10525 [cond-mat, physics:physics]},
  language = {en},
  primaryClass = {cond-mat, physics:physics}
}

@article{wunderlich2018,
  title = {Demonstrating {{Advantages}} of {{Neuromorphic Computation}}: {{A Pilot Study}}},
  shorttitle = {Demonstrating {{Advantages}} of {{Neuromorphic Computation}}},
  author = {Wunderlich, Timo and Kungl, Akos F. and M{\"u}ller, Eric and Hartel, Andreas and Stradmann, Yannik and Aamir, Syed Ahmed and Gr{\"u}bl, Andreas and Heimbrecht, Arthur and Schreiber, Korbinian and St{\"o}ckel, David and Pehle, Christian and Billaudelle, Sebastian and Kiene, Gerd and Mauch, Christian and Schemmel, Johannes and Meier, Karlheinz and Petrovici, Mihai A.},
  year = {2018},
  month = nov,
  abstract = {Neuromorphic devices represent an attempt to mimic aspects of the brain's architecture and dynamics with the aim of replicating its hallmark functional capabilities in terms of computational power, robust learning and energy efficiency. We employ a single-chip prototype of the BrainScaleS 2 neuromorphic system to implement a proof-of-concept demonstration of reward-modulated spike-timing-dependent plasticity in a spiking network that learns to play the Pong video game by smooth pursuit. This system combines an electronic mixed-signal substrate for emulating neuron and synapse dynamics with an embedded digital processor for on-chip learning, which in this work also serves to simulate the virtual environment and learning agent. The analog emulation of neuronal membrane dynamics enables a 1000-fold acceleration with respect to biological real-time, with the entire chip operating on a power budget of 57mW. Compared to an equivalent simulation using state-of-the-art software, the on-chip emulation is at least one order of magnitude faster and three orders of magnitude more energy-efficient. We demonstrate how on-chip learning can mitigate the effects of fixed-pattern noise, which is unavoidable in analog substrates, while making use of temporal variability for action exploration. Learning compensates imperfections of the physical substrate, as manifested in neuronal parameter variability, by adapting synaptic weights to match respective excitability of individual neurons.},
  archivePrefix = {arXiv},
  eprint = {1811.03618},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/V8C9EMU3/Wunderlich et al_2018_Demonstrating Advantages of Neuromorphic Computation.pdf},
  journal = {arXiv:1811.03618 [cs]},
  keywords = {neuromorphic,rl},
  primaryClass = {cs}
}

@article{Xia_Yang19,
  ids = {xia2019},
  title = {Memristive Crossbar Arrays for Brain-Inspired Computing},
  author = {Xia, Q. and Yang, J. J.},
  year = {2019},
  volume = {18},
  pages = {309},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/MWCFK5PN/Xia_Yang_2019_Memristive crossbar arrays for brain-inspired computing.pdf},
  journal = {Nature materials},
  keywords = {neuromorphic},
  number = {4}
}

@article{xiao2019,
  title = {Biologically-Plausible Learning Algorithms Can Scale to Large Datasets},
  author = {Xiao, Will and Chen, Honglin},
  year = {2019},
  pages = {9},
  abstract = {The backpropagation (BP) algorithm is often thought to be biologically implausible in the brain. One of the main reasons is that BP requires symmetric weight matrices in the feedforward and feedback pathways. To address this ``weight transport problem'' (Grossberg, 1987), two biologically-plausible algorithms, proposed by Liao et al. (2016b) and Lillicrap et al. (2016), relax BP's weight symmetry requirements and demonstrate comparable learning capabilities to that of BP on small datasets. However, a recent study by Bartunov et al. (2018) finds that although feedback alignment (FA) and some variants of target-propagation (TP) perform well on MNIST and CIFAR, they perform significantly worse than BP on ImageNet. Here, we additionally evaluate the sign-symmetry (SS) algorithm (Liao et al., 2016b), which differs from both BP and FA in that the feedback and feedforward weights do not share magnitudes but share signs. We examined the performance of sign-symmetry and feedback alignment on ImageNet and MS COCO datasets using different network architectures (ResNet-18 and AlexNet for ImageNet; RetinaNet for MS COCO). Surprisingly, networks trained with signsymmetry can attain classification performance approaching that of BP-trained networks. These results complement the study by Bartunov et al. (2018) and establish a new benchmark for future biologically-plausible learning algorithms on more difficult datasets and more complex architectures.},
  file = {/Users/x0r/Zotero/storage/LP8E7EIP/Xiao_Chen_2019_Biologically-plausible learning algorithms can scale to large datasets.pdf},
  keywords = {neuroscience,SNN,To read},
  language = {en}
}

@article{xu2014,
  title = {Parallel {{Programming}} of {{Resistive Cross}}-Point {{Array}} for {{Synaptic Plasticity}}},
  author = {Xu, Zihan and Mohanty, Abinash and Chen, Pai-Yu and Kadetotad, Deepak and Lin, Binbin and Ye, Jieping and Vrudhula, Sarma and Yu, Shimeng and Seo, Jae-sun and Cao, Yu},
  year = {2014},
  volume = {41},
  pages = {126--133},
  issn = {18770509},
  doi = {10.1016/j.procs.2014.11.094},
  abstract = {This paper proposes a parallel programming scheme for the cross-point array with resistive random access memory (RRAM). Synaptic plasticity in unsupervised learning is realized by tuning the conductance of each RRAM cell. Inspired by the spike-timing-dependent-plasticity (STDP), the programming strength is encoded into the spike firing rate (i.e., pulse frequency) and the overlap time (i.e., duty cycle) of the pre-synaptic node and post-synaptic node, and simultaneously applied to all RRAM cells in the cross-point array. Such an approach achieves parallel programming of the entire RRAM array, only requiring local information from pre-synaptic and post-synaptic nodes to each RRAM cell. As demonstrated by digital peripheral circuits implemented in 65nm CMOS, the programming time of a 40kb RRAM array is 84 ns, indicating 900X speedup as compared to state-ofthe-art software approach of sparse coding in image feature extraction.},
  file = {/Users/x0r/Zotero/storage/YKN2UGDK/Xu et al_2014_Parallel Programming of Resistive Cross-point Array for Synaptic Plasticity.pdf},
  journal = {Procedia Computer Science},
  keywords = {neuromorphic},
  language = {en}
}

@inproceedings{xu2015,
  title = {Overcoming the Challenges of Crossbar Resistive Memory Architectures},
  author = {Xu, Cong and Niu, Dimin and Muralimanohar, Naveen and Balasubramonian, Rajeev and Zhang, Tao and Yu, Shimeng and Xie, Yuan},
  year = {2015},
  month = feb,
  pages = {476--488},
  publisher = {{IEEE}},
  doi = {10.1109/HPCA.2015.7056056},
  abstract = {The scalability of DRAM faces challenges from increasing power consumption and the difficulty of building high aspect ratio capacitors. Consequently, emerging memory technologies including Phase Change Memory (PCM), Spin-Transfer Torque RAM (STT-RAM), and Resistive RAM (ReRAM) are being actively pursued as replacements for DRAM memory. Among these candidates, ReRAM has superior characteristics such as high density, low write energy, and high endurance, making it a very attractive cost-efficient alternative to DRAM. In this paper, we present a comprehensive study of ReRAMbased memory systems. ReRAM's high density comes from its unique crossbar architecture where some peripheral circuits are laid below multiple layers of ReRAM cells. A crossbar architecture introduces special constraints on operating voltages, write latency, and array size. The access latency of a crossbar is a function of the data patterns involved in a write operation. These combined with ReRAM's exponential relationship between its write voltage and switching latency provide opportunities for architectural optimizations. This paper makes several key contributions. First, we study the crossbar architecture and describe trade-offs involving voltage drop, write latency, and data pattern. We then analyze microarchitectural enhancements such as double-sided ground biasing and multiphase reset operations to improve write performance. At the architecture level, a simple compression based data encoding scheme is proposed to further bring down the latency. As the compressibility of a block varies based on its content, write latency is not uniform across blocks. To mitigate the impact of slow writes on performance, we propose and evaluate a novel scheduling policy that makes writing decisions based on latency and activity of a bank. The experimental results show that our architecture improves the performance of a system using ReRAM-based main memory by about 44\% over a conservative baseline and 14\% over an aggressive baseline on average, and has less than 10\% performance degradation compared to an ideal DRAM-only system.},
  file = {/Users/x0r/Zotero/storage/B4QG8UDW/Xu et al_2015_Overcoming the challenges of crossbar resistive memory architectures.pdf},
  isbn = {978-1-4799-8930-0},
  keywords = {neuromorphic},
  language = {en}
}

@inproceedings{xu2017,
  title = {Performance {{Evaluation}} of {{Deep Learning Tools}} in {{Docker Containers}}},
  booktitle = {2017 3rd {{International Conference}} on {{Big Data Computing}} and {{Communications}} ({{BIGCOM}})},
  author = {Xu, Pengfei and Shi, Shaohuai and Chu, Xiaowen},
  year = {2017},
  month = aug,
  pages = {395--403},
  publisher = {{IEEE}},
  address = {{Chengdu}},
  doi = {10.1109/BIGCOM.2017.32},
  abstract = {With the success of deep learning techniques in a broad range of application domains, many deep learning software frameworks have been developed and are being updated frequently to adapt to new hardware features and software libraries, which bring a big challenge for end users and system administrators. To address this problem, container techniques are widely used to simplify the deployment and management of deep learning software. However, it remains unknown whether container techniques bring any performance penalty to deep learning applications. The purpose of this work is to systematically evaluate the impact of docker container on the performance of deep learning applications. We first benchmark the performance of system components (IO, CPU and GPU) in a docker container and the host system and compare the results to see if there's any difference. According to our results, we find that computational intensive jobs, either running on CPU or GPU, have small overhead indicating docker containers can be applied to deep learning programs. Then we evaluate the performance of some popular deep learning tools deployed in a docker container and the host system. It turns out that the docker container will not cause noticeable drawbacks while running those deep learning tools. So encapsulating deep learning tool in a container is a feasible solution.},
  file = {/Users/x0r/Zotero/storage/YH6WFHZH/Xu et al_2017_Performance Evaluation of Deep Learning Tools in Docker Containers.pdf},
  isbn = {978-1-5386-3349-6},
  keywords = {perf},
  language = {en}
}

@article{yamamoto,
  title = {Crystal {{Graph Neural Networks}} for {{Data Mining}} in {{Materials Science}}},
  author = {Yamamoto, Takenori},
  pages = {11},
  abstract = {Machine learning methods have been employed for materials prediction in various ways. It has recently been proposed that a crystalline material is represented by a multigraph called a crystal graph. Convolutional neural networks adapted to those graphs have successfully predicted bulk properties of materials with the use of equilibrium bond distances as spatial information. An investigation into graph neural networks for small molecules has recently shown that the no distance model performs almost as well as the distance model. This paper proposes crystal graph neural networks (CGNNs) that use no bond distances, and introduces a scale-invariant graph coordinator that makes up crystal graphs for the CGNN models to be trained on the dataset based on a theoretical materials database. The CGNN models predict the bulk properties such as formation energy, unit cell volume, band gap, and total magnetization for every testing material, and the average errors are less than the corresponding ones of the database. The predicted band gaps and total magnetizations are used for the metalinsulator and nonmagnet-magnet binary classifications, which result in success. This paper presents discussions about high-throughput screening of candidate materials with the use of the predicted formation energies, and also about the future progress of materials data mining on the basis of the CGNN architectures.},
  file = {/Users/x0r/Zotero/storage/H3UJ8IIM/Yamamoto_Crystal Graph Neural Networks for Data Mining in Materials Science.pdf},
  keywords = {dl,material},
  language = {en}
}

@article{yang,
  title = {Weakly-Supervised {{Disentangling}} with {{Recurrent Transformations}} for {{3D View Synthesis}}},
  author = {Yang, Jimei and Reed, Scott and Yang, Ming-Hsuan and Lee, Honglak},
  pages = {9},
  abstract = {An important problem for both graphics and vision is to synthesize novel views of a 3D object from a single image. This is in particular challenging due to the partial observability inherent in projecting a 3D object onto the image space, and the ill-posedness of inferring object shape and pose. However, we can train a neural network to address the problem if we restrict our attention to specific object classes (in our case faces and chairs) for which we can gather ample training data. In this paper, we propose a novel recurrent convolutional encoder-decoder network that is trained end-to-end on the task of rendering rotated objects starting from a single image. The recurrent structure allows our model to capture longterm dependencies along a sequence of transformations, and we demonstrate the quality of its predictions for human faces on the Multi-PIE dataset and for a dataset of 3D chair models, and also show its ability of disentangling latent data factors without using object class labels.},
  file = {/Users/x0r/Zotero/storage/WYSBYK69/Yang et al_Weakly-supervised Disentangling with Recurrent Transformations for 3D View.pdf},
  keywords = {dl},
  language = {en}
}

@article{yang2017,
  title = {Designing {{Energy}}-{{Efficient Convolutional Neural Networks}} Using {{Energy}}-{{Aware Pruning}}},
  author = {Yang, Tien-Ju and Chen, Yu-Hsin and Sze, Vivienne},
  year = {2017},
  month = apr,
  abstract = {Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or amount of computation, we find that they do not necessarily result in lower energy consumption, and therefore do not serve as a good metric for energy cost estimation. To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses energy consumption estimation of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements that target realistic battery-powered system setups. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in output feature maps instead of filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is further globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet are reduced by 3.7x and 1.6x, respectively, with less than 1\% top-5 accuracy loss. Finally, we show that pruning the AlexNet with a reduced number of target classes can greatly decrease the number of weights but the energy reduction is limited. Energy modeling tool and energy-aware pruned models available at http://eyeriss.mit.edu/energy.html},
  archivePrefix = {arXiv},
  eprint = {1611.05128},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/J32WY5X6/Yang et al. - 2017 - Designing Energy-Efficient Convolutional Neural Ne.pdf;/Users/x0r/Zotero/storage/IXJSFEVZ/1611.html},
  journal = {arXiv:1611.05128 [cs]},
  primaryClass = {cs}
}

@article{yin2006,
  title = {Finite {{Element Analysis}} of {{Dependence}} of {{Programming Characteristics}} of {{Phase}}-{{Change Memory}} on {{Material Properties}} of {{Chalcogenides}}},
  author = {Yin, You and Sone, Hayato and Hosaka, Sumio},
  year = {2006},
  month = nov,
  volume = {45},
  pages = {8600--8603},
  issn = {0021-4922, 1347-4065},
  doi = {10.1143/JJAP.45.8600},
  file = {/Users/x0r/Zotero/storage/SG3VNM3L/Yin et al_2006_Finite Element Analysis of Dependence of Programming Characteristics of.pdf},
  journal = {Japanese Journal of Applied Physics},
  keywords = {modeling,PCM},
  language = {en},
  number = {11}
}

@article{yogatama2019,
  title = {Learning and {{Evaluating General Linguistic Intelligence}}},
  author = {Yogatama, Dani and {d'Autume}, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
  year = {2019},
  month = jan,
  abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
  archivePrefix = {arXiv},
  eprint = {1901.11373},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/ZVTB6VAI/Yogatama et al_2019_Learning and Evaluating General Linguistic Intelligence.pdf},
  journal = {arXiv:1901.11373 [cs, stat]},
  keywords = {AGI,nlp},
  primaryClass = {cs, stat}
}

@article{Yu_etal11a,
  ids = {yu2011},
  title = {An Electronic Synapse Device Based on Metal Oxide Resistive Switching Memory for Neuromorphic Computation},
  author = {Yu, Shimeng and Wu, Yi and Jeyasingh, Rakesh and Kuzum, Duygu and Wong, H-SP},
  year = {2011},
  volume = {58},
  pages = {2729--2737},
  publisher = {{[object Object]}},
  file = {/Users/x0r/Zotero/storage/69VLC52T/Yu et al_2011_An Electronic Synapse Device Based on Metal Oxide Resistive Switching Memory.pdf},
  journal = {Electron Devices, IEEE Transactions on},
  keywords = {neuromorphic,ReRAM},
  number = {8}
}

@article{yu2013,
  title = {Stochastic Learning in Oxide Binary Synaptic Device for Neuromorphic Computing},
  author = {Yu, Shimeng and Gao, Bin and Fang, Zheng and Yu, Hongyu and Kang, Jinfeng and Wong, H.-S. Philip},
  year = {2013},
  volume = {7},
  issn = {1662-453X},
  doi = {10.3389/fnins.2013.00186},
  abstract = {Hardware implementation of neuromorphic computing is attractive as a computing paradigm beyond the conventional digital computing. In this work, we show that the SET (off-to-on) transition of metal oxide resistive switching memory becomes probabilistic under a weak programming condition. The switching variability of the binary synaptic device implements a stochastic learning rule. Such stochastic SET transition was statistically measured and modeled for a simulation of a winner-take-all network for competitive learning. The simulation illustrates that with such stochastic learning, the orientation classification function of input patterns can be effectively realized. The system performance metrics were compared between the conventional approach using the analog synapse and the approach in this work that employs the binary synapse utilizing the stochastic learning. The feasibility of using binary synapse in the neurormorphic computing may relax the constraints to engineer continuous multilevel intermediate states and widens the material choice for the synaptic device design.},
  file = {/Users/x0r/Zotero/storage/QZAANMCM/Yu et al_2013_Stochastic learning in oxide binary synaptic device for neuromorphic computing.pdf},
  journal = {Frontiers in Neuroscience},
  keywords = {neuromorphic},
  language = {en}
}

@inproceedings{yu2014,
  title = {Design Considerations of Synaptic Device for Neuromorphic Computing},
  author = {Yu, Shimeng and Kuzum, Duygu and Wong, H.-S. Philip},
  year = {2014},
  month = jun,
  pages = {1062--1065},
  publisher = {{IEEE}},
  doi = {10.1109/ISCAS.2014.6865322},
  abstract = {Hardware implementation of neuromorphic computing is attractive as a computing paradigm beyond the conventional digital Boolean computing. Recently, two-terminal emerging memory devices that show electrically-triggered resistance modulation have been proposed as synaptic devices for neuromorphic computing. The synaptic device candidates include phase change memory (PCM), resistive RAM (RRAM) and conductive bridge RAM (CBRAM), etc. In this paper, we discuss the general design considerations of synaptic devices for plasticity and learning. As a rule of thumb for performance metrics assessment, an ideal synaptic device should have characteristics such as dimension, energy consumption, operation frequency, dynamic range, etc. that are scalable to biological systems with comparable complexity.},
  file = {/Users/x0r/Zotero/storage/3JMZ3HPC/Yu et al_2014_Design considerations of synaptic device for neuromorphic computing.pdf},
  isbn = {978-1-4799-3432-4 978-1-4799-3431-7},
  keywords = {neuromorphic},
  language = {en}
}

@article{yu2016,
  title = {Resistive {{Random Access Memory}} ({{RRAM}})},
  author = {Yu, Shimeng},
  year = {2016},
  month = mar,
  volume = {2},
  pages = {1--79},
  issn = {2381-1412, 2381-1439},
  doi = {10.2200/S00681ED1V01Y201510EET006},
  file = {/Users/x0r/Zotero/storage/5AMLST9P/Yu_2016_Resistive Random Access Memory (RRAM).pdf},
  journal = {Synthesis Lectures on Emerging Engineering Technologies},
  keywords = {neuromorphic,ReRAM},
  language = {en},
  number = {5}
}

@book{yu2017,
  title = {Neuro-Inspired {{Computing Using Resistive Synaptic Devices}}},
  editor = {Yu, Shimeng},
  year = {2017},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-54313-0},
  file = {/Users/x0r/Zotero/storage/C77YDIJ6/Yu_2017_Neuro-inspired Computing Using Resistive Synaptic Devices.pdf},
  isbn = {978-3-319-54312-3 978-3-319-54313-0},
  keywords = {neuromorphic},
  language = {en}
}

@article{zador2019,
  title = {A Critique of Pure Learning and What Artificial Neural Networks Can Learn from Animal Brains},
  author = {Zador, Anthony M.},
  year = {2019},
  month = aug,
  volume = {10},
  pages = {1--7},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-11786-6},
  abstract = {Recent gains in artificial neural networks rely heavily on large amounts of training data. Here, the author suggests that for AI to learn from animal brains, it is important to consider that animal behaviour results from brain connectivity specified in the genome through evolution, and not due to unique learning algorithms.},
  copyright = {2019 The Author(s)},
  file = {/Users/x0r/Zotero/storage/GB6BU54V/Zador_2019_A critique of pure learning and what artificial neural networks can learn from.pdf;/Users/x0r/Zotero/storage/ZZR9D2FU/s41467-019-11786-6.html},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{zakharov2019,
  title = {Few-{{Shot Adversarial Learning}} of {{Realistic Neural Talking Head Models}}},
  author = {Zakharov, Egor and Shysheya, Aliaksandra and Burkov, Egor and Lempitsky, Victor},
  year = {2019},
  month = may,
  abstract = {Several recent works have shown how highly realistic human head images can be obtained by training convolutional neural networks to generate them. In order to create a personalized talking head model, these works require training on a large dataset of images of a single person. However, in many practical scenarios, such personalized talking head models need to be learned from a few image views of a person, potentially even a single image. Here, we present a system with such few-shot capability. It performs lengthy meta-learning on a large dataset of videos, and after that is able to frame few- and one-shot learning of neural talking head models of previously unseen people as adversarial training problems with high capacity generators and discriminators. Crucially, the system is able to initialize the parameters of both the generator and the discriminator in a person-specific way, so that training can be based on just a few images and done quickly, despite the need to tune tens of millions of parameters. We show that such an approach is able to learn highly realistic and personalized talking head models of new people and even portrait paintings.},
  archivePrefix = {arXiv},
  eprint = {1905.08233},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/Q6FLJ7AS/Zakharov et al_2019_Few-Shot Adversarial Learning of Realistic Neural Talking Head Models.pdf},
  journal = {arXiv:1905.08233 [cs]},
  keywords = {dl,GAN},
  primaryClass = {cs}
}

@article{zaremba2015,
  title = {Reinforcement {{Learning Neural Turing Machines}} - {{Revised}}},
  author = {Zaremba, Wojciech and Sutskever, Ilya},
  year = {2015},
  month = may,
  abstract = {The Neural Turing Machine (NTM) is more expressive than all previously considered models because of its external memory. It can be viewed as a broader effort to use abstract external Interfaces and to learn a parametric model that interacts with them.},
  archivePrefix = {arXiv},
  eprint = {1505.00521},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/I46YBV6G/Zaremba_Sutskever_2015_Reinforcement Learning Neural Turing Machines - Revised.pdf},
  journal = {arXiv:1505.00521 [cs]},
  keywords = {NTM,rl},
  language = {en},
  primaryClass = {cs}
}

@article{zeilinger2005,
  title = {The Message of the Quantum},
  author = {Zeilinger, Anton},
  year = {2005},
  month = dec,
  volume = {438},
  pages = {743--743},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/438743a},
  journal = {Nature},
  keywords = {science,To read},
  language = {en},
  number = {7069}
}

@article{zenke2015,
  ids = {Zenke\_etal15},
  title = {Diverse Synaptic Plasticity Mechanisms Orchestrated to Form and Retrieve Memories in Spiking Neural Networks},
  author = {Zenke, Friedemann and Agnes, Everton J. and Gerstner, Wulfram},
  year = {2015},
  month = dec,
  volume = {6},
  issn = {2041-1723},
  doi = {10.1038/ncomms7922},
  file = {/Users/x0r/Zotero/storage/RVN6CLP5/Zenke et al_2015_Diverse synaptic plasticity mechanisms orchestrated to form and retrieve.pdf},
  journal = {Nature Communications},
  keywords = {neuroscience,SNN},
  language = {en},
  number = {1}
}

@article{zenke2017,
  title = {Continual {{Learning Through Synaptic Intelligence}}},
  author = {Zenke, Friedemann and Poole, Ben and Ganguli, Surya},
  year = {2017},
  month = mar,
  abstract = {Deep learning has led to remarkable advances when applied to problems where the data distribution does not change over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, and solve a diversity of tasks simultaneously. Furthermore, synapses in biological neurons are not simply real-valued scalars, but possess complex molecular machinery enabling non-trivial learning dynamics. In this study, we take a first step toward bringing this biological complexity into artificial neural networks. We introduce a model of intelligent synapses that accumulate task relevant information over time, and exploit this information to efficiently consolidate memories of old tasks to protect them from being overwritten as new tasks are learned. We apply our framework to learning sequences of related classification problems, and show that it dramatically reduces catastrophic forgetting while maintaining computational efficiency.},
  archivePrefix = {arXiv},
  eprint = {1703.04200},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/NJ3IAUJF/Zenke et al_2017_Continual Learning Through Synaptic Intelligence.pdf},
  journal = {arXiv:1703.04200 [cs, q-bio, stat]},
  keywords = {dl},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{zenke2018,
  title = {{{SuperSpike}}: {{Supervised}} Learning in Multi-Layer Spiking Neural Networks},
  shorttitle = {{{SuperSpike}}},
  author = {Zenke, Friedemann and Ganguli, Surya},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {1514--1541},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco_a_01086},
  abstract = {A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in-vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in-silico. Here we revisit the problem of supervised learning in temporally coding multi-layer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike-time patterns.},
  archivePrefix = {arXiv},
  eprint = {1705.11146},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/36J9QV7R/Zenke_Ganguli_2018_SuperSpike.pdf},
  journal = {Neural Computation},
  keywords = {SNN},
  language = {en},
  number = {6}
}

@article{zhang,
  title = {Circuit-{{GNN}}: {{Graph Neural Networks}} for {{Distributed Circuit Design}}},
  author = {Zhang, Guo and He, Hao and Katabi, Dina},
  pages = {10},
  abstract = {We present Circuit-GNN, a graph neural network (GNN) model for designing distributed circuits. Today, designing distributed circuits is a slow process that can take months from an expert engineer. Our model both automates and speeds up the process. The model learns to simulate the electromagnetic (EM) properties of distributed circuits. Hence, it can be used to replace traditional EM simulators, which typically take tens of minutes for each design iteration. Further, by leveraging neural networks' differentiability, we can use our model to solve the inverse problem \textendash i.e., given desirable EM specifications, we propagate the gradient to optimize the circuit parameters and topology to satisfy the specifications. We exploit the flexibility of GNN to create one model that works for different circuit topologies. We compare our model with a commercial simulator showing that it reduces simulation time by four orders of magnitude. We also demonstrate the value of our model by using it to design a Terahertz channelizer, a difficult task that requires a specialized expert. The results show that our model produces a channelizer whose performance is as good as a manually optimized design, and can save the expert several weeks of topology and parameter optimization. Most interestingly, our model comes up with new designs that differ from the limited templates commonly used by engineers in the field, hence significantly expanding the design space.},
  file = {/Users/x0r/Zotero/storage/GQIHSPHA/Zhang et al_Circuit-GNN.pdf},
  keywords = {dl,material},
  language = {en}
}

@article{zhang2017,
  title = {{{UNDERSTANDING DEEP LEARNING REQUIRES RE}}- {{THINKING GENERALIZATION}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz},
  year = {2017},
  pages = {15},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
  file = {/Users/x0r/Zotero/storage/YCYGAAUW/Zhang et al_2017_UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING GENERALIZATION.pdf},
  keywords = {dl},
  language = {en}
}

@article{zhao2020,
  title = {Reliability of Analog Resistive Switching Memory for Neuromorphic Computing},
  author = {Zhao, Meiran and Gao, Bin and Tang, Jianshi and Qian, He and Wu, Huaqiang},
  year = {2020},
  month = mar,
  volume = {7},
  pages = {011301},
  issn = {1931-9401},
  doi = {10.1063/1.5124915},
  abstract = {As artificial intelligence calls for novel energy-efficient hardware, neuromorphic computing systems based on analog resistive switching memory (RSM) devices have drawn great attention recently. Different from the well-studied binary RSMs, the analog RSMs are featured by a continuous and controllable conductance-tuning ability and thus are capable of combining analog computing and data storage at the device level. Although significant research achievements on analog RSMs have been accomplished, there have been few works demonstrating largescale neuromorphic systems. A major bottleneck lies in the reliability issues of the analog RSM, such as endurance and retention degradation and read/write noises and disturbances. Owing to the complexity of resistive switching mechanisms, studies on the origins of reliability degradation and the corresponding optimization methodology face many challenges. In this article, aiming on the high-performance neuromorphic computing applications, we provide a comprehensive review on the status of reliability studies of analog RSMs, the reliability requirements, and evaluation criteria and outlook for future reliability research directions in this field.},
  file = {/Users/x0r/Zotero/storage/UGU9428E/Zhao et al. - 2020 - Reliability of analog resistive switching memory f.pdf},
  journal = {Applied Physics Reviews},
  language = {en},
  number = {1}
}

@article{zhou2019,
  title = {Watch, {{Try}}, {{Learn}}: {{Meta}}-{{Learning}} from {{Demonstrations}} and {{Reward}}},
  shorttitle = {Watch, {{Try}}, {{Learn}}},
  author = {Zhou, Allan and Jang, Eric and Kappler, Daniel and Herzog, Alex and Khansari, Mohi and Wohlhart, Paul and Bai, Yunfei and Kalakrishnan, Mrinal and Levine, Sergey and Finn, Chelsea},
  year = {2019},
  month = jun,
  abstract = {Imitation learning allows agents to learn complex behaviors from demonstrations. However, learning a complex vision-based task may require an impractical number of demonstrations. Meta-imitation learning is a promising approach towards enabling agents to learn a new task from one or a few demonstrations by leveraging experience from learning similar tasks. In the presence of task ambiguity or unobserved dynamics, demonstrations alone may not provide enough information; an agent must also try the task to successfully infer a policy. In this work, we propose a method that can learn to learn from both demonstrations and trial-and-error experience with sparse reward feedback. In comparison to meta-imitation, this approach enables the agent to effectively and efficiently improve itself autonomously beyond the demonstration data. In comparison to meta-reinforcement learning, we can scale to substantially broader distributions of tasks, as the demonstration reduces the burden of exploration. Our experiments show that our method significantly outperforms prior approaches on a set of challenging, vision-based control tasks.},
  archivePrefix = {arXiv},
  eprint = {1906.03352},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/YSYTQEH6/Zhou et al_2019_Watch, Try, Learn.pdf;/Users/x0r/Zotero/storage/F9SG5F55/1906.html},
  journal = {arXiv:1906.03352 [cs, stat]},
  keywords = {meta-rl},
  primaryClass = {cs, stat}
}

@article{zhu2014,
  title = {One Order of Magnitude Faster Phase Change at Reduced Power in {{Ti}}-{{Sb}}-{{Te}}},
  author = {Zhu, Min and Xia, Mengjiao and Rao, Feng and Li, Xianbin and Wu, Liangcai and Ji, Xinglong and Lv, Shilong and Song, Zhitang and Feng, Songlin and Sun, Hongbo and Zhang, Shengbai},
  year = {2014},
  month = dec,
  volume = {5},
  issn = {2041-1723},
  doi = {10.1038/ncomms5086},
  file = {/Users/x0r/Zotero/storage/YRKR3HC2/Zhu et al_2014_One order of magnitude faster phase change at reduced power in Ti-Sb-Te.pdf},
  journal = {Nature Communications},
  keywords = {neuromorphic},
  language = {en},
  number = {1}
}

@article{zhu2019,
  title = {{{EENA}}: {{Efficient Evolution}} of {{Neural Architecture}}},
  shorttitle = {{{EENA}}},
  author = {Zhu, Hui and An, Zhulin and Yang, Chuanguang and Xu, Kaiqiang and Xu, Yongjun},
  year = {2019},
  month = may,
  abstract = {Latest algorithms for automatic neural architecture search perform remarkable but are basically directionless in search space and computational expensive in training of every intermediate architecture. In this paper, we propose a method for efficient architecture search called EENA (Efficient Evolution of Neural Architecture). Due to the elaborately designed mutation and crossover operations, the evolution process can be guided by the information have already been learned. Therefore, less computational effort will be required while the searching and training time can be reduced significantly. On CIFAR-10 classification, EENA using minimal computational resources (0.65 GPU-days) can design highly effective neural architecture which achieves 2.56\% test error with 8.47M parameters. Furthermore, the best architecture discovered is also transferable for CIFAR-100.},
  archivePrefix = {arXiv},
  eprint = {1905.07320},
  eprinttype = {arxiv},
  file = {/Users/x0r/Zotero/storage/5LQ98IYB/Zhu et al_2019_EENA.pdf},
  journal = {arXiv:1905.07320 [cs, stat]},
  keywords = {architecture-search,dl},
  primaryClass = {cs, stat}
}

@article{zipoli2016,
  title = {Structural Origin of Resistance Drift in Amorphous {{GeTe}}},
  author = {Zipoli, Federico and Krebs, Daniel and Curioni, Alessandro},
  year = {2016},
  month = mar,
  volume = {93},
  issn = {2469-9950, 2469-9969},
  doi = {10.1103/PhysRevB.93.115201},
  file = {/Users/x0r/Zotero/storage/DWPUUQYM/Zipoli et al_2016_Structural origin of resistance drift in amorphous GeTe.pdf},
  journal = {Physical Review B},
  keywords = {drift},
  language = {en},
  number = {11}
}

@misc{zotero-1362,
  title = {Settings - {{SWITCHdrive}}},
  file = {/Users/x0r/Zotero/storage/DUBYSAQA/personal.html},
  howpublished = {https://drive.switch.ch/index.php/settings/personal?sectionid=security}
}

@misc{zotero-187,
  title = {Shtetl-{{Optimized}} \guillemotright{} {{Blog Archive}} \guillemotright{} {{Why I Am Not An Integrated Information Theorist}} (or, {{The Unconscious Expander}})},
  keywords = {IIT},
  language = {en-US}
}

@misc{zotero-308,
  title = {{{OTCBVS}}},
  howpublished = {http://vcipl-okstate.org/pbvs/bench/},
  keywords = {polarcam}
}

@misc{zotero-333,
  title = {A Neural Network Model of Flexible Grasp Movement Generation | {{bioRxiv}}},
  file = {/Users/x0r/Zotero/storage/NBEK6WEE/742189v1.html},
  howpublished = {https://www.biorxiv.org/content/10.1101/742189v1},
  keywords = {SNN}
}

@unpublished{zotero-389,
  title = {A {{Roadmap}} for {{Understanding Memory}}: {{Decomposing Cognitive Processes}} into {{Operations}} and {{Representations}}},
  file = {/Users/x0r/Zotero/storage/WFFYS4BD/A Roadmap for Understanding Memory.pdf}
}

@article{zotero-4693,
  title = {The {{Promise}} of {{Neural Networks}}},
  pages = {5},
  file = {/Users/x0r/Zotero/storage/6SGJGD8J/The Promise of Neural Networks.pdf},
  keywords = {To read},
  language = {en}
}

@misc{zotero-4813,
  title = {Role of {{Dopamine Neurons}} in {{Reward}} and {{Aversion}}: {{A Synaptic Plasticity Perspective}} | {{Elsevier Enhanced Reader}}},
  shorttitle = {Role of {{Dopamine Neurons}} in {{Reward}} and {{Aversion}}},
  doi = {10.1016/j.neuron.2015.04.015},
  file = {/Users/x0r/Zotero/storage/SFCFYP2F/Role of Dopamine Neurons in Reward and Aversion A.pdf;/Users/x0r/Zotero/storage/6C4MUNGS/S0896627315003657.html},
  language = {en}
}

@misc{zotero-4830,
  title = {An {{Imperfect Dopaminergic Error Signal Can Drive Temporal}}-{{Difference Learning}}},
  file = {/Users/x0r/Zotero/storage/8QK3K6UR/PMC3093351.html},
  howpublished = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3093351/}
}

@misc{zotero-538,
  title = {Simply {{Logical}}},
  howpublished = {https://book.simply-logical.space/}
}


